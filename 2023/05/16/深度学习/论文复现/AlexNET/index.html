<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AlexNet | Maxswordsman</title><meta name="author" content="Maxswordsman"><meta name="copyright" content="Maxswordsman"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AlexNET一、预备知识1.网络可视化1pip install torchsummary  输入：为模型、输入尺寸、批数量、设备  输出：模型的参数信息 123from torchsummary import summarydef summary(model, input_size, batch_size&#x3D;-1, device&#x3D;&quot;cuda&quot;)  # 函数默认是cuda，">
<meta property="og:type" content="article">
<meta property="og:title" content="AlexNet">
<meta property="og:url" content="https://maxswordsman.github.io/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/AlexNET/index.html">
<meta property="og:site_name" content="Maxswordsman">
<meta property="og:description" content="AlexNET一、预备知识1.网络可视化1pip install torchsummary  输入：为模型、输入尺寸、批数量、设备  输出：模型的参数信息 123from torchsummary import summarydef summary(model, input_size, batch_size&#x3D;-1, device&#x3D;&quot;cuda&quot;)  # 函数默认是cuda，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://maxswordsman.github.io/image/head2.webp">
<meta property="article:published_time" content="2023-05-16T15:40:00.000Z">
<meta property="article:modified_time" content="2023-05-16T15:40:00.000Z">
<meta property="article:author" content="Maxswordsman">
<meta property="article:tag" content="CNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maxswordsman.github.io/image/head2.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://maxswordsman.github.io/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/AlexNET/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AlexNet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-16 23:40:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/iconfont.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/head2.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movie/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/book/"><i class="fa-fw fas fa-book"></i><span> 书单</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友人帐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Maxswordsman"><span class="site-name">Maxswordsman</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movie/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/book/"><i class="fa-fw fas fa-book"></i><span> 书单</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友人帐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AlexNet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-16T15:40:00.000Z" title="发表于 2023-05-16 23:40:00">2023-05-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-16T15:40:00.000Z" title="更新于 2023-05-16 23:40:00">2023-05-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>40分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AlexNet"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer" />


<h2 id="AlexNET"><a href="#AlexNET" class="headerlink" title="AlexNET"></a>AlexNET</h2><h3 id="一、预备知识"><a href="#一、预备知识" class="headerlink" title="一、预备知识"></a>一、预备知识</h3><h4 id="1-网络可视化"><a href="#1-网络可视化" class="headerlink" title="1.网络可视化"></a>1.网络可视化</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchsummary</span><br></pre></td></tr></table></figure>

<p><strong>输入：</strong>为模型、输入尺寸、批数量、设备 </p>
<p><strong>输出：</strong>模型的参数信息</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torchsummary import summary</span><br><span class="line"></span><br><span class="line">def <span class="title function_">summary</span><span class="params">(model, input_size, batch_size=<span class="number">-1</span>, device=<span class="string">&quot;cuda&quot;</span>)</span>  # 函数默认是cuda，若是在cpu下就需要修改</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchsummary import summary</span><br><span class="line">from torchvision.models import vgg16  # 以 vgg16 为例</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">myNet = vgg16()  # 实例化网络，可以换成自己的网络</span><br><span class="line"># 将模型移动到gpu上</span><br><span class="line">myNet = myNet.to(device)</span><br><span class="line">summary(myNet, (<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>))  # 输出网络结构</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305062235519.png" alt="image-20230506223506480"></p>
<h4 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2.数据集"></a>2.数据集</h4><p>百度网盘下载地址：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">链接: https:<span class="comment">//pan.baidu.com/s/1Uro6RuEbRGGCQ8iXvF2SAQ 密码: hl31</span></span><br></pre></td></tr></table></figure>

<p><code>ImageNet</code> 数据集太大了1000类别，而且达到100多G的大小，因此换成<code>Mini-ImageNet</code>测试网络</p>
<p><code>Mini-ImageNet</code>数据集大约3G左右，100个类别，每一个类别均有600张图片左右，共60000张图片，而且图片都是可变分辨率的（图片大小尺寸不固定）</p>
<p>数据集的结构：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── mini-imagenet: 数据集根目录</span><br><span class="line">     ├── images: 所有的图片都存在这个文件夹中</span><br><span class="line">     ├── train.csv: 对应训练集的标签文件</span><br><span class="line">     ├── val.csv: 对应验证集的标签文件</span><br><span class="line">     └── test.csv: 对应测试集的标签文件</span><br></pre></td></tr></table></figure>

<p><code>Mini-Imagenet</code>数据集中包含了<code>train.csv</code>、<code>val.csv</code>以及<code>test.csv</code>三个文件,但是提供的标签文件并不是从每个类别中进行采样的，因此无法直接用于训练分类，</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.csv 包含<span class="number">38400</span>张图片，共<span class="number">64</span>个类别</span><br><span class="line">val.csv   包含<span class="number">9600</span>张图片，共<span class="number">16</span>个类别</span><br><span class="line">test.csv  包含<span class="number">12000</span>张图片，共<span class="number">20</span>个类别</span><br></pre></td></tr></table></figure>

<p>按照上述链接下载文件之后，对images进行解压，在使用<code>panads</code>对数据集进行分割，需要自己构建一个新的<code>new_train.csv</code>与<code>new_val.csv</code>以<code>new_test.val</code>，代码中<code>imagenet_class_index.json</code>的下载地址为：<a target="_blank" rel="noopener" href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/pytorch_classification/mini_imagenet/imagenet_class_index.json">json</a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.将train.csv与val.csv以及test.csv 进行合并(乱序)之后再按照比例进行分割为</span></span><br><span class="line"><span class="string">        new_train.csv与new_val.csv以及new_test.csv</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">print(BASE_DIR)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    读取csv下的分类</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def read_csv_classes(csv_dir: str, csv_name: str):</span><br><span class="line">    # 读取mini-imagenet 下的csv文件</span><br><span class="line">    data = pd.read_csv(os.path.join(csv_dir, csv_name))</span><br><span class="line"></span><br><span class="line">    # 得到csv文件下的label列的元素  并对其进行去重 drop_duplicates()</span><br><span class="line">    label_set = <span class="built_in">set</span>(data[<span class="string">&quot;label&quot;</span>].drop_duplicates().values)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;&#123;&#125; have &#123;&#125; images and &#123;&#125; classes.&quot;</span>.format(csv_name,</span><br><span class="line">                                                     data.shape[<span class="number">0</span>],</span><br><span class="line">                                                     len(label_set)))</span><br><span class="line">    <span class="keyword">return</span> data, label_set</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    进行分割数据集  6:2:2</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def calculate_split_info(path: str, save_path:str,label_dict: dict, rate1: <span class="type">float</span> = <span class="number">0.2</span>,rate2: <span class="type">float</span> = <span class="number">0.2</span>):</span><br><span class="line">    # image_dir 为mini-imagenet 下的images路径 存放的是所有图片</span><br><span class="line">    image_dir = os.path.join(path, <span class="string">&quot;images&quot;</span>)</span><br><span class="line">    # 得到image_dir路径下以jpg为后缀的文件的列表</span><br><span class="line">    images_list = [i <span class="keyword">for</span> i in os.listdir(image_dir) <span class="keyword">if</span> i.endswith(<span class="string">&quot;.jpg&quot;</span>)]</span><br><span class="line">    # 输出数据集中的图片数量</span><br><span class="line">    print(<span class="string">&quot;find &#123;&#125; images in dataset.&quot;</span>.format(len(images_list)))</span><br><span class="line"></span><br><span class="line">    train_data, train_label = read_csv_classes(path, <span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line">    val_data, val_label = read_csv_classes(path, <span class="string">&quot;val.csv&quot;</span>)</span><br><span class="line">    test_data, test_label = read_csv_classes(path, <span class="string">&quot;test.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    # 得到 train test val 三个数据集中的标签  总共为 <span class="number">100</span>类</span><br><span class="line">    labels = (train_label | val_label | test_label)</span><br><span class="line">    labels = <span class="built_in">list</span>(labels)</span><br><span class="line">    labels.sort()</span><br><span class="line">    print(<span class="string">&quot;all classes: &#123;&#125;&quot;</span>.format(len(labels)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #  得到类似于 <span class="string">&#x27;n01532829&#x27;</span>: [<span class="number">0</span>, <span class="string">&#x27;house_finch&#x27;</span>] 这样格式的字典</span><br><span class="line">    classes_label = dict([(label, [index, label_dict[label]]) <span class="keyword">for</span> index, label in enumerate(labels)])</span><br><span class="line">    # 将得到的字典写入json文件中</span><br><span class="line">    json_str = json.dumps(classes_label, indent=<span class="number">4</span>)</span><br><span class="line">    with open(<span class="string">&#x27;./Data/classes_name.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) as json_file:</span><br><span class="line">        json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">    # 将train.csv  test.csv  val.csv 三个文件的内容拼接到一起 （得到所有数据的csv文件  里面的样本数量总共为<span class="number">60000</span>）</span><br><span class="line">    <span class="meta"># pd.concat()函数可以沿着指定的轴将多个dataframe或者series拼接到一起</span></span><br><span class="line">    data = pd.concat([train_data, val_data, test_data], axis=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">&quot;total data shape: &#123;&#125;&quot;</span>.format(data.shape))</span><br><span class="line"></span><br><span class="line">    # 在每一个类别中分割数据集</span><br><span class="line">    num_every_classes = []</span><br><span class="line">    split_train_data = []</span><br><span class="line">    split_val_data = []</span><br><span class="line">    split_test_data = []</span><br><span class="line">    <span class="keyword">for</span> label in labels:</span><br><span class="line">        # class_data 为每个类 对应的图片的图片的DataFrame</span><br><span class="line">        # 每个类别的图片数量为 <span class="number">600</span></span><br><span class="line">        class_data = data[data[<span class="string">&quot;label&quot;</span>] == label]</span><br><span class="line"></span><br><span class="line">        num_every_classes.append(class_data.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #  乱序</span><br><span class="line">        # DataFrame.sample 可用来对DataFrame进行随机抽样</span><br><span class="line">        <span class="meta"># frac 参数用于指定抽取的样本条数的比例（这里的代码表示全部进行抽取即乱序）</span></span><br><span class="line">        # random_state 参数 可以复现抽样结果 第二次与第一次抽取的结果一致</span><br><span class="line">        shuffle_data = class_data.sample(frac=<span class="number">1</span>, random_state=<span class="number">1</span>)</span><br><span class="line">        # 分割比例</span><br><span class="line">        num_train_sample = <span class="type">int</span>(class_data.shape[<span class="number">0</span>] * (<span class="number">1</span> - rate1 - rate2))</span><br><span class="line">        num_val_sample = <span class="type">int</span>(class_data.shape[<span class="number">0</span>] * rate1)</span><br><span class="line">        new_test_sample = <span class="type">int</span>(class_data.shape[<span class="number">0</span>] * rate2)</span><br><span class="line">        # 因为每个类别为<span class="number">600</span>张  对每个类别求的分割的比例</span><br><span class="line">        split_train_data.append(shuffle_data[:num_train_sample])</span><br><span class="line">        split_val_data.append(shuffle_data[num_train_sample:(num_val_sample + num_train_sample)])</span><br><span class="line">        split_test_data.append(shuffle_data[(num_val_sample + num_train_sample):<span class="type">int</span>(class_data.shape[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">        <span class="meta"># imshow</span></span><br><span class="line">        imshow_flag = False</span><br><span class="line">        <span class="keyword">if</span> imshow_flag:</span><br><span class="line">            img_name, img_label = shuffle_data.iloc[<span class="number">0</span>].values</span><br><span class="line">            img = Image.open(os.path.join(image_dir, img_name))</span><br><span class="line">            plt.imshow(img)</span><br><span class="line">            plt.title(<span class="string">&quot;class: &quot;</span> + classes_label[img_label][<span class="number">1</span>])</span><br><span class="line">            plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="meta"># plot classes distribution</span></span><br><span class="line">    plot_flag = False</span><br><span class="line">    <span class="keyword">if</span> plot_flag:</span><br><span class="line">        plt.bar(range(<span class="number">1</span>, <span class="number">101</span>), num_every_classes, align=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="meta"># concatenate data 将分割的数据集 创建一个新的csv文件  并将其内容进行拼接</span></span><br><span class="line">    new_train_data = pd.concat(split_train_data, axis=<span class="number">0</span>)</span><br><span class="line">    new_val_data = pd.concat(split_val_data, axis=<span class="number">0</span>)</span><br><span class="line">    new_test_data = pd.concat(split_test_data, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># save new csv data</span></span><br><span class="line">    new_train_data.to_csv(os.path.join(save_path, <span class="string">&quot;new_train.csv&quot;</span>))</span><br><span class="line">    new_val_data.to_csv(os.path.join(save_path, <span class="string">&quot;new_val.csv&quot;</span>))</span><br><span class="line">    new_test_data.to_csv(os.path.join(save_path, <span class="string">&quot;new_test.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    data_dir = <span class="string">&quot;/media/zxz/新加卷/DataSET/mini-imagenet/&quot;</span>  # 指向数据集的根目录</span><br><span class="line">    json_path = <span class="string">&quot;./Data/imagenet_class_index.json&quot;</span>  # 指向imagenet的索引标签文件</span><br><span class="line">    save_path = <span class="string">&quot;./Data&quot;</span>   # 创建的new_train.csv  与  ne_val.csv 需要保留的地址</span><br><span class="line"></span><br><span class="line">    <span class="meta"># load imagenet labels</span></span><br><span class="line">    label_dict = json.load(open(json_path, <span class="string">&quot;r&quot;</span>))</span><br><span class="line">    # 得到一个字典，其中键代表 label  而值代表label对应的事物的英语单词 如： <span class="string">&#x27;n01440764&#x27;</span>: <span class="string">&#x27;tench&#x27;</span></span><br><span class="line">    label_dict = dict([(v[<span class="number">0</span>], v[<span class="number">1</span>]) <span class="keyword">for</span> k, v in label_dict.items()])</span><br><span class="line"></span><br><span class="line">    calculate_split_info(data_dir, save_path,label_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>一般训练集、验证集、测试集按照<code>6:2:2</code>的比例进行分割,分割后得到的<code>csv</code>文件如下</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_train.csv 包含<span class="number">36000</span>张图片，共<span class="number">100</span>个类别</span><br><span class="line">new_val.csv   包含<span class="number">12000</span>张图片，共<span class="number">100</span>个类别</span><br><span class="line">new_test.csv  包含<span class="number">12000</span>张图片，共<span class="number">100</span>个类别</span><br></pre></td></tr></table></figure>

<p>根据创建的csv文件划分为原始如下形式：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Mini-imageNet</span><br><span class="line">    new_train</span><br><span class="line">    	class1_dir</span><br><span class="line">   		class2_dir</span><br><span class="line">    	...</span><br><span class="line">   	new_val</span><br><span class="line">    	class1_dir</span><br><span class="line">   		class2_dir</span><br><span class="line">    	...</span><br><span class="line">   	new_test</span><br><span class="line">    	class1_dir</span><br><span class="line">   		class2_dir</span><br><span class="line">    	...</span><br></pre></td></tr></table></figure>

<p>代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    根据split_dataset1 新创建的csv文件   对原始的数据集进行分割</span></span><br><span class="line"><span class="string">        1.根据new_train.csv 文件中的图片 创建new_train文件夹 将图片复制到 new_train文件夹的对应的label下的文件夹下</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def <span class="title function_">copy_to_move</span><span class="params">(base_path:str,root_path:str,csv_path:str,move_to_dir:str)</span>:</span><br><span class="line">    # 读取csv文件  获取所有的img文件名称</span><br><span class="line">    handle_csv = os.path.join(base_path, csv_path)</span><br><span class="line">    data = pd.read_csv(handle_csv)</span><br><span class="line"></span><br><span class="line">    # 将csv文件中的图片名字  装入列表</span><br><span class="line">    handle_filename = <span class="built_in">list</span>(data[<span class="string">&quot;filename&quot;</span>].values)</span><br><span class="line">    handle_label = <span class="built_in">list</span>(data[<span class="string">&quot;label&quot;</span>].values)   <span class="meta">#  classes = 100</span></span><br><span class="line">    print(<span class="string">&quot;the train_cav data num is &#123;&#125; classes is &#123;&#125;&quot;</span>.format(len(handle_filename), len(<span class="built_in">set</span>(handle_label))))</span><br><span class="line"></span><br><span class="line">    dst = move_to_dir   #  提前创建一个new_train or new_test or new_val 文件夹，将CSV对应的img 复制到文件夹中</span><br><span class="line">    <span class="keyword">for</span> i, name in enumerate(handle_filename):</span><br><span class="line">        imgx = os.path.join(root_path, name)</span><br><span class="line">        print(f<span class="string">&quot;第&#123;i&#125;张图片已经copy完成&quot;</span>)</span><br><span class="line">        print(imgx)</span><br><span class="line">        shutil.copy(imgx, dst)</span><br><span class="line"></span><br><span class="line">    files = os.listdir(dst)  # 上一步创建的文件夹</span><br><span class="line">    pre = dst</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, img in enumerate(files):</span><br><span class="line">        # <span class="number">1.</span> 首先遍历每个文件，创建文件夹</span><br><span class="line">        #  n0153282900000138.jpg</span><br><span class="line">        dir_name = img.split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>][:<span class="number">9</span>]  # 这里就是为了截取label，根据img name 前<span class="number">9</span>个为label</span><br><span class="line">        dir_path = os.path.join(pre,dir_name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> not os.path.exists(dir_path):</span><br><span class="line">            os.mkdir(dir_path)  # 创建该类文件夹</span><br><span class="line"></span><br><span class="line">        #  直接判断该文件，归类</span><br><span class="line">        img_path = os.path.join(pre,img)</span><br><span class="line">        <span class="keyword">if</span> not os.path.isdir(img_path):</span><br><span class="line">            <span class="keyword">if</span> img[:<span class="number">9</span>] == dir_name:  # 由于每个类包含很多img文件，判断该文件是否属于该类</span><br><span class="line">                shutil.move(img_path, dir_path)  <span class="meta"># true的话，移动到该类目录</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    # 当前程序文件所在的目录</span><br><span class="line">    BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">    print(BASE_DIR)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># mini-imagenet数据集原始的images文件下</span></span><br><span class="line">    root_path_images = <span class="string">&quot;/media/zxz/新加卷/DataSET/mini-imagenet/images&quot;</span></span><br><span class="line">    new_train_csv_path = <span class="string">&quot;./Mini-ImageNet/new_train.csv&quot;</span></span><br><span class="line">    new_val_csv_path = <span class="string">&quot;./Mini-ImageNet/new_val.csv&quot;</span></span><br><span class="line">    new_test_csv_path = <span class="string">&quot;./Mini-ImageNet/new_test.csv&quot;</span></span><br><span class="line"></span><br><span class="line">    new_train_dir = <span class="string">&quot;./Mini-ImageNet/new_train&quot;</span></span><br><span class="line">    new_val_dir = <span class="string">&quot;./Mini-ImageNet/new_val&quot;</span></span><br><span class="line">    new_test_dir = <span class="string">&quot;./Mini-ImageNet/new_test&quot;</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;创建new_train文件夹.....&quot;</span>)</span><br><span class="line">    copy_to_move(BASE_DIR,root_path_images,new_train_csv_path,new_train_dir)</span><br><span class="line">    print()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;创建new_val文件夹.....&quot;</span>)</span><br><span class="line">    copy_to_move(BASE_DIR, root_path_images, new_val_csv_path, new_val_dir)</span><br><span class="line">    print()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;创建new_test文件夹.....&quot;</span>)</span><br><span class="line">    copy_to_move(BASE_DIR, root_path_images, new_test_csv_path, new_test_dir)</span><br><span class="line">    print()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>划分的数据集图片：</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305081527871.png" alt="image-20230508152703849"></p>
<h3 id="二、论文细节"><a href="#二、论文细节" class="headerlink" title="二、论文细节"></a>二、论文细节</h3><h4 id="1-ReLu与Tanh收敛速度的比较"><a href="#1-ReLu与Tanh收敛速度的比较" class="headerlink" title="1.ReLu与Tanh收敛速度的比较"></a>1.<code>ReLu</code>与<code>Tanh</code>收敛速度的比较</h4><p>在论文的3.1节中提到ReLu相较于Tanh收敛速度更快，且ReLu无需对输入数据进行归一化防止饱和，在不对数据进行归一化的情况下，比较如下：</p>
<h5 id="（1）代码"><a href="#（1）代码" class="headerlink" title="（1）代码"></a>（1）代码</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">代码功能:</span></span><br><span class="line"><span class="string">    复现论文中3.1的部分比较</span></span><br><span class="line"><span class="string">        对于特定的四层卷积神经网络 达到25%的训练误差 所迭代的论轮数</span></span><br><span class="line"><span class="string">        没有对数据进行任何的正规化</span></span><br><span class="line"><span class="string">        每个网络的学习速率都是独立选择的，以使训练尽可能快</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.下载数据集</span></span><br><span class="line"><span class="string">        len(train_data) = 50000</span></span><br><span class="line"><span class="string">        len(test_data) = 10000</span></span><br><span class="line"><span class="string">        pic shape = [2,32,32]</span></span><br><span class="line"><span class="string">        classes = 10</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../Cifar-10&quot;</span>,transform=torchvision.transforms.PILToTensor(),train=True,download=True)</span><br><span class="line"></span><br><span class="line"># 利用DataLoader加载数据集</span><br><span class="line">train_dataloader = DataLoader(train_data,batch_size=<span class="number">128</span>,shuffle=True)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    3.搭建四层卷积网络</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 搭建神经网络</span><br><span class="line">class Module(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            # Layer1</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  #  <span class="number">32</span> <span class="number">32</span> <span class="number">32</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>) , # <span class="number">32</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            # Layer2</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            #Layer3</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            # # Layer4</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   # <span class="number">128</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=<span class="number">2</span>),  # <span class="number">128</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            # Linear Layer</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">2</span>*<span class="number">2</span>,<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,input):</span><br><span class="line">        input = self.model(input)</span><br><span class="line">        <span class="keyword">return</span> input</span><br><span class="line"></span><br><span class="line">class Module2(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            # Layer1</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  #  <span class="number">32</span> <span class="number">32</span> <span class="number">32</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>) , # <span class="number">32</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            # Layer2</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            #Layer3</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            # # Layer4</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   # <span class="number">128</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=<span class="number">2</span>),  # <span class="number">128</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            # Linear Layer</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">2</span>*<span class="number">2</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,input):</span><br><span class="line">        input = self.model(input)</span><br><span class="line">        <span class="keyword">return</span> input</span><br><span class="line"></span><br><span class="line"><span class="meta"># <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span></span><br><span class="line"><span class="meta">#     x = torch.rand([1, 3, 32, 32])</span></span><br><span class="line"><span class="meta">#     model = Module()</span></span><br><span class="line"><span class="meta">#     y = model(x)</span></span><br><span class="line"><span class="meta">#     print(y.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    4.训练模型</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 定义程序运行设备，若无法使用GPU则在CPU上进行运算</span><br><span class="line">device_1 = <span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">device = torch.device(device_1)</span><br><span class="line"></span><br><span class="line"># 创建网络模型（Relu）</span><br><span class="line">model1 = Module()</span><br><span class="line">model1 = model1.to(device=device)</span><br><span class="line"># （Tanh）</span><br><span class="line">model2 = Module2()</span><br><span class="line">model2 = model2.to(device=device)</span><br><span class="line"></span><br><span class="line"># 定义损失函数</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn = loss_fn.to(device=device)</span><br><span class="line"></span><br><span class="line"># 定义优化器</span><br><span class="line">learn_rate = <span class="number">0.01</span></span><br><span class="line">optimizer1 = torch.optim.SGD(model1.parameters(),lr=learn_rate,momentum=<span class="number">0.9</span>,weight_decay=<span class="number">0.0005</span>)</span><br><span class="line">optimizer2 = torch.optim.SGD(model2.parameters(),lr=learn_rate,momentum=<span class="number">0.9</span>,weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">def train(lun,dataloader,model,loss_fn,optimizer):</span><br><span class="line">    # 将模型转化为训练模式</span><br><span class="line">    model.train()</span><br><span class="line">    loss,acc,step,epoch_error_rate = <span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0</span>,<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data in dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        imgs = imgs.<span class="type">float</span>()</span><br><span class="line">        <span class="meta"># imgs = torch.tensor(np.array(imgs))</span></span><br><span class="line">        <span class="meta"># targets = torch.tensor(np.array(targets))</span></span><br><span class="line">        # 对数据进行GPU加速</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        # 将数据传入网路模型</span><br><span class="line">        output = model(imgs) # 分别得到每一张图片为那一个target的概率值</span><br><span class="line">        # 求解当前损失值(当前批次的损失)</span><br><span class="line">        cur_loss = loss_fn(output,targets)</span><br><span class="line">        # 求解当前训练批次的正确率</span><br><span class="line">        _, pred = torch.max(output, axis=<span class="number">1</span>)</span><br><span class="line">        cur_acc = torch.sum(targets == pred) / output.shape[<span class="number">0</span>]</span><br><span class="line">        # 求解当前训练批次的错误率</span><br><span class="line">        batch_error = torch.sum(targets != pred) / output.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        # 反向传播</span><br><span class="line">        optimizer.zero_grad() # 优化前将梯度清<span class="number">0</span></span><br><span class="line">        cur_loss.backward()        # 反向传播，求得每一个节点的梯度</span><br><span class="line">        optimizer.step()       # 对模型的每一个参数进行优化</span><br><span class="line"></span><br><span class="line">        # 将训练集下的每一轮的每一个批次的的错误率累加（跳出<span class="keyword">for</span>循环最后得到这一轮的总错误率）</span><br><span class="line">        epoch_error_rate += batch_error.item()</span><br><span class="line">        <span class="meta"># step 该训练集目前训练到多少批次</span></span><br><span class="line">        step = step +<span class="number">1</span>  # 本轮样本的训练次数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 本轮训练的每一批次的平均损失</span><br><span class="line">    train_error_rate = epoch_error_rate / step # 本轮训练的平均损失</span><br><span class="line">    print(<span class="string">&quot;train_error_rate: &#123;&#125;&quot;</span>.format(train_error_rate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_error_rate</span><br><span class="line"></span><br><span class="line"># 定义画图函数</span><br><span class="line">def matplot_loss(train_error_relu, train_error_tanh):</span><br><span class="line">    plt.plot(train_error_relu, label=<span class="string">&#x27;error_relu&#x27;</span>)  # 画一个折线名字named = error_relu</span><br><span class="line">    plt.plot(train_error_tanh, label=<span class="string">&#x27;error_tanh&#x27;</span>)      # 画一个折线名字named = error_tanh</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)                    # （说明那条曲线是什么的标签）指定图例的位置。默认为loc=best 左上方</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;train_error_rate&#x27;</span>)                        # 二维图形的y轴名称</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)                       # 二维图形的X轴名称</span><br><span class="line">    plt.title(<span class="string">&quot;train_error_relu vs train_error_tanh&quot;</span>)       # 图的标题</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建列表用于存储数据画图</span><br><span class="line">relu_train_error_rate = []</span><br><span class="line">tanh_train_error_rate = []</span><br><span class="line"></span><br><span class="line"># 训练轮数实现</span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> i in range(epoch):</span><br><span class="line">        print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;-------Relu 第 &#123;&#125; 轮训练开始------&quot;</span>.format(i + <span class="number">1</span>))</span><br><span class="line">        train_error_relu = train(i + <span class="number">1</span>, train_dataloader, model1, loss_fn, optimizer1)</span><br><span class="line">        relu_train_error_rate.append(train_error_relu)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i in range(epoch):</span><br><span class="line">        print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;-------Tanh 第 &#123;&#125; 轮训练开始------&quot;</span>.format(i + <span class="number">1</span>))</span><br><span class="line">        train_error_tanh = train(i + <span class="number">1</span>, train_dataloader, model2, loss_fn, optimizer2)</span><br><span class="line">        tanh_train_error_rate.append(train_error_tanh)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(relu_train_error_rate)</span><br><span class="line">    print(tanh_train_error_rate)</span><br><span class="line"></span><br><span class="line">    matplot_loss(relu_train_error_rate,tanh_train_error_rate)</span><br></pre></td></tr></table></figure>

<h5 id="（2）结果"><a href="#（2）结果" class="headerlink" title="（2）结果"></a>（2）结果</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091445057.png" alt="image-20230509144434172"></p>
<p>对上面的结果进行比较可以得到，<code>ReLu</code>的收敛速度在17轮之前确实是优于<code>tanh</code></p>
<h4 id="2-LRN"><a href="#2-LRN" class="headerlink" title="2.LRN"></a>2.<code>LRN</code></h4><p>论文的3.3接提到的局部响应标准化（<code>LRN</code>）有助于AlexNet泛化能力的提升，受真实的神经元<strong>侧抑制</strong>启发</p>
<p>**侧抑制:**细胞分化变为不同时，会对周围细胞产生抑制信号，组织他们像相同的方向分化，最终表现为细胞命运的不同</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091459835.png" alt="image-20230509145958807"></p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091500540.png" alt="image-20230509150018496"></p>
<ul>
<li>a 表示<strong>卷积层（包括卷积操作和激活操作）后的输出结果</strong>。这个输出的结果是一个四维数组 [batch,height,width,channel]。这个输出结构中的一个位置 [a,b,c,d]，可以理解成在某一张特征图中的某一个通道下的某个高度和某个宽度位置的点，即<strong>第 a 张特征图的第 d 个通道下的高度为 b 宽度为 c 的点。</strong></li>
<li><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091502049.png" alt="image-20230509150204010">表示第 i 个通道的特征图在位置（x,y)运用激活函数 ReLU 后的输出。n 是同一位置上临近的 feature map 的数目，N 是特征图的总数。</li>
</ul>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091504802.png" alt="image-20230509150421758"></p>
<p>即公式中的分母，若此处的分母越大即表示对该处的像素值抑制程度越大。若<img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091507434.png" alt="image-20230509150754414">周围的deepth_radius范围存在较大的像素值，那么对于<img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091509677.png">的输出存在较大的抑制</p>
<p>论文中提到使用<code>LRN</code>分别减少了<code>top-1</code>和<code>top-5</code>的1.4%与1.2%的错误率</p>
<h5 id="（1）Pyotrch中LRN的实现"><a href="#（1）Pyotrch中LRN的实现" class="headerlink" title="（1）Pyotrch中LRN的实现"></a>（1）Pyotrch中LRN的实现</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LocalResponseNorm(size, alpha=<span class="number">0.0001</span>, beta=<span class="number">0.75</span>, k=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091514922.png" alt="image-20230509151431897"></p>
<p>在2014年的<code>《Very Deep Convolutional Networks for Large-Scale Image Recognition》</code>提到<code>LRN</code>技术实际用处不大</p>
<h4 id="3-Overall-architecture"><a href="#3-Overall-architecture" class="headerlink" title="3. Overall architecture"></a>3. Overall architecture</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091538588.png" alt="image-20230509153832557"></p>
<ul>
<li>首先输入的是一张<code>224x224x3</code>（因为是彩色&#96;RGB三通道的图）</li>
<li>第一层用的卷积核的大小是 <code>11∗11∗3 </code>，卷积核的个数是<code>48+48=96</code>，从这一层开始两个<code>GPU</code>开始分开运行，现在定义处理上半层特征图的叫<code>GPU_A</code>，处理下半层特征图的叫<code>GPU_B</code>，每个<code>GPU</code>负责48个卷积核的运算，上半层<code>GPU_A</code>生成48张特征图，下半层<code>GPU_B</code>生成48张特征图。这一层卷积结束之后，还需要<code>LRN</code>（<code>Local Response Normalization </code>局部响应归一化）和<code>Max_Pooling</code>（最大池化）</li>
<li>第二层和第一层同理，两个<code>GPU</code>分别处理自己上一层传来的<code>output</code>（那48张特征图），卷积核的大小是 <code>5∗5∗48</code> ，然后一共有<code>128+128=256</code>个卷积核，所以两个<code>GPU</code>各自利用自己上一层的<code>output</code>生成<code>128</code>张特征图。这一层的卷积结束之后还需要<code>LRN</code>（<code>Local Response Normalization </code>局部响应归一化）和<code>Max_Pooling</code>（最大池化）</li>
<li>第三层和前两层不同，这一层两个<code>GPU</code>都要是将两个<code>GPU</code>的上一层的全部输出<code>output</code>作为输入<code>input</code>，所以这一层的卷积核大小是 <code>3∗3∗ （128[来自GPU_A]+128[来自GPU_B]）</code>，也就是这层的卷积核是 <code>3∗3∗256 </code>，而不是像前两层那样只是把自己上一层的输出当成输入，这层一共有<code>192+192=384</code>个卷积核，<code>GPU_A</code>负责前192个卷积核的生成的特征图，<code>GPU_B</code>负责后<code>192</code>个卷积核生成的特征图</li>
<li>第四层和第五层同第三层</li>
<li>第六层，接了一个全连接层<code>(FC)</code>，首先将<code>128[来自GPU_A]和128[来自GPU_B]</code>的一共256张特征图拉直成一个超长的向量，连接到一个大小为4096的全连接层中，其中4096个神经元的前2048个神经元由<code>GPU_A</code>运算，后2048个神经元由<code>GPU_B</code>来运算</li>
<li>第七层和第六层同理</li>
<li>第八层是再连接到一个大小为1000的全连接层中，用softmax，来算1000种分类的分布</li>
</ul>
<h5 id="（1）pytorch代码实现"><a href="#（1）pytorch代码实现" class="headerlink" title="（1）pytorch代码实现"></a>（1）pytorch代码实现</h5><p><strong><code>AlexNet.py</code></strong>,需要注意的是，目前网络全是在一块GPU上进行加速运算的，因此与原来的架构不一样</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    搭建AlexNet网络模型</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.搭建网络模型</span></span><br><span class="line"><span class="string">        # 输入为 224*224*3</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">class <span class="title function_">AlexNet</span><span class="params">(nn.Module)</span>:</span><br><span class="line">    def __<span class="title function_">init__</span><span class="params">(self, num_classes: <span class="type">int</span> = <span class="number">100</span>, dropout: <span class="type">float</span> = <span class="number">0.5</span>)</span> -&gt; None:</span><br><span class="line">        <span class="title function_">super</span><span class="params">()</span>.__<span class="title function_">init__</span><span class="params">()</span></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            # Layer1</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.LocalResponseNorm(size=<span class="number">5</span>,alpha=<span class="number">10e-4</span>,beta=<span class="number">0.75</span>,k=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            # Layer2</span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.LocalResponseNorm(size=<span class="number">5</span>, alpha=<span class="number">10e-4</span>, beta=<span class="number">0.75</span>, k=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            # Layer3</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Layer4</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Layer5</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), # <span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span></span><br><span class="line">        )</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            # Linear1</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Linear2</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Linear3</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x= self.flatten(x)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta"># <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span></span><br><span class="line"><span class="meta">#     x = torch.rand([1, 3, 224, 224])</span></span><br><span class="line"><span class="meta">#     model = AlexNet()</span></span><br><span class="line"><span class="meta">#     y = model(x)</span></span><br><span class="line"><span class="meta">#     print(y.shape)</span></span><br></pre></td></tr></table></figure>



<h4 id="4-Data-Augmentation-数据增强"><a href="#4-Data-Augmentation-数据增强" class="headerlink" title="4. Data Augmentation(数据增强)"></a>4. Data Augmentation(数据增强)</h4><p>论文在训练阶段使用两种数据增强的方式减少数据的过拟合，都允许用很少的计算从原始图像生成转换</p>
<h5 id="（1）第一种"><a href="#（1）第一种" class="headerlink" title="（1）第一种"></a>（1）第一种</h5><p>从<code>256x256</code>的图像中随机扣下<code>224x224</code>大小的图片，并进行随机的水平翻转，这样相当于将数据增加了<code>2048倍(32x32x2)</code></p>
<ul>
<li>（数据保证符合网络期望的输入数据）将短边减少到256，长边也保证高宽比往下降，长边多出来的以中心为界将两个边进行裁剪在第二节 <code>2  The Dataset</code>中提到过，<code>ImageNet</code>是一个可变分辨率的数据集因此，</li>
</ul>
<p>实现代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line">    </span><br><span class="line"># 标准化所求的数据集的均值与方差</span><br><span class="line">norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    </span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">256</span>)),      # (<span class="number">256</span>, <span class="number">256</span>) 区别 按照长宽比进行缩放</span><br><span class="line">        transforms.CenterCrop(<span class="number">256</span>),    # 将长边多余的地方进行裁剪</span><br><span class="line">        transforms.RandomCrop(<span class="number">224</span>),    # 随机裁剪<span class="number">224</span>*<span class="number">224</span></span><br><span class="line">        transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),  # 以<span class="number">50</span>%的概率进行水平翻转</span><br><span class="line">        transforms.ToTensor(),                   # 转变为tensor()数据</span><br><span class="line">        transforms.Normalize(norm_mean, norm_std),  # 标准化</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>



<h5 id="（2）第二种"><a href="#（2）第二种" class="headerlink" title="（2）第二种"></a>（2）第二种</h5><p>第二种方法改变训练图像中<code>RGB</code>通道的强度，对整个<code>ImageNet</code>训练集的<code>RGB</code>像素值集执行<code>PCA</code>主成分分析,然后对主成分上的数进行微小的扰动，以此<strong>图像色彩</strong>就会发生微小的变化，增加图像的丰富性多样性 </p>
<p><strong>暂时不清楚如何对其进行操作…..</strong>,在<code>AlexNet</code>实现时候效果有限</p>
<p>同时在测试阶段也有对数据进行的操作：</p>
<h5 id="（3）测试阶段数据处理"><a href="#（3）测试阶段数据处理" class="headerlink" title="（3）测试阶段数据处理"></a>（3）测试阶段数据处理</h5><p>在测试时，网络通过提取<code>5个224 × 224</code>的patch(四个角斑和中心斑)及其水平反射(共10个patch)进行预测，并将网络的<code>softmax</code>层对这10个patch的预测取平均</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">valid_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">        transforms.TenCrop(<span class="number">224</span>, vertical_flip=False),</span><br><span class="line">        transforms.Lambda(lambda crops: torch.<span class="built_in">stack</span>([normalizes(transforms.ToTensor()(crop)) <span class="keyword">for</span> crop in crops])),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>



<h4 id="5-Dropout"><a href="#5-Dropout" class="headerlink" title="5. Dropout"></a>5. Dropout</h4><p>在网络架构的前两个全链接层后添加<code>Dropout</code>，防止了过拟合（论文作者最开始的理解是<code>Dropout</code>是做模型融合，实际上是在正则化，之后本文作者写了一篇<code>JMLR</code>文章说明Dropout实际等价一个<code>L2</code>的正则,使用<code>Dopout</code></p>
<p><strong>可以提高模型的泛化性</strong></p>
<h4 id="6-Details-of-learning"><a href="#6-Details-of-learning" class="headerlink" title="6. Details of learning"></a>6. Details of learning</h4><p>在论文中使用<code>SGD</code>随机梯度下降法作为优化函数进行权重参数优化,其中<code>dataloader</code>中的<code>batch_size = 128</code>,<code>momentum = 0.9</code> ,<code>weight decay = 0.0005</code></p>
<p>训练细节：</p>
<ul>
<li>权重参数初始化，标准差&#x3D;0.01  均值&#x3D;0   的高斯正太分布</li>
<li>有关学习率的调整，所有层的学习率相同，但是在验证的正确率随着当前学习率停止提高时，将学习率除以10继续训练；学习率<code>learn_rate = 0.01</code>  初始值</li>
<li>训练拟合时，第一层卷积的可视化，也需要进行演示</li>
</ul>
<h3 id="三、实现"><a href="#三、实现" class="headerlink" title="三、实现"></a>三、实现</h3><h5 id="1-模型训练"><a href="#1-模型训练" class="headerlink" title="1.模型训练"></a>1.模型训练</h5><p>模型使用<code>Mini-ImageNet</code>数据集对网络模型进行训练拟合</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    对网络模型进行训练拟合，并且保存模型最好的验证正确率的参数权重</span></span><br><span class="line"><span class="string">        训练数据集 使用MINI-ImageNet</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line">from AlexNet import AlexNet</span><br><span class="line">from torch.optim import lr_scheduler</span><br><span class="line">import logging</span><br><span class="line">import colorlog</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.定义相关全局变量</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 当前Train.py文件所在的目录位置</span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 得到数据集所在文件目录</span><br><span class="line">data_dir = os.path.join(BASE_DIR,<span class="string">&quot;../Mini-ImageNet/&quot;</span>)</span><br><span class="line">train_data_dir = os.path.join(data_dir,<span class="string">&quot;./new_train&quot;</span>)</span><br><span class="line">test_data_dir = os.path.join(data_dir,<span class="string">&quot;./new_test&quot;</span>)</span><br><span class="line">num_classes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">MAX_EPOCH = <span class="number">1</span>    # 最大训练epoch</span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">classes = <span class="number">100</span></span><br><span class="line">start_epoch = <span class="number">-1</span></span><br><span class="line">log_interval = <span class="number">1</span></span><br><span class="line">val_interval = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># 设置<span class="built_in">log</span>输出--控制台输出并保存到文件中</span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">logger.setLevel(level=logging.INFO)</span><br><span class="line">handler = logging.FileHandler(<span class="string">&quot;../Log/log.txt&quot;</span>,mode=<span class="string">&#x27;w+&#x27;</span>)</span><br><span class="line">handler.setLevel(logging.INFO)</span><br><span class="line">formatter = logging.Formatter(<span class="string">&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;</span>)</span><br><span class="line">handler.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line">log_colors_config = &#123;</span><br><span class="line">    <span class="string">&#x27;INFO&#x27;</span>: <span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">console_formatter = colorlog.ColoredFormatter(log_colors=log_colors_config)</span><br><span class="line">console = logging.StreamHandler()</span><br><span class="line">console.setFormatter(console_formatter)</span><br><span class="line">console.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line"># 重复日志问题：</span><br><span class="line"># <span class="number">1</span>、防止多次addHandler；</span><br><span class="line"># <span class="number">2</span>、loggername 保证每次添加的时候不一样；</span><br><span class="line"># <span class="number">3</span>、显示完<span class="built_in">log</span>之后调用removeHandler</span><br><span class="line"><span class="keyword">if</span> not logger.handlers:</span><br><span class="line">    logger.addHandler(handler)</span><br><span class="line">    logger.addHandler(console)</span><br><span class="line"></span><br><span class="line">handler.close()</span><br><span class="line">console.close()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    3.加载训练以及验证数据集</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>)),      # (<span class="number">256</span>, <span class="number">256</span>) 区别</span><br><span class="line">    transforms.CenterCrop(<span class="number">256</span>),</span><br><span class="line">    transforms.RandomCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(norm_mean, norm_std),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">normalizes = transforms.Normalize(norm_mean, norm_std)</span><br><span class="line">valid_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">    transforms.TenCrop(<span class="number">224</span>, vertical_flip=False), #  一张图片会得到<span class="number">10</span>张图片 <span class="number">10</span>张图片会以<span class="built_in">list</span>形式存储</span><br><span class="line">    # 将<span class="built_in">list</span>中的图片依次去取出做normalizes()  torch.<span class="built_in">stack</span>就将<span class="number">10</span>张图片进行拼接得到一个<span class="number">4</span>D张量  [B  C  H  W]  B = <span class="number">10</span></span><br><span class="line">    transforms.Lambda(lambda crops: torch.<span class="built_in">stack</span>([normalizes(transforms.ToTensor()(crop)) <span class="keyword">for</span> crop in crops])),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 构建Dataset实例</span><br><span class="line">train_dataset = ImageFolder(train_data_dir, transform=train_transform)</span><br><span class="line">test_dataset = ImageFolder(test_data_dir, transform=valid_transform)</span><br><span class="line"></span><br><span class="line"># 构建DataLoder</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)</span><br><span class="line">valid_loader = DataLoader(dataset=test_dataset, batch_size=<span class="number">4</span>,shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    4.导入网络模型</span></span><br><span class="line"><span class="string">        配置损失函数</span></span><br><span class="line"><span class="string">        配置优化器</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 模型</span><br><span class="line">alexnet_model = AlexNet(num_classes=<span class="number">100</span>,dropout=<span class="number">0.5</span>)</span><br><span class="line">alexnet_model.to(device)</span><br><span class="line"></span><br><span class="line"># 损失函数</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 优化器 （对最后一层使用了softmax）--- 卷积层的学习率可以小一些  在线性层学习率可以大一些 --- trick</span><br><span class="line">flag = <span class="number">0</span></span><br><span class="line"><span class="meta"># flag = 1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="meta"># map() 会根据提供的函数对指定序列(可以迭代对象)做映射</span></span><br><span class="line">    <span class="meta"># id() 函数返回指定对象的唯一 id    id 是对象的内存地址</span></span><br><span class="line">    # 该模型有三个线性层  每个线性层对应 一个输入参数与权重的乘法  以及一个加法（偏置）对应六个id</span><br><span class="line">    fc_params_id = <span class="built_in">list</span>(<span class="built_in">map</span>(id, alexnet_model.classifier.parameters()))  # 返回的是parameters的 内存地址</span><br><span class="line">    # 如 lambda x: x ** <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]   x**<span class="number">2</span> 是函数表达式  x 参数  取值范围是  [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    <span class="meta"># lambda p: id(p) not in fc_params_id, alexnet_model.parameters()</span></span><br><span class="line">    # 得到的id是模型参数中 不属于 fc_params_id(线性层)列表中的id</span><br><span class="line">    <span class="meta"># filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表  filter(function, iterable)</span></span><br><span class="line">    base_params = filter(lambda p: id(p) not in fc_params_id, alexnet_model.parameters())</span><br><span class="line">    optimizer = optim.SGD([</span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: base_params, <span class="string">&#x27;lr&#x27;</span>: LR * <span class="number">0.1</span>&#125;,  # <span class="number">0</span></span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: alexnet_model.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: LR&#125;], momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=<span class="number">0.9</span>)  # 选择优化器</span><br><span class="line"></span><br><span class="line"># 学习率每隔<span class="number">10</span>轮变为原来的<span class="number">0.1</span></span><br><span class="line">lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    train_curve = <span class="built_in">list</span>()</span><br><span class="line">    train_ACC_curve = <span class="built_in">list</span>()</span><br><span class="line">    valid_curve = <span class="built_in">list</span>()</span><br><span class="line">    valid_ACC_curve = <span class="built_in">list</span>()</span><br><span class="line">    min_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch in range(start_epoch + <span class="number">1</span>, MAX_EPOCH):</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                5.训练网络</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        loss_mean = <span class="number">0.</span></span><br><span class="line">        correct = <span class="number">0.</span></span><br><span class="line">        total = <span class="number">0.</span></span><br><span class="line">        train_ACC = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">&quot;----------------Train: Epoch &#123;&#125;----------------------&quot;</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line">        alexnet_model.train()</span><br><span class="line">        <span class="keyword">for</span> i, data in enumerate(train_loader):</span><br><span class="line"></span><br><span class="line">            <span class="meta"># forward</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">            outputs = alexnet_model(inputs)</span><br><span class="line"></span><br><span class="line">            <span class="meta"># backward</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            <span class="meta"># update weights</span></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            # 统计分类情况</span><br><span class="line">            _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)   # 累加这一轮 每一批次的样本数量 = 这一轮的总样本数量</span><br><span class="line">            correct += (predicted == labels).squeeze().cpu().sum().numpy()  # 将该轮中每一个批次预测正确的样本数量进行累加</span><br><span class="line"></span><br><span class="line">            # 打印训练信息</span><br><span class="line">            loss_mean += loss.item()  # 将该轮中每一批次的损失进行累积  得到本轮的总损失</span><br><span class="line">            train_curve.append(loss.item())</span><br><span class="line">            train_ACC = correct / total</span><br><span class="line">            train_ACC_curve.append(train_ACC)</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % log_interval == <span class="number">0</span>:   # 在本轮训练中 当训练了log_interval的批次时名就打印一次训练信息</span><br><span class="line">                loss_mean = loss_mean / log_interval</span><br><span class="line"></span><br><span class="line">                logger.info(</span><br><span class="line">                    <span class="string">&quot;Training:Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Train_Loss: &#123;:.4f&#125; Train_Acc:&#123;:.2%&#125;&quot;</span>.format(</span><br><span class="line">                        epoch + <span class="number">1</span>, MAX_EPOCH, i + <span class="number">1</span>, len(train_loader), loss_mean, train_ACC)</span><br><span class="line">                )</span><br><span class="line">                loss_mean = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        lr_scheduler.step()  # 更新学习率</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">            6.验证网络</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % val_interval == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            correct_val = <span class="number">0.</span></span><br><span class="line">            total_val = <span class="number">0.</span></span><br><span class="line">            loss_val = <span class="number">0.</span></span><br><span class="line">            Valid_Acc = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">            logger.info(<span class="string">&quot;----------------Valid: Epoch &#123;&#125;----------------------&quot;</span>.format(epoch + <span class="number">1</span>))</span><br><span class="line">            alexnet_model.eval()</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> j, data in enumerate(valid_loader):</span><br><span class="line">                    inputs, labels = data</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">                    bs, ncrops, c, h, w = inputs.size()  # [<span class="number">4</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>]</span><br><span class="line">                    outputs = alexnet_model(inputs.view(<span class="number">-1</span>, c, h, w))  # [<span class="number">40</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>]</span><br><span class="line">                    # 论文中对于验证的相关操作 --- 对于网络的softmax层输出的<span class="number">10</span>个patch 预测取平均</span><br><span class="line">                    <span class="meta"># outputs.view(bs, ncrops, -1)  [4,10,100]</span></span><br><span class="line">                    <span class="meta"># torch.mean(x,dim)  dim表示对于输入x的那一个维度求平均 [bs,ncrops,100].mean(1) 对dim=1求平均</span></span><br><span class="line">                    outputs_avg = outputs.view(bs, ncrops, <span class="number">-1</span>).mean(<span class="number">1</span>)  #  outputs_avg.shape = [<span class="number">4</span>,<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">                    loss = criterion(outputs_avg, labels)</span><br><span class="line"></span><br><span class="line">                    _, predicted = torch.max(outputs_avg.data, <span class="number">1</span>)  # 该批次验证预测的结果</span><br><span class="line">                    total_val += labels.size(<span class="number">0</span>)  # 本轮累积批次验证的样本总数</span><br><span class="line">                    correct_val += (predicted == labels).squeeze().cpu().sum().numpy()  #  本轮累积批次验证正确的样本数</span><br><span class="line"></span><br><span class="line">                    loss_val += loss.item()</span><br><span class="line"></span><br><span class="line">                loss_val_mean = loss_val / len(valid_loader)  # 本轮验证 每一个批次的平均损失</span><br><span class="line">                valid_curve.append(loss_val_mean)</span><br><span class="line">                Valid_Acc = correct_val/total_val  # 本轮的平均正确率</span><br><span class="line">                valid_ACC_curve.append(Valid_Acc)</span><br><span class="line"></span><br><span class="line">                logger.info(</span><br><span class="line">                    <span class="string">&quot;Valid:\t Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Valid_Loss: &#123;:.4f&#125; Valid_Acc:&#123;:.2%&#125;&quot;</span>.format(</span><br><span class="line">                        epoch+<span class="number">1</span>, MAX_EPOCH, j + <span class="number">1</span>, len(valid_loader), loss_val_mean, Valid_Acc)</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                    7.保存网络模型</span></span><br><span class="line"><span class="string">                &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">                <span class="keyword">if</span> Valid_Acc &gt; min_acc:</span><br><span class="line">                    folder = <span class="string">&#x27;../Models&#x27;</span></span><br><span class="line">                    <span class="keyword">if</span> not os.path.exists(folder):  # 当前目录不存在则进行创建</span><br><span class="line">                        os.mkdir(folder)</span><br><span class="line">                    min_acc = Valid_Acc</span><br><span class="line">                    logger.info(<span class="string">&quot;save best model Epoch : &#123;&#125;&quot;</span>.format(epoch + <span class="number">1</span>))</span><br><span class="line">                    # 保存权重文件</span><br><span class="line">                    torch.save(alexnet_model.state_dict(), <span class="string">&#x27;../Models/best_model_AlexNet.pth&#x27;</span>)</span><br><span class="line">                # 保存最后一轮的权重文件</span><br><span class="line">                <span class="keyword">if</span> epoch+<span class="number">1</span> == MAX_EPOCH:</span><br><span class="line">                    torch.save(alexnet_model.state_dict(), <span class="string">&#x27;../Models/last_model_AlexNet.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            alexnet_model.train()</span><br><span class="line">            logger.info(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        7.结果可视化</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    train_x = range(len(train_curve))</span><br><span class="line">    train_y = train_curve</span><br><span class="line">    train_acc_x = range(len(train_ACC_curve))</span><br><span class="line">    train_acc_y = train_ACC_curve</span><br><span class="line"></span><br><span class="line">    train_iters = len(train_loader)</span><br><span class="line">    #  由于valid中记录的是epoch_loss，需要对记录点进行转换到iterations</span><br><span class="line">    valid_x = np.arange(<span class="number">1</span>, len(valid_curve) + <span class="number">1</span>) * train_iters * val_interval</span><br><span class="line">    valid_y = valid_curve</span><br><span class="line">    valid_acc_x = np.arange(<span class="number">1</span>, len(valid_ACC_curve) + <span class="number">1</span>) * train_iters * val_interval</span><br><span class="line">    valid_acc_y = valid_ACC_curve</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(train_x, train_y, label=<span class="string">&#x27;Train_loss&#x27;</span>)</span><br><span class="line">    plt.plot(valid_x, valid_y, label=<span class="string">&#x27;Valid_loss&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss value&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(train_acc_x, train_acc_y, label=<span class="string">&#x27;Train_acc&#x27;</span>)</span><br><span class="line">    plt.plot(valid_acc_x, valid_acc_y, label=<span class="string">&#x27;Valid_acc&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;acc value&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and Validation acc&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">&#x27;End....&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>训练结束之后，会将最优以及最后训练的模型进行保存，此外还会将训练的日志在控制台输出以及存储至<code>log.txt</code>文件中</p>
<p>最后会得到<code>训练损失vs验证损失</code>  以及  <code>训练正确率vs验证正确率</code>的可视化曲线</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121048551.png" alt="image-20230512104741717"></p>
<p>训练得到的结果出现过拟合的现象，大概在迭代的10000次（10000&#x2F;282&#x3D;35）轮左右出现过拟合….</p>
<p>训练损失在降低，但是验证损失在增加，而且此时的测试正确率不再有变化…</p>
<h5 id="（1）过拟合解决方法"><a href="#（1）过拟合解决方法" class="headerlink" title="（1）过拟合解决方法"></a>（1）过拟合解决方法</h5><p>过拟合出现的主要原因是因为：<strong>数据太少+模型太复杂</strong></p>
<ul>
<li>增加数据量<ul>
<li>多收集数据集，扩大数据集的量</li>
<li>数据增强（通过图片的旋转、平移、亮度、切割），增加数据的多样性</li>
</ul>
</li>
<li>正则化方法<ul>
<li><code>L1</code>正则、<code>L2</code>正则（使得某些权重<code>w</code>不会过大）</li>
<li>Dropout</li>
</ul>
</li>
<li>多模型组合</li>
<li>贝叶斯方法</li>
</ul>
<h5 id="（2）Pytorch实现正则化"><a href="#（2）Pytorch实现正则化" class="headerlink" title="（2）Pytorch实现正则化"></a>（2）Pytorch实现正则化</h5><p>在<code>pytorch</code>中进行<code>L2</code>正则化，最直接的方式可以直接用优化器自带的<code>weight_decay</code>选项指定权值衰减率，相当于<code>L2</code>正则化中的<code>λ</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=<span class="number">0.9</span>,weight_decay=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>

<p>对上述模型加入<code>L2</code>的正则，并加载之前过拟合的训练的权重参数，发现其训练正确率在下降….</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121557715.png" alt="image-20230512155714678"></p>
<h4 id="2-模型测试"><a href="#2-模型测试" class="headerlink" title="2.模型测试"></a>2.模型测试</h4><p>测试集数据与验证集数据的数量是一致的均为<code>12000</code>张</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    用于测试模型的正确率</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line">from AlexNet import AlexNet</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.得到有关100分类标签的列表(按照正确的顺序)</span></span><br><span class="line"><span class="string">        参数 class_name.json的路径</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def <span class="title function_">get_classes_name</span><span class="params">(json_path)</span>:</span><br><span class="line">    classes_name_list = <span class="built_in">list</span>()</span><br><span class="line">    with <span class="title function_">open</span><span class="params">(json_path, <span class="string">&quot;r&quot;</span>)</span> as f:</span><br><span class="line">        class_names_dict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k,v in class_names_dict.items():</span><br><span class="line">        classes_name_list.append(v[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> classes_name_list</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    3.定义相关全局变量</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data_dir = os.path.join(BASE_DIR,<span class="string">&quot;../Mini-ImageNet/&quot;</span>)</span><br><span class="line">test_data_dir = os.path.join(data_dir,<span class="string">&quot;./new_test&quot;</span>)</span><br><span class="line"></span><br><span class="line">path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;../Models/best_model_AlexNet.pth&quot;</span>)</span><br><span class="line">num_classes=<span class="number">100</span></span><br><span class="line"></span><br><span class="line">classes_name = <span class="built_in">list</span>()   # 用于存储 Mini-ImageNet <span class="number">100</span>分类名字的列表</span><br><span class="line">classes_name_json_path = os.path.join(BASE_DIR, <span class="string">&quot;../Mini-ImageNet/classes_name.json&quot;</span>)  # 模型参数路径</span><br><span class="line">classes_name = get_classes_name(classes_name_json_path)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    4.测试数据集</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line">test_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>), #  一张图片会得到<span class="number">10</span>张图片 <span class="number">10</span>张图片会以<span class="built_in">list</span>形式存储</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(norm_mean,norm_std),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_dataset = ImageFolder(test_data_dir, transform=test_transform)</span><br><span class="line">test_loader = DataLoader(dataset=test_dataset, batch_size=<span class="number">1</span>,shuffle=True)  # batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    5.导入网络模型</span></span><br><span class="line"><span class="string">        加载模型参数</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">alexnet_model = AlexNet(num_classes=<span class="number">100</span>,dropout=<span class="number">0.5</span>)</span><br><span class="line">pretrained_state_dict = torch.load(path_state_dict)</span><br><span class="line">alexnet_model.load_state_dict(pretrained_state_dict)</span><br><span class="line">alexnet_model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    6.验证</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">alexnet_model.eval()</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    sum = len(test_dataset)</span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data in test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs,targets = imgs.to(device),targets.to(device)</span><br><span class="line"></span><br><span class="line">        output = alexnet_model(imgs)</span><br><span class="line">        _, pred = torch.max(output, axis=<span class="number">1</span>)</span><br><span class="line">        predicted_point = pred[<span class="number">0</span>].item()</span><br><span class="line">        <span class="keyword">if</span> predicted_point == <span class="type">int</span>(targets.item()):</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        predicted = classes_name[predicted_point]</span><br><span class="line">        actual = classes_name[<span class="type">int</span>(targets.item())]</span><br><span class="line">        print(f<span class="number">&#x27;</span>the predicted_point is <span class="string">&quot;&#123;predicted_point&#125; &quot;</span>predicted:<span class="string">&quot;&#123;predicted&#125;&quot;</span>, Actual:<span class="string">&quot;&#123;actual&#125;&quot;</span><span class="string">&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print(f&quot;the Number of samples is &#123;sum&#125;&quot;)</span></span><br><span class="line"><span class="string">print(f&quot;the Accuracy  is &#123;float(right/sum)*100&#125; %&quot;)</span></span><br></pre></td></tr></table></figure>

<p>测试结果与训练输出的曲线结果类似，测试正确率与验证正确率都是在 <code>50%</code> 左右（这是用过拟合的模型进行测试的）—- 第一次过拟合的模型放在<code>My_Proj/Models/model_overfitting</code>目录下</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121119943.png" alt="image-20230512111954917"></p>
<h4 id="3-卷积核的可视化"><a href="#3-卷积核的可视化" class="headerlink" title="3.卷积核的可视化"></a>3.卷积核的可视化</h4><p>在论文第五节中提到，将第一层的卷积核提取出来可以看到第一层96个卷积核（<code>GPU0  GPU1</code>）分别48个卷积核</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    训练拟合后对第一层的卷积核进行可视化</span></span><br><span class="line"><span class="string">        Web端 可视化的结果存储在 Visualization_Log目录下</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from PIL import Image</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">import torchvision.models as models</span><br><span class="line">import torchvision.utils as vutils</span><br><span class="line"></span><br><span class="line">from AlexNet import AlexNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.定义相关全局变量</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    log_dir = os.path.join(BASE_DIR, <span class="string">&quot;../Visualization_Log&quot;</span>)   #  用于存储 web 可视化结果的目录</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(log_dir=log_dir, filename_suffix=<span class="string">&quot;_kernel&quot;</span>)  #  创建对象  filename_suffix文件名后缀</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        3.导入模型以及预训练参数</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;../Models/best_model_AlexNet.pth&quot;</span> )</span><br><span class="line">    <span class="meta"># alexnet = models.alexnet()</span></span><br><span class="line">    alexnet  = AlexNet()</span><br><span class="line">    pretrained_state_dict = torch.load(path_state_dict)</span><br><span class="line">    alexnet.load_state_dict(pretrained_state_dict)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        4.卷积核可视化</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    kernel_num = <span class="number">-1</span></span><br><span class="line">    vis_max = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sub_module in alexnet.modules():  <span class="meta"># model.modules()迭代遍历模型的所有子层</span></span><br><span class="line">        <span class="keyword">if</span> not isinstance(sub_module, nn.Conv2d):   # 判断当前迭代的层是否是卷积层</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        kernel_num += <span class="number">1</span>  # 当前迭代的子层是卷积层</span><br><span class="line">        <span class="keyword">if</span> kernel_num &gt; vis_max:   # 大于需要可视化最大层数量时  <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        kernels = sub_module.weight</span><br><span class="line">        c_out, c_int, k_h, k_w = tuple(kernels.shape)  # c_out与下一层的特征映射图数量一致(卷积核的数量)  c_int卷积核的通道数</span><br><span class="line"></span><br><span class="line">        # 拆分channel---将单个卷积核心的每一个通道的图像可视化</span><br><span class="line">        <span class="keyword">for</span> o_idx in range(c_out):</span><br><span class="line">            kernel_idx = kernels[o_idx, :, :, :]  # 获得(C, h, w)</span><br><span class="line">            # make_grid需要 BCHW，这里拓展C维度变为（C，<span class="number">1</span>， h, w）  unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度</span><br><span class="line">            # 这样一个卷积核的 多个通道就被拆开了</span><br><span class="line">            kernel_idx = kernel_idx.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            kernel_grid = vutils.make_grid(kernel_idx,normalize=True, scale_each=True, nrow=<span class="number">8</span>) <span class="meta"># normalize=True 将值缩放值 [0-1] 之间</span></span><br><span class="line">            writer.add_image(<span class="string">&#x27;&#123;&#125;_Convlayer_split_in_channel&#x27;</span>.format(kernel_num), kernel_grid, global_step=o_idx)</span><br><span class="line"></span><br><span class="line">        # 将单个卷积核 进行可视化 第一层卷积核 有<span class="number">64</span>个</span><br><span class="line">        kernel_all = kernels.view(<span class="number">-1</span>, <span class="number">3</span>, k_h, k_w)</span><br><span class="line">        kernel_grid = vutils.make_grid(kernel_all, normalize=False, scale_each=True, nrow=<span class="number">8</span>)  <span class="meta"># c, h, w</span></span><br><span class="line">        writer.add_image(<span class="string">&#x27;&#123;&#125;_all&#x27;</span>.format(kernel_num), kernel_grid, global_step=<span class="number">620</span>)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">&quot;&#123;&#125;_convlayer shape:&#123;&#125;&quot;</span>.format(kernel_num, tuple(kernels.shape)))</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        5.可视化第一层卷积后的特征映射图</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    writer = SummaryWriter(log_dir=log_dir, filename_suffix=<span class="string">&quot;_feature map&quot;</span>)</span><br><span class="line"></span><br><span class="line">    # 输入数据</span><br><span class="line">    path_img = os.path.join(BASE_DIR, <span class="string">&quot;../../deep_eyes/A_alexnet/data/tiger cat.jpg&quot;</span>)</span><br><span class="line">    normMean = [<span class="number">0.49139968</span>, <span class="number">0.48215827</span>, <span class="number">0.44653124</span>]</span><br><span class="line">    normStd = [<span class="number">0.24703233</span>, <span class="number">0.24348505</span>, <span class="number">0.26158768</span>]</span><br><span class="line">    norm_transform = transforms.Normalize(normMean, normStd)</span><br><span class="line">    img_transforms = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        norm_transform</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    img_pil = Image.open(path_img).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    img_tensor = img_transforms(img_pil)</span><br><span class="line">    img_tensor.unsqueeze_(<span class="number">0</span>)  <span class="meta"># chw --&gt; bchw</span></span><br><span class="line"></span><br><span class="line">    # 前向传播</span><br><span class="line">    convlayer1 = alexnet.features[<span class="number">0</span>]   #  拿到模型的第一层卷积</span><br><span class="line">    fmap_1 = convlayer1(img_tensor)    # 得到特征映射图  shape = [<span class="number">1</span>,<span class="number">64</span>,<span class="number">55</span>,<span class="number">55</span>]</span><br><span class="line"></span><br><span class="line">    # 预处理 transpose方法的作用是交换矩阵的两个维度</span><br><span class="line">    fmap_1.transpose_(<span class="number">0</span>, <span class="number">1</span>)  <span class="meta"># bchw=(1, 64, 55, 55) --&gt; (64, 1, 55, 55)</span></span><br><span class="line">    fmap_1_grid = vutils.make_grid(fmap_1, normalize=False, scale_each=True, nrow=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    writer.add_image(<span class="string">&#x27;feature map in conv1&#x27;</span>, fmap_1_grid, global_step=<span class="number">620</span>)</span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure>

<p>在<code>pycharm</code>控制台的终端,进入存放web可视化文件的目录下<code>My_Proj/Visualization_Log</code>目录下</p>
<p>终端输入:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=./</span><br></pre></td></tr></table></figure>

<p>终端输出一个主机<code>host</code></p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121133692.png" alt="image-20230512113315661"></p>
<p>点击，会在web端的<code>0_all</code>出现可视化的第一层卷积核：</p>
<p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121135618.png" alt="image-20230512113525576"></p>
<p>后面几层卷积核，特征相对高级，很抽象。而且卷积核尺寸小，看不懂卷积学习的内容是什么</p>
<p>第一层卷积最为低级，而且卷积核尺寸为<code>11x11</code>，可以看到第一层卷积学习了图片数据的颜色、纹理、边缘这些较为低级的特征。</p>
<p>神经网络对数据的特征进行提取符合由低级至高级特征提取的过程</p>
<h3 id="四、结论"><a href="#四、结论" class="headerlink" title="四、结论"></a>四、结论</h3><h4 id="1-重点"><a href="#1-重点" class="headerlink" title="1.重点"></a>1.重点</h4><ul>
<li><code>AlexNet</code>本质是一个更大更深的<code>LeNet</code>,主要改进有,<code>Dropout</code>丢弃法、<code>ReLu</code>激活函数、<code>maxpooling</code>重叠池化；<code>ReLu</code>与<code>sigmoid</code>相比梯度更大，且<code>ReLu</code>在零点处一阶导更好（减缓梯度消失），<code>maxpooling</code>取最大池化，取最大值，输出梯度更大</li>
<li>局部响应标准化（<code>LRN</code>）有助于<code>AlexNet</code>泛化能力的提升，受真实的神经元<strong>侧抑制</strong>启发；但是在论文<code>VGG</code>说该方法作用不大，而且有更好的<code>Batch Normalization</code></li>
<li><code>PCA</code>对数据集图片的颜色进行扰动，对于模型的性能提升并不大，而且实现相对复杂，目前就不实现了…</li>
</ul>
<h4 id="2-启发性"><a href="#2-启发性" class="headerlink" title="2.启发性"></a>2.启发性</h4><ul>
<li>初始的<code>224x224x3</code>的图片经过五层卷积之后，最后会被展平成一个<code>256x6x6</code>的向量进入线性层，直到最后一个分类层（输出层）之前，向量长度为<code>4096</code>，则一张图片会表示为<code>4096</code>的维度，<strong>这个长度为4096的向量非常好的抓住了输入图片的语义信息。若两个图片最后的4096的向量的距离（欧几里德距离非常相近的话，那么这两张图片很有可能是同一个物体的图片）————-深度学习设计的网络可以通过中间的各种隐含层的操作将一张图片最后压缩为一个特征向量（知识的压缩），而这个向量可以很好的将中间的语义信息表示出来（变成了一个机器可以理解的东西）</strong>    （论文的6.1 Qualitative  Evaluations 中提及）</li>
<li>将神经网络在倒数第二层的输出拿出来，得到一个长向量。然后将每个图片均拿出来，然后给定一张图片看一下和我这个向量最近的图片是谁（欧几里德距离），<strong>如果两幅图像产生的特征激活向量具有小的欧几里德距离，我们可以说神经网络认为它们是相似的（注意原始的图像之间的距离是不相近的，但是通过神经网络提取得到的高级特征向量之后，欧几里德距离是相近的）</strong>—<strong>深度神经网络的图片训练出来的最后那个向量，在语义空间的表示特别好（非常好的特征）</strong>，相似的图片会将其放在一起</li>
<li><code>6.1 Qualitative  Evaluations </code>最后提出可以使用<code>AlexNet</code>做图像检索、图像聚类、图像编码，利用两个4096维实值向量之间的欧氏距离来计算相似度是低效的，但通过<strong>训练自动编码器</strong>将这些向量压缩成简短的二进制码可以提高效率。这将产生一种比对原始像素应用自动编码器好得多的图像检索方法，后者不使用图像标签，因此倾向于检索具有相似边缘模式的图像，无论它们在语义上是否相似。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://maxswordsman.github.io">Maxswordsman</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://maxswordsman.github.io/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/AlexNET/">https://maxswordsman.github.io/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/AlexNET/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://maxswordsman.github.io" target="_blank">Maxswordsman</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CNN/">CNN</a></div><div class="post_share"><div class="social-share" data-image="/image/head2.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pytorch/%E4%BD%BF%E7%94%A8numpy%E5%88%9B%E5%BB%BA%E4%B8%89%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="使用numpy创建三层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">使用numpy创建三层神经网络</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/MAKEFILE%EF%BC%884%EF%BC%89/" title="MAKEFILE（4）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">MAKEFILE（4）</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/head2.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Maxswordsman</div><div class="author-info__description">不颓好胜之心</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/maxswordsman"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/maxswordsman" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:2723937292@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://gitee.com/zhou-xuezhi" target="_blank" title="Gitee"><i class="iconfont  icon-gitee" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNET"><span class="toc-text">AlexNET</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-text">一、预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">1.网络可视化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.数据集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%AE%BA%E6%96%87%E7%BB%86%E8%8A%82"><span class="toc-text">二、论文细节</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-ReLu%E4%B8%8ETanh%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">1.ReLu与Tanh收敛速度的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81"><span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E7%BB%93%E6%9E%9C"><span class="toc-text">（2）结果</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-LRN"><span class="toc-text">2.LRN</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89Pyotrch%E4%B8%ADLRN%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">（1）Pyotrch中LRN的实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Overall-architecture"><span class="toc-text">3. Overall architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89pytorch%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">（1）pytorch代码实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Data-Augmentation-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-text">4. Data Augmentation(数据增强)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E7%AC%AC%E4%B8%80%E7%A7%8D"><span class="toc-text">（1）第一种</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E7%AC%AC%E4%BA%8C%E7%A7%8D"><span class="toc-text">（2）第二种</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-text">（3）测试阶段数据处理</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Dropout"><span class="toc-text">5. Dropout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-Details-of-learning"><span class="toc-text">6. Details of learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">三、实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text">1.模型训练</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E8%BF%87%E6%8B%9F%E5%90%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-text">（1）过拟合解决方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89Pytorch%E5%AE%9E%E7%8E%B0%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">（2）Pytorch实现正则化</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95"><span class="toc-text">2.模型测试</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">3.卷积核的可视化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%BB%93%E8%AE%BA"><span class="toc-text">四、结论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%87%8D%E7%82%B9"><span class="toc-text">1.重点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%90%AF%E5%8F%91%E6%80%A7"><span class="toc-text">2.启发性</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By Maxswordsman</div><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.<p><a target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a> &nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Coding-0cedbe?style=flat&logo=Codio" title="本站采用双线部署，联通线路托管于Coding"></a>&nbsp;<a target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script defer type="text/javascript" src="https://cdn1.tianli0.top/npm/sweetalert2@8.19.0/dist/sweetalert2.all.js"></script><script defer src="/js/lunar.js"></script><script defer src="/js/day.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>