<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hexo魔改教程</title>
      <link href="/2023/05/31/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Web/Hexo%E9%AD%94%E6%94%B9%E6%95%99%E7%A8%8B/"/>
      <url>/2023/05/31/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Web/Hexo%E9%AD%94%E6%94%B9%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="Hexo魔改教程"><a href="#Hexo魔改教程" class="headerlink" title="Hexo魔改教程"></a>Hexo魔改教程</h2><p><a href="https://www.fomal.cc/posts/e593433d.html">大佬教程点这里</a></p><p>每次加入新的文章之后，在本地的项目文件下（工作目录下）,<code>git bash</code>输入</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo cl &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure><h3 id="一、社交图标"><a href="#一、社交图标" class="headerlink" title="一、社交图标"></a>一、社交图标</h3><p><a href="https://www.wzhecnu.cn/2021/07/22/blog/hexo-02-zhu-ti-mei-hua/">魔改教程</a></p><p>第一：需要进行修改的是，将<code>icofont</code>下载的代码，复制到<code>&quot;source/css/iconfont.css&quot;</code></p><p>第二：在<code>_config.butterfly.yml</code>文件下进行的</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230603170455.png" alt="image-20230603170448249"></p><p>添加代码：<code>&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/iconfont.css&quot;&gt;</code></p><p> 第三：在<code>_config.butterfly.yml</code>文件的社交图标部分，添加你需要的代码</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230603170727.png" alt="image-20230603170727859"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iconfont  icon-gitee: https:<span class="comment">//gitee.com/zhou-xuezhi || Gitee || &#x27;#24292e&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git使用教程</title>
      <link href="/2023/05/31/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Git/Git%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2023/05/31/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Git/Git%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="Git使用教程"><a href="#Git使用教程" class="headerlink" title="Git使用教程"></a>Git使用教程</h2><p><a href="https://mp.weixin.qq.com/s/Bf7uVhGiu47uOELjmC5uXQ">文档来源</a></p><h3 id="一、配置"><a href="#一、配置" class="headerlink" title="一、配置"></a>一、配置</h3><h4 id="1-Git配置"><a href="#1-Git配置" class="headerlink" title="1.Git配置"></a>1.Git配置</h4><h5 id="（1）所有配置（系统-用户）"><a href="#（1）所有配置（系统-用户）" class="headerlink" title="（1）所有配置（系统\用户）"></a>（1）所有配置（系统\用户）</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git cnfig -l</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531105332.png" alt="image-20230531105325132"></p><h5 id="（2）系统配置"><a href="#（2）系统配置" class="headerlink" title="（2）系统配置"></a>（2）系统配置</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --system --<span class="built_in">list</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531105519.png" alt="image-20230531105519567"></p><h5 id="（3）当前用户配置"><a href="#（3）当前用户配置" class="headerlink" title="（3）当前用户配置"></a>（3）当前用户配置</h5><p>这是必须配置的（相当于告诉github你的身份，用户标识）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global  --<span class="built_in">list</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531105604.png"></p><p><strong>设置用户名以及邮箱</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;kuangshen&quot;</span>  #名称</span><br><span class="line">git config --global user.email <span class="number">24736743</span>@qq.com   #邮箱</span><br></pre></td></tr></table></figure><p>当你安装Git后首先要做的事情是设置你的用户名称和e-mail地址。这是非常重要的，因为每次Git提交都会使用该信息。它被永远的嵌入到了你的提交中</p><h5 id="（4）git相关配置文件"><a href="#（4）git相关配置文件" class="headerlink" title="（4）git相关配置文件"></a>（4）git相关配置文件</h5><p><strong>所有的配置文件，其实都保存在本地</strong></p><ul><li><p><code>Git\etc\gitconfog</code> :该目录位于 Git安装目录下 —-为系统级（System）别的配置文件</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531111253.png" alt="image-20230531111253609"></p></li><li><p>C:\Users\PC(Administrator)\ .gitconfig    只适用于当前登录用户的配置  –global 全局</p></li></ul><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531110928.png" alt="image-20230531110928554"></p><p>可以将用户本地文件进行清空，然后重新按照<code>设置用户名以及邮箱</code>的指令进行设置</p><h3 id="二、GIT相关理论"><a href="#二、GIT相关理论" class="headerlink" title="二、GIT相关理论"></a>二、GIT相关理论</h3><p>Git本地有三个工作区域：**工作目录（Working Directory）、暂存区(Stage&#x2F;Index)、资源库(Repository或Git Directory)<strong>。如果在加上远程的</strong>git仓库(Remote Directory)**就可以分为四个工作区域。文件在这四个区域之间的转换关系如下：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531111613.png" alt="image-20230531111613688"></p><ul><li><p>Workspace：工作区，就是你平时存放项目代码的地方（<strong>一般是本地的项目目录</strong>）</p></li><li><p>Index &#x2F; Stage：暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息</p></li><li><p>Repository：仓库区（或本地仓库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本</p></li><li><p>Remote：远程仓库（<code>Github/Gitee</code>），托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换</p></li></ul><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531112426.png" alt="image-20230531112426685"></p><ul><li>Directory：使用Git管理的一个目录，也就是一个仓库，包含我们的工作空间和Git的管理空间。</li><li><code>WorkSpace</code>：需要通过Git进行版本控制的目录和文件，这些目录和文件组成了工作空间。</li><li>.git：存放Git管理信息的目录，<strong>初始化仓库的时候自动创建</strong>。(隐藏文件夹)</li><li>Index&#x2F;Stage：暂存区，或者叫待提交更新区，在提交进入<code>repo</code>之前，我们可以把所有的更新放在暂存区。</li><li><code>Local Repo</code>：本地仓库，一个存放在本地的版本库；HEAD会只是当前的开发分支（branch）。</li><li>Stash：隐藏，是一个工作状态保存栈，用于保存&#x2F;恢复<code>WorkSpace</code>中的临时状态。</li></ul><h4 id="1-工作流程"><a href="#1-工作流程" class="headerlink" title="1.工作流程"></a>1.工作流程</h4><p>git的工作流程一般是这样的：</p><p>１、在工作目录中添加、修改文件；(本地的工作目录)</p><p>２、将需要进行版本管理的文件放入暂存区域；（<code>git add .</code>）</p><p>３、将暂存区域的文件提交到(<strong>本地</strong>)git仓库。(<code>git commit</code>)</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531112950.png" alt="image-20230531112950463"></p><p>如上图如果最后需要推送到远程的 <code>github</code> 或者 <code>gitee</code> 仓库需要 <code>pull</code></p><h3 id="三、Git项目搭建"><a href="#三、Git项目搭建" class="headerlink" title="三、Git项目搭建"></a>三、Git项目搭建</h3><p>工作目录（<code>WorkSpace</code>)一般就是你希望Git帮助你管理的文件夹，可以是你项目的目录，也可以是一个空目录，<strong>建议不要有中文</strong></p><p>日常使用只要记住下图6个命令：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531121824.png" alt="image-20230531121824834"></p><h4 id="1-本地仓库搭建"><a href="#1-本地仓库搭建" class="headerlink" title="1.本地仓库搭建"></a>1.本地仓库搭建</h4><p>创建本地仓库的方法有两种：一种是创建全新的仓库，另一种是克隆远程仓库</p><h5 id="（1）新仓库"><a href="#（1）新仓库" class="headerlink" title="（1）新仓库"></a>（1）新仓库</h5><p>在项目目录下，<code>git bash</code>输入</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 在当前目录新建一个Git代码库</span><br><span class="line">git init</span><br></pre></td></tr></table></figure><p>执行后可以看到，仅仅在项目目录多出了一个.git目录，关于版本等的所有信息都在这个目录里面（隐藏文件夹）</p><h5 id="（2）克隆远程仓库"><a href="#（2）克隆远程仓库" class="headerlink" title="（2）克隆远程仓库"></a>（2）克隆远程仓库</h5><p>将远程服务器上的仓库完全镜像一份至本地</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 克隆一个项目和它的整个代码历史(版本信息)</span><br><span class="line">git clone [url]  <span class="meta"># https:<span class="comment">//gitee.com/kuangstudy/openclass.git</span></span></span><br></pre></td></tr></table></figure><h4 id="2-Git文件操作"><a href="#2-Git文件操作" class="headerlink" title="2.Git文件操作"></a>2.Git文件操作</h4><p><strong>文件的四种状态</strong></p><p>版本控制就是对文件的版本控制，要对文件进行修改、提交等操作，首先要知道文件当前在什么状态，不然可能会提交了现在还不想提交的文件，或者要提交的文件没提交上</p><ul><li><code>Untracked</code>: 未跟踪, 此文件在文件夹中, 但并没有加入到git库, 不参与版本控制. 通过<code>git add</code> 状态变为<code>Staged</code></li><li><code>Unmodify</code>: 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为<code>Modified</code>. 如果使用<code>git rm</code>移出版本库, 则成为<code>Untracked</code>文件</li><li><code>Modified</code>: 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过<code>git add</code>可进入暂存<code>staged</code>状态, 使用<code>git checkout</code> 则丢弃修改过, 返回到<code>unmodify</code>状态, 这个<code>git checkout</code>即从库中取出文件, 覆盖当前修改</li></ul><ul><li><code>Staged</code>: 暂存状态. 执行<code>git commit</code>则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为<code>Unmodify</code>状态. 执行<code>git reset HEAD filename</code>取消暂存, 文件状态为<code>Modified</code></li></ul><h5 id="（1）操作"><a href="#（1）操作" class="headerlink" title="（1）操作"></a>（1）操作</h5><p>在本地项目目录下,<code>git bash</code>,输入</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531123649.png" alt="image-20230531123649256"></p><p>当前目录下为空，没有文件待提交</p><p>在目录下创建一个<code>hello.txt</code>,使用<code>git status</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531123852.png" alt="image-20230531123852493"></p><p>出现一个没有被跟踪（<code>Unstracked</code>）的文件</p><p>使用<code>git add .</code>将文件添加至暂存区里面</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531124052.png" alt="image-20230531124052028"></p><p>在使用<code>git status</code>,出现</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531124126.png" alt="image-20230531124126932"></p><p>存在一个文件，为<strong>待提交</strong>的状态了，暂存去有一个新文件了</p><p>提交暂存区的内容到，本地的<code>git</code>仓库,输入<code>git commit -m &quot;消息内容&quot;</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">&quot;new file hello.txt&quot;</span>   # -m 表示提交信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531124729.png" alt="image-20230531124728984"></p><p>使用<code>git status</code>,继续查看当前状态</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531124814.png" alt="image-20230531124814244"></p><p>没有文件待提交</p><p><strong>总结</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#查看指定文件状态</span><br><span class="line">git status [filename]</span><br><span class="line"></span><br><span class="line">#查看所有文件状态</span><br><span class="line">git status</span><br><span class="line"></span><br><span class="line"><span class="meta"># git add .                  添加所有文件到暂存区</span></span><br><span class="line"><span class="meta"># git commit -m <span class="string">&quot;消息内容&quot;</span>    提交暂存区中的内容到本地仓库 -m 提交信息</span></span><br></pre></td></tr></table></figure><h4 id="3-忽略文件"><a href="#3-忽略文件" class="headerlink" title="3.忽略文件"></a>3.忽略文件</h4><p>有些时候我们不想把某些文件纳入版本控制中，比如数据库文件，临时文件，设计文件等</p><p>在主目录下建立<code>.gitignore</code>文件，此文件有如下规则：</p><ul><li><p>忽略文件中的空行或以井号（#）开始的行将会被忽略。</p></li><li><p>可以使用Linux通配符。例如：星号<code>（*）</code>代表任意多个字符，问号<code>（？）</code>代表一个字符，方括号<code>（[abc]）</code>代表可选字符范围，大括号<code>（&#123;string1,string2,...&#125;）</code>代表可选的字符串等。</p></li><li><p>如果名称的最前面有一个感叹号<code>（!）</code>，表示例外规则，将不被忽略。</p></li><li><p>如果名称的最前面是一个路径分隔符<code>（/）</code>，表示要忽略的文件在此目录下，而子目录中的文件不忽略。</p></li><li><p>如果名称的最后面是一个路径分隔符<code>（/）</code>，表示要忽略的是此目录下该名称的子目录，而非文件（默认文件或目录都忽略）。</p></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#为注释</span><br><span class="line">*.txt        #忽略所有 .txt结尾的文件,这样的话上传就不会被选中！</span><br><span class="line">!lib.txt     #但lib.txt除外</span><br><span class="line">/temp        #仅忽略项目根目录下的TODO文件,不包括其它目录temp</span><br><span class="line">build/       #忽略build/目录下的所有文件</span><br><span class="line">doc<span class="comment">/*.txt    #会忽略 doc/notes.txt 但不包括 doc/server/arch.t</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531125831.png" alt="image-20230531125831364"></p><h4 id="4-使用gitee进行托管"><a href="#4-使用gitee进行托管" class="headerlink" title="4.使用gitee进行托管"></a>4.使用<code>gitee</code>进行托管</h4><h5 id="（1）本机绑定SSH公钥"><a href="#（1）本机绑定SSH公钥" class="headerlink" title="（1）本机绑定SSH公钥"></a>（1）本机绑定SSH公钥</h5><p>设置本机绑定SSH公钥，实现免密码登录</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 进入 C:\Users\Administrator\.ssh 目录</span><br><span class="line"># 生成公钥</span><br><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><p>若你的机器上面已经有了<code>SSH-Key</code>,你可以继续创建 只要重新设置不同的公钥文件名就可以不将之前的覆盖了</p><p>下图将名字设置为了 <code>gitee_lab.rsa</code>不至于覆盖值的rsa文件，密码可以不进行设置，否则每次进行<code>git</code>提交都需要输入密码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 进入 C:\Users\Administrator\.ssh 目录</span><br><span class="line"># 生成公钥</span><br><span class="line"> ssh-keygen -t rsa -C <span class="string">&quot;XXXXX@XXX.com&quot;</span>    #用户邮箱</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531140441.png" alt="image-20230531140441285"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531140941.png" alt="image-20230531140941636"></p><p><a href="https://blog.csdn.net/jiangyu1013/article/details/103559435">参考</a></p><p>在<code>Gitee</code>上面进行公钥绑定</p><p>设置—&gt;安全设置—-&gt;SSH公钥</p><p>将上述的<code>gitee_lab.rsa.pub</code>用记事本打开，将其复制到SHH公钥的文本框内</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531141207.png" alt="image-20230531141207971"></p><p>也可以不进行重新创建别的<code>SSH-Key</code>,使用一个<code>SSH-Key</code>同时链接<code>github以及gitee</code>.<a href="https://blog.csdn.net/weixin_43197640/article/details/113465872">参考</a></p><p>在<code>gitee</code>添加公钥之后，在<code>.ssh</code>文件下创建<code>1.txt</code>文件在将整个文件包含<code>txt</code>重命名为<code>config</code>文件，写入以下内容：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531152450.png" alt="image-20230531152450832"></p><p>使用</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@gitee.com   # 查看是否链接成功</span><br></pre></td></tr></table></figure><p><strong>在<code>Gitee</code>进行仓库创建</strong></p><h5 id="（2）上传本地文件"><a href="#（2）上传本地文件" class="headerlink" title="（2）上传本地文件"></a>（2）上传本地文件</h5><p>在本地创建一个工作目录（项目文件目录），在目录中打开<code>git bash</code></p><p>将当前文件夹初始化为git本地仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>将本地仓库与远程创建的<code>gitee</code>仓库建立链接</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># git remote add origin &#123;仓库地址&#125;</span></span><br><span class="line">git remote add origin https:<span class="comment">//gitee.com/zhou-xuezhi/test.git</span></span><br></pre></td></tr></table></figure><p>将码云上的仓库pull到本地文件夹</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><p>对本地工作目录，进行添加你需要修改的文件</p><p>将当前文件夹添加至暂存区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><p>将暂存的文件提交到本地仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m ‘第一次提交’</span><br></pre></td></tr></table></figure><p>将本地仓库的文件推送到远程仓库中</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><p><strong>注意</strong></p><p>如果本地工作目录（项目目录）中的文件有修改，只需要进行本地项目目录<code>git bash</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add *</span><br><span class="line">git commit -m <span class="string">&#x27;再次提交&#x27;</span></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p><strong>注意：需要在gitee的个人管理中的邮箱管理添加你的个人邮箱，gitee才可以进行代码托管</strong></p><p>之后在将本地仓库中的文件推送到远程仓库中时，会弹出以下界面：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531154239.png" alt="image-20230531154239888"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230531154258.png" alt="image-20230531154258578"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Username <span class="keyword">for</span> ‘https:<span class="comment">//gitee.com’: 输入的是gitee上你的邮箱账号,</span></span><br><span class="line">Password <span class="keyword">for</span> ‘https:<span class="comment">//gitee.com’’: 输入gitee的登录密码</span></span><br></pre></td></tr></table></figure><h4 id="5-使用github进行托管"><a href="#5-使用github进行托管" class="headerlink" title="5.使用github进行托管"></a>5.使用<code>github</code>进行托管</h4><p>在<code>github</code>上面创建一个<code>CLion_Cracking</code>的仓库</p><p>新建一个<code>README.md</code>文件</p><p>使用以下命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br><span class="line">git branch -M main</span><br><span class="line">git remote add origin git@github.com:maxswordsman/CLion_Crakcing.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><h3 id="三、git分支"><a href="#三、git分支" class="headerlink" title="三、git分支"></a>三、git分支</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 列出所有本地分支</span><br><span class="line">git branch</span><br><span class="line">    </span><br><span class="line"># 重命名分支</span><br><span class="line">git branch -M &#123;branch_name&#125;</span><br><span class="line"></span><br><span class="line"># 列出所有远程分支</span><br><span class="line">git branch -r</span><br><span class="line"></span><br><span class="line"># 新建一个分支，但依然停留在当前分支</span><br><span class="line">git branch [branch-name]</span><br><span class="line"></span><br><span class="line"># 新建一个分支，并切换到该分支</span><br><span class="line">git checkout -b [branch]</span><br><span class="line"></span><br><span class="line"># 合并指定分支到当前分支</span><br><span class="line">$ git merge [branch]</span><br><span class="line"></span><br><span class="line"># 删除分支</span><br><span class="line">$ git branch -d [branch-name]</span><br><span class="line"></span><br><span class="line"># 删除远程分支</span><br><span class="line">$ git push origin --delete [branch-name]</span><br><span class="line">$ git branch -dr [remote/branch]</span><br></pre></td></tr></table></figure><p><code>master</code>主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说<code>dev</code>分支代码稳定后可以合并到主分支<code>master</code>上来。</p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tonic.utils.plot.grid()</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SNN/Tonic.utils.plot.grid()/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SNN/Tonic.utils.plot.grid()/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="Tonic-utils-plot-grid"><a href="#Tonic-utils-plot-grid" class="headerlink" title="Tonic.utils.plot.grid()"></a>Tonic.utils.plot.grid()</h2><p>将事件可视化，绘制成帧</p><p><a href="https://tonic.readthedocs.io/en/latest/how-tos/visualizing-data.html">函数链接</a></p><h3 id="一、函数原型"><a href="#一、函数原型" class="headerlink" title="一、函数原型"></a>一、<strong>函数原型</strong></h3><p><strong>函数作用：</strong>将事件累积成等于轴的乘积的帧，以供视觉检查</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">import tonic.transforms as transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def <span class="title function_">plot_event_grid</span><span class="params">(events, axis_array=(<span class="number">1</span>, <span class="number">3</span>), plot_frame_number=False)</span>:</span><br><span class="line">    &quot;&quot;&quot;Plot events accumulated as frames equal to the product of axes <span class="keyword">for</span> visual inspection.</span><br><span class="line"></span><br><span class="line">    Parameters:</span><br><span class="line">        events: Structured numpy <span class="built_in">array</span> of shape [num_events, num_event_channels].</span><br><span class="line">            事件流，numpy数组</span><br><span class="line">            </span><br><span class="line">        axis_array: dimensions of plotting grid. The larger the grid,</span><br><span class="line">                    the more fine-grained the events will be sliced in time.</span><br><span class="line">                    将事件流，绘制成帧，并用网格形式表示出来，此处可设定帧的数量，以axis_array=(<span class="number">2</span>,<span class="number">3</span>)为例，则帧的数量为<span class="number">2</span>*<span class="number">3</span>=<span class="number">6</span>，以<span class="number">2</span>行<span class="number">3</span>列的网格显示出                     来</span><br><span class="line">                        </span><br><span class="line">        plot_frame_number: optional index of frame when plotting</span><br><span class="line">            绘图时帧的可选索引</span><br><span class="line"></span><br><span class="line">    Example:</span><br><span class="line">        &gt;&gt;&gt; import tonic</span><br><span class="line">        &gt;&gt;&gt; dataset = tonic.datasets.NMNIST(save_to=<span class="string">&#x27;./data&#x27;</span>)</span><br><span class="line">        &gt;&gt;&gt; events, target = dataset[<span class="number">100</span>]</span><br><span class="line">        &gt;&gt;&gt; tonic.utils.plot_event_grid(events)</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">        None</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    try:</span></span><br><span class="line"><span class="string">        import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="string">    except ImportError:</span></span><br><span class="line"><span class="string">        raise ImportError(</span></span><br><span class="line"><span class="string">            &quot;</span>Please install the matplotlib package to plot events. This is an optional<span class="string">&quot;</span></span><br><span class="line"><span class="string">            &quot;</span> dependency.<span class="string">&quot;</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 事件流events是一个numpy列表，其中dtype为 [(&#x27;x&#x27;, &#x27;&lt;i8&#x27;), (&#x27;y&#x27;, &#x27;&lt;i8&#x27;), (&#x27;t&#x27;, &#x27;&lt;i8&#x27;), (&#x27;p&#x27;, &#x27;&lt;i8&#x27;)]</span></span><br><span class="line"><span class="string">        # events.dtype.names 为 (&#x27;x&#x27;, &#x27;y&#x27;, &#x27;t&#x27;, &#x27;p&#x27;) 表示一个事件的坐标x,y（像素坐标系） 时间辍，极性</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">    if &quot;</span>y<span class="string">&quot; in events.dtype.names:</span></span><br><span class="line"><span class="string"># 得到了 events 事件流中事件在像素坐标系中横坐标与纵坐标的最大值，用于绘制帧的尺寸</span></span><br><span class="line"><span class="string">        sensor_size_x = int(events[&quot;</span>x<span class="string">&quot;].max() + 1)</span></span><br><span class="line"><span class="string">        sensor_size_y = int(events[&quot;</span>y<span class="string">&quot;].max() + 1)</span></span><br><span class="line"><span class="string">        # np.unique(events[&quot;</span>p<span class="string">&quot;]) 对于一个以为列表或者数组，去除重复元素,并按照元素从小到大返回一个新的列表或者元组</span></span><br><span class="line"><span class="string">        # 此处得到了，事件有机种极性，也代表着之后的事件帧有几个通道</span></span><br><span class="line"><span class="string">        sensor_size_p = len(np.unique(events[&quot;</span>p<span class="string">&quot;]))</span></span><br><span class="line"><span class="string">        # 此处为帧的尺寸，长、宽、通道数</span></span><br><span class="line"><span class="string">        sensor_size = (sensor_size_x, sensor_size_y, sensor_size_p)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 将事件转化为事件帧，sensor_size为事件帧的尺寸，n_time_bins为按照时间箱得到固定的帧数</span></span><br><span class="line"><span class="string">            # np.product(axis_array) 返回数组或者元组 axis_array中元素的乘积。即时间流通过切片方式得到的事件帧的数量</span></span><br><span class="line"><span class="string">        transform = transforms.ToFrame(</span></span><br><span class="line"><span class="string">            sensor_size=sensor_size, n_time_bins=np.product(axis_array)</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 将事件流转化为事件帧</span></span><br><span class="line"><span class="string">        frames = transform(events)</span></span><br><span class="line"><span class="string">        # *axis_array在python中表示 将数组或者元组解开成几个独立的参数，传入函数，如axis_array=(2,3),则*axis_array = 2 3</span></span><br><span class="line"><span class="string">        # plt.subplots(m,n) 绘制 m*n个子图，字图在一个大图中以m行n列的方式显示</span></span><br><span class="line"><span class="string">        # 函数的返回值是一个元组，包括一个图形对象和所有的 axes_array 对象。</span></span><br><span class="line"><span class="string">        # 其中 axes_array 对象（含所有子图的一个数组）的数量等于 nrows * ncols，且每个 axes_array 对象均可通过索引值访问</span></span><br><span class="line"><span class="string">            # fig 的类型就是一个大的画布，所有子图均显示在上面（&lt;class &#x27;matplotlib.figure.Figure&#x27;&gt;）</span></span><br><span class="line"><span class="string">        fig, axes_array = plt.subplots(*axis_array)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if 1 in axis_array:</span></span><br><span class="line"><span class="string">            axes_array = axes_array.reshape(1, -1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for i in range(axis_array[0]):</span></span><br><span class="line"><span class="string">            for j in range(axis_array[1]):</span></span><br><span class="line"><span class="string"># 按照索引得到一帧图像</span></span><br><span class="line"><span class="string">                frame = frames[i * axis_array[1] + j]</span></span><br><span class="line"><span class="string">                # 将事件帧的两个通道的像素值进行作差---并在子图中显示出来，可以简单的看出事件相机在录制数据时的移动方向</span></span><br><span class="line"><span class="string">                axes_array[i, j].imshow(frame[1] - frame[0])</span></span><br><span class="line"><span class="string">                # 关闭子图中所有坐标轴线、刻度标记和标签</span></span><br><span class="line"><span class="string">                axes_array[i, j].axis(&quot;</span>off<span class="string">&quot;)</span></span><br><span class="line"><span class="string">                # 是否给每张子图设置一个标题 title</span></span><br><span class="line"><span class="string">                if plot_frame_number:</span></span><br><span class="line"><span class="string">                    axes_array[i, j].title.set_text(str(i * axis_array[1] + j))</span></span><br><span class="line"><span class="string">        # 自动调整子图参数，使之填充整个图像区域</span></span><br><span class="line"><span class="string">        plt.tight_layout()</span></span><br><span class="line"><span class="string">        # 显示所有的figure，这是必要的，若没有则会出现，只出现一张子图的情况</span></span><br><span class="line"><span class="string">        plt.show()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    else:</span></span><br><span class="line"><span class="string">        sensor_size_x = int(events[&quot;</span>x<span class="string">&quot;].max() + 1)</span></span><br><span class="line"><span class="string">        frame_transform = transforms.ToFrame(</span></span><br><span class="line"><span class="string">            sensor_size=(sensor_size_x, 1, 1), n_time_bins=sensor_size_x * 2</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        frames = frame_transform(events)</span></span><br><span class="line"><span class="string">        plt.imshow(frames.squeeze().T)</span></span><br><span class="line"><span class="string">        plt.xlabel(&quot;</span>Time<span class="string">&quot;)</span></span><br><span class="line"><span class="string">        plt.ylabel(&quot;</span>Channels<span class="string">&quot;)</span></span><br></pre></td></tr></table></figure><h4 id="1-实例程序："><a href="#1-实例程序：" class="headerlink" title="1.实例程序："></a>1.实例程序：</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    将事件数据转换为帧</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tonic</span><br><span class="line">import tonic.transforms as transforms</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 下载 NMINST 数据</span><br><span class="line">dataset = tonic.datasets.NMNIST(save_to=<span class="string">&quot;/home/zxz/Proj/DP/Do/demo_03/Tonic_dir/tutorials/data&quot;</span>,train=False)</span><br><span class="line"></span><br><span class="line">print(len(dataset))  # <span class="number">10000</span></span><br><span class="line">events,target = dataset[<span class="number">1000</span>]</span><br><span class="line">print(events)  <span class="meta"># numpy数组</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    每行对应一个事件，由四个参数组成:(x坐标、y坐标、时间戳、极性)</span></span><br><span class="line"><span class="string">        x和y坐标对应34*34网格中的地址</span></span><br><span class="line"><span class="string">        事件的时间戳以微秒为单位记录</span></span><br><span class="line"><span class="string">        极性指的是是否发生了峰上(+1)或峰下(-1);即，亮度的增加或减少</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"># 将许多事件累积到一个框架中，使其可视化---将事件流划分为<span class="number">3</span>*<span class="number">2</span>个事件帧</span><br><span class="line">tonic.utils.plot_event_grid(events,axis_array=(<span class="number">3</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p><strong>显示效果</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302051424871.png" alt="image-20230205142409828"></p><h4 id="2-手写相同效果的程序"><a href="#2-手写相同效果的程序" class="headerlink" title="2.手写相同效果的程序"></a>2.手写相同效果的程序</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    将事件数据转换为帧---对tonic中的一些操作进行合并</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tonic</span><br><span class="line">import tonic.transforms as transforms</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    事件帧的可视化程序，将事件帧可视化</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def <span class="title function_">plot_frames</span><span class="params">(frames)</span>:</span><br><span class="line">    <span class="meta"># subplots 生成 1*len(frames) 个子图像</span></span><br><span class="line">    # 函数的返回值是一个元组，包括一个图形对象和所有的 axes 对象。</span><br><span class="line">        # 其中 axes 对象的数量等于 nrows * ncols，且每个 axes 对象均可通过索引值访问（从1开始）</span><br><span class="line">    fig, axes = plt.subplots(<span class="number">1</span>, len(frames))</span><br><span class="line">    <span class="meta"># zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表</span></span><br><span class="line">    <span class="keyword">for</span> axis, frame in <span class="title function_">zip</span><span class="params">(axes, frames)</span>:</span><br><span class="line">        # 让画布上的每一个子图显示一个图像</span><br><span class="line">        axis.<span class="title function_">imshow</span><span class="params">((frame[<span class="number">1</span>]-frame[<span class="number">0</span>]))</span></span><br><span class="line">        # 关闭所有坐标轴线、刻度标记和标签<span class="title function_">next</span><span class="params">(iternext(iter</span></span><br><span class="line"><span class="params">        axis.axis(<span class="string">&quot;off&quot;</span>)</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params">    # 自动调整子图参数，使之填充整个图像区域</span></span><br><span class="line"><span class="params">    plt.tight_layout()</span></span><br><span class="line"><span class="params">    # 显示所有的figure，这是必要的，若没有则会出现，只出现一张子图的情况</span></span><br><span class="line"><span class="params">    plt.show()</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params"># 设置 CPU 生成随机数的 种子,可见不同的随机种子能够生成不同的随机数,但只要随机种子一样，每次运行代码都会生成该种子下的随机数</span></span><br><span class="line"><span class="params">torch.manual_seed(<span class="number">1234</span>)</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params"># 手动转化</span></span><br><span class="line"><span class="params">sensor_size = tonic.datasets.NMNIST.sensor_size</span></span><br><span class="line"><span class="params"># 帧有尺寸(时间，极性数，高度和宽度)。让我们为N-MNIST样本中的三次扫视绘制一个帧。我们将利用两个相机的极性差来观察运动方向</span></span><br><span class="line"><span class="params">    # 用时间箱将事件流划分为 数量为n_time_bins的事件帧</span></span><br><span class="line"><span class="params">frame_transfrom = transforms.ToFrame(sensor_size=sensor_size,n_time_bins=<span class="number">6</span>)</span></span><br><span class="line"><span class="params"># 对事件进行去噪</span></span><br><span class="line"><span class="params">denoise_transform = tonic.transforms.Denoise(filter_time=<span class="number">10000</span>)</span></span><br><span class="line"><span class="params"># 对事件的变化操作</span></span><br><span class="line"><span class="params">transform = transforms.Compose([denoise_transform,frame_transfrom])</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params"># 下载 NMINST 数据 --- 已经对NMINST中数据进行处理变为帧</span></span><br><span class="line"><span class="params">dataset = tonic.datasets.NMNIST(</span></span><br><span class="line"><span class="params">    save_to=<span class="string">&quot;/home/zxz/Proj/DP/Do/demo_03/Tonic_dir/tutorials/data&quot;</span>, train=False, transform=transform</span></span><br><span class="line"><span class="params">)</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params">frame,target = dataset[<span class="number">1000</span>]</span></span><br><span class="line"><span class="params">plot_frames(frame)</span></span><br></pre></td></tr></table></figure><p><strong>显示效果</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302051432511.png" alt="image-20230205143215481"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> SNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SNN工具链 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tonic.Transforms.ToFrame()</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SNN/Tonic.Transforms.ToFrame()/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SNN/Tonic.Transforms.ToFrame()/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="Tonic-Transforms-ToFrame"><a href="#Tonic-Transforms-ToFrame" class="headerlink" title="Tonic.Transforms.ToFrame()"></a>Tonic.Transforms.ToFrame()</h2><p>Tonic是一个方便下载、操作和加载基于事件&#x2F;基于峰值的数据的工具。它就像PyTorch Vision，但用于神经形态数据!</p><p><a href="https://tonic.readthedocs.io/en/latest/auto_examples/representations/plot_toframe.html#sphx-glr-auto-examples-representations-plot-toframe-py">函数链接</a></p><p><a href="https://blog.csdn.net/black_buaa/article/details/108478804">事件相机的数据处理</a>：该博客中介绍了如何对事件数据进行可视化</p><p><strong>函数原型：</strong></p><p><strong>函数作用：通过一定的方法，将事件流可视化，得到帧</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">@dataclass(frozen=True)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToFrame</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;Accumulate events to frames by slicing along constant time (time_window), constant number of</span></span><br><span class="line"><span class="string">    events (spike_count) or constant number of frames (n_time_bins / n_event_bins). All the events</span></span><br><span class="line"><span class="string">    in one slice are added up in a frame for each polarity. You can set one of the first 4</span></span><br><span class="line"><span class="string">    parameters to choose the slicing method. Depending on which method you choose, overlap will</span></span><br><span class="line"><span class="string">    assume different functionality, whether that might be temporal overlap, number of events or</span></span><br><span class="line"><span class="string">    fraction of a bin. As a rule of thumb, here are some considerations if you are unsure which</span></span><br><span class="line"><span class="string">    slicing method to choose:</span></span><br><span class="line"><span class="string">通过沿着常数时间(时间窗口)、常数数量的事件(峰值计数)或常数数量的帧(n个时间箱/ n个事件箱)将事件累积到帧中。一个切片中的所有事件都被添加到每个极性的帧   中。您可以设置前4个参数中的一个来选择切片方法。根据您选择的方法，重叠将假设不同的功能，无论是时间重叠、事件数量还是bin的部分。根据经验，如果你不确定     要选择哪种切片方法，这里有一些注意事项</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * If your recordings are of roughly the same length, a safe option is to set time_window. Bare in mind</span></span><br><span class="line"><span class="string">      that the number of events can vary greatly from slice to slice, but will give you some consistency when</span></span><br><span class="line"><span class="string">      training RNNs or other algorithms that have time steps.</span></span><br><span class="line"><span class="string">      如果你的录音长度大致相同，一个安全的选择是设置时间窗口。请记住，不同切片的事件数量可能会有很大差异，但在训练rnn或其他具有时间步长的算法时，会给你一       些一致性。</span></span><br><span class="line"><span class="string">      方法：利用固定时间窗口。以某一固定时间进行累计得到事件帧。但是不能改善状态差异导致的帧间的信息量差异</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * If your recordings have roughly the same amount of activity / number of events and you are more interested</span></span><br><span class="line"><span class="string">      in the spatial composition, then setting spike_count will give you frames that are visually more consistent.</span></span><br><span class="line"><span class="string">      如果您的记录具有大致相同的活动数量/事件数量，并且您对空间构成更感兴趣，那么设置峰值计数将为您提供视觉上更一致的帧</span></span><br><span class="line"><span class="string">      方法：利用事件数量。以达到某一数量的事件数为阈值，输出一个事件帧</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * The previous time_window and spike_count methods will likely result in a different amount of frames for each</span></span><br><span class="line"><span class="string">      recording. If your training method benefits from consistent number of frames across a dataset (for easier</span></span><br><span class="line"><span class="string">      batching for example), or you want a parameter that is easier to set than the exact window length or number</span></span><br><span class="line"><span class="string">      of events per slice, consider fixing the number of frames by setting n_time_bins or n_event_bins. The two</span></span><br><span class="line"><span class="string">      methods slightly differ with respect to how the slices are distributed across the recording. You can define</span></span><br><span class="line"><span class="string">      an overlap between 0 and 1 to provide some robustness.</span></span><br><span class="line"><span class="string">      之前的时间窗口和峰值计数方法可能会导致每个记录的帧数不同。如果你的训练方法受益于数据集中一致的帧数(例如，为了更容易批处理)，或者你想要一个比精确的       窗口长度或每个切片的事件数量更容易设置的参数，请考虑通过设置n个时间箱或n个事件箱来固定帧数。这两种方法在切片如何在记录中分布方面略有不同。可以为pro       定义0和1之间的重叠</span></span><br><span class="line"><span class="string">      方法：n个时间箱或n个事件箱来得到固定帧数，利用事件信息数量及事件阈值双限制的方式进行累计得到事件帧。这样产生的事件帧既可以保证高时间分辨率又可以使       事件中具有足够的信息进一步处理</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        sensor_size: a 3-tuple of x,y,p for sensor_size. If omitted, the sensor size is calculated for that sample. However,</span></span><br><span class="line"><span class="string">                    do use this feature sparingly as when not all pixels fire in a sample, this might cause issues with batching/</span></span><br><span class="line"><span class="string">                    stacking tensors further down the line.</span></span><br><span class="line"><span class="string">                    帧的尺寸大小，p表示通道数量</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">        time_window (float): time window length for one frame. Use the same time unit as timestamps in the event recordings.</span></span><br><span class="line"><span class="string">                             Good if you want temporal consistency in your training, bad if you need some visual consistency</span></span><br><span class="line"><span class="string">                             for every frame if the recording&#x27;s activity is not consistent.</span></span><br><span class="line"><span class="string">                             一帧的时间窗口长度。在事件记录中使用与时间戳相同的时间单位。如果你想在训练中保持时间一致性，这很好;如果你需要每一帧的视                              觉一致性，如果记录的活动不一致，那就不好了。</span></span><br><span class="line"><span class="string">                             时间窗口 </span></span><br><span class="line"><span class="string">                                 </span></span><br><span class="line"><span class="string">        spike_count (int): number of events per frame. Good for training CNNs which do not care about temporal consistency.</span></span><br><span class="line"><span class="string">                           每帧的事件数。适合训练不关心时间一致性的cnn</span></span><br><span class="line"><span class="string">                                 </span></span><br><span class="line"><span class="string">        n_time_bins (int): fixed number of frames, sliced along time axis. Good for generating a pre-determined number of</span></span><br><span class="line"><span class="string">                           frames which might help with batching.</span></span><br><span class="line"><span class="string">                           固定帧数，沿时间轴切片。用于生成预先确定的帧数，这可能有助于批处理。</span></span><br><span class="line"><span class="string">                           时间箱---得到固定的帧数</span></span><br><span class="line"><span class="string">                                 </span></span><br><span class="line"><span class="string">        n_event_bins (int): fixed number of frames, sliced along number of events in the recording. Good for generating a</span></span><br><span class="line"><span class="string">                            pre-determined number of frames which might help with batching.</span></span><br><span class="line"><span class="string">                            固定帧数，沿着记录中的事件数切片。用于生成预先确定的帧数，这可能有助于批处理。</span></span><br><span class="line"><span class="string">                            事件箱</span></span><br><span class="line"><span class="string">                                 </span></span><br><span class="line"><span class="string">        overlap (float): overlap between frames defined either in time units, number of events or number of bins between 0 and 1.</span></span><br><span class="line"><span class="string">        include_incomplete (bool): if True, includes overhang slice when time_window or spike_count is specified.</span></span><br><span class="line"><span class="string">                                   Not valid for bin_count methods.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; from tonic.transforms import ToFrame</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transform1 = ToFrame(time_window=10000, overlap=300, include_incomplete=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transform2 = ToFrame(spike_count=3000, overlap=100, include_incomplete=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transform3 = ToFrame(n_time_bins=100, overlap=0.1)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    sensor_size: Optional[Tuple[<span class="type">int</span>, <span class="type">int</span>, <span class="type">int</span>]]</span><br><span class="line">    time_window: Optional[<span class="type">float</span>] = None</span><br><span class="line">    event_count: Optional[<span class="type">int</span>] = None</span><br><span class="line">    n_time_bins: Optional[<span class="type">int</span>] = None</span><br><span class="line">    n_event_bins: Optional[<span class="type">int</span>] = None</span><br><span class="line">    overlap: <span class="type">float</span> = <span class="number">0</span></span><br><span class="line">    include_incomplete: <span class="type">bool</span> = False</span><br><span class="line"></span><br><span class="line">    def __call__(self, events):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> functional.to_frame_numpy(</span><br><span class="line">            events=events,</span><br><span class="line">            sensor_size=self.sensor_size,</span><br><span class="line">            time_window=self.time_window,</span><br><span class="line">            event_count=self.event_count,</span><br><span class="line">            n_time_bins=self.n_time_bins,</span><br><span class="line">            n_event_bins=self.n_event_bins,</span><br><span class="line">            overlap=self.overlap,</span><br><span class="line">            include_incomplete=self.include_incomplete,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302051152367.png" alt="image-20230205115223331"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> SNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SNN工具链 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Tonic加载标准本地的事件数据</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SNN/%E4%BD%BF%E7%94%A8Tonic%E5%8A%A0%E8%BD%BD%E6%A0%87%E5%87%86%E6%9C%AC%E5%9C%B0%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%95%B0%E6%8D%AE/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SNN/%E4%BD%BF%E7%94%A8Tonic%E5%8A%A0%E8%BD%BD%E6%A0%87%E5%87%86%E6%9C%AC%E5%9C%B0%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="使用Tonic加载标准本地的事件数据"><a href="#使用Tonic加载标准本地的事件数据" class="headerlink" title="使用Tonic加载标准本地的事件数据"></a>使用Tonic加载标准本地的事件数据</h2><p><a href="https://zhuanlan.zhihu.com/p/149824829">os.walk()</a></p><p><a href="https://www.cnblogs.com/poloyy/p/15154008.html">callable and optional</a></p><p><a href="https://blog.csdn.net/ETalien_/article/details/103090294">numpy数组的保存：二进制文件（bin or npy）</a></p><p><a href="https://tonic.readthedocs.io/en/latest/how-tos/wrapping_own_data.html">Tonic帮助文档：How do I wrap my own recordings?</a></p><p>如果您在磁盘上有自己的记录，并且希望使用Tonic进行快速数据加载和应用转换，那么您可以将它们包装在一个自定义类中。最简单的选择是使用torchvision DatasetFolder类。如果这不适用于您的情况，您可以编写自己的类，在其中提供init、getitem和len方法的最小集合，然后就可以开始了。这个笔记是关于从本地numpy文件读取事件记录的模板类。我们将从创建一些虚拟文件开始</p><h3 id="1-模拟创建随机的事件流数据"><a href="#1-模拟创建随机的事件流数据" class="headerlink" title="1.模拟创建随机的事件流数据"></a>1.模拟创建随机的事件流数据</h3><p><strong>程序通过<code>np.random.rand(n_events)</code>创建<code>n_events</code>个服从<code>0-1</code>均匀分布的随机样本值，并将其存储在<code>numpy</code>数组中，通过<code>sensor_size[index]</code>确定事件四元组中每一个元素的范围大小。<code>dtype</code>中确定了四元组中每个元素的数据类型为<code>int</code></strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    模型创建随机的事件流数据  事件四元组</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import numpy as np</span><br><span class="line">from tonic import Dataset, transforms</span><br><span class="line">import torch</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">sensor_size = (<span class="number">100</span>, <span class="number">200</span>, <span class="number">2</span>)</span><br><span class="line">n_recordings = <span class="number">10</span>   # 文件中包含<span class="number">10</span>个事件流  例如 NMNIST 中包含 <span class="number">10000</span> 个事件流</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    创建随机的事件流，并且保留在本地，以二进制文件格式保留</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def create_random_input(</span><br><span class="line">    sensor_size=sensor_size,</span><br><span class="line">    # 事件流中的事件个数</span><br><span class="line">    n_events=<span class="number">10000</span>,</span><br><span class="line">    # 事件流中的事件以元组的形式表示，(<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;p&#x27;</span>) x、y坐标  t 时间辍（微秒级别）  p 事件极性  均为整型</span><br><span class="line">    dtype=np.dtype([(<span class="string">&quot;x&quot;</span>, <span class="type">int</span>), (<span class="string">&quot;y&quot;</span>, <span class="type">int</span>), (<span class="string">&quot;t&quot;</span>, <span class="type">int</span>), (<span class="string">&quot;p&quot;</span>, <span class="type">int</span>)])</span><br><span class="line">):</span><br><span class="line">    events = np.zeros(n_events, dtype=dtype)</span><br><span class="line">    <span class="meta"># np.random.rand() 返回一个或者一组服从“0-1”均匀分布的随机样本值</span></span><br><span class="line">        <span class="meta"># np.random.rand(5)  return:[0.16132617 0.74789463 0.51725874 0.34676313 0.73510629]</span></span><br><span class="line">        <span class="meta"># np.random.rand(n_events) * sensor_size[0] 将该组样本值中的每一个元素均乘以 sensor_size[0]</span></span><br><span class="line">    events[<span class="string">&quot;x&quot;</span>] = np.random.rand(n_events) * sensor_size[<span class="number">0</span>]</span><br><span class="line">    events[<span class="string">&quot;y&quot;</span>] = np.random.rand(n_events) * sensor_size[<span class="number">1</span>]</span><br><span class="line">    events[<span class="string">&quot;p&quot;</span>] = np.random.rand(n_events) * sensor_size[<span class="number">2</span>]</span><br><span class="line">    events[<span class="string">&quot;t&quot;</span>] = np.sort(np.random.rand(n_events) * <span class="number">1e6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> events</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>模拟事件流效果显示：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(create_random_input())</span><br><span class="line">print(type(create_random_input()))</span><br><span class="line">    </span><br><span class="line"># 运行结果</span><br><span class="line">[(<span class="number">91</span>,   <span class="number">2</span>,    <span class="number">307</span>, <span class="number">0</span>) (<span class="number">38</span>,   <span class="number">1</span>,    <span class="number">577</span>, <span class="number">0</span>) (<span class="number">95</span>,  <span class="number">62</span>,   <span class="number">1119</span>, <span class="number">0</span>) ...</span><br><span class="line">(<span class="number">60</span>, <span class="number">104</span>, <span class="number">999259</span>, <span class="number">1</span>) (<span class="number">71</span>,  <span class="number">20</span>, <span class="number">999370</span>, <span class="number">0</span>) (<span class="number">35</span>, <span class="number">156</span>, <span class="number">999674</span>, <span class="number">0</span>)]</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">numpy</span>.<span class="title">ndarray</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p><strong>保存事件流数据</strong></p><p>**保存为<code>npy格式</code>**如下图</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 将事件流数据保留到本地  保存的文件格式为  npy</span><br><span class="line">[</span><br><span class="line">    # 数组是以未压缩的原始二进制格式保存在扩展名为.npy的文件中</span><br><span class="line">    np.save(f<span class="string">&quot;../Tonic_dir/tutorials/data/rand_by_myself/recording&#123;i&#125;.npy&quot;</span>, create_random_input())</span><br><span class="line">    <span class="keyword">for</span> i in <span class="title function_">range</span><span class="params">(<span class="number">0</span>,<span class="number">10</span>)</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302071917631.png" alt="image-20230207191528867"></p><p><strong>保存为<code>bin</code>格式</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 将事件流数据保留到本地  文件保存的格式为 bin</span><br><span class="line">[</span><br><span class="line">    # 数组是以未压缩的原始二进制格式保存在扩展名为.bin的文件中</span><br><span class="line">    create_random_input().tofile(f<span class="string">&quot;../Tonic_dir/tutorials/data/rand_by_myself/recording&#123;i&#125;.bin&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i in <span class="title function_">range</span><span class="params">(<span class="number">30</span>,<span class="number">40</span>)</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302071918175.png" alt="image-20230207191814152"></p><h3 id="2-继承Dataset类加载本地的事件流数据"><a href="#2-继承Dataset类加载本地的事件流数据" class="headerlink" title="2.继承Dataset类加载本地的事件流数据"></a>2.继承Dataset类加载本地的事件流数据</h3><p>该程序只针对 以<code>npy</code>为后缀的事件流数据，以<code>bin</code>为后缀的数据可以根据文字开头<code>numpy数组的保存：二进制文件</code>链接跳转，修改<code>os.path.join(self.data_dir,f&quot;recording&#123;i&#125;.npy&quot;) for i in range(n_recordings)</code>以及<code>events = np.load(self.filenames[index])</code>两个部分，修改为读取<code>bin</code>文件的方式</p><p>使用此程序，需要修改<code>self.data_dir</code>即事件流文件目录路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">    继承Dataset类加载本地的事件流数据</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">class MyRecordings(Dataset):</span><br><span class="line">    sensor_size = (</span><br><span class="line">        34,</span><br><span class="line">        34,</span><br><span class="line">        2,</span><br><span class="line">    )  # the sensor size of the event camera or the number of channels of the silicon cochlear that was used</span><br><span class="line">    ordering = (</span><br><span class="line">        &quot;xytp&quot;  # the order in which your event channels are provided in your recordings</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        train=True,</span><br><span class="line">        transform=None,</span><br><span class="line">        target_transform=None,</span><br><span class="line">    ):</span><br><span class="line">        super(MyRecordings, self).__init__(</span><br><span class="line">            save_to=&#x27;./&#x27;, transform=transform, target_transform=target_transform</span><br><span class="line">        )</span><br><span class="line">        self.train = train</span><br><span class="line"></span><br><span class="line">        # replace the strings with your training/testing file locations or pass as an argument</span><br><span class="line">        # 本地事件流文件路径</span><br><span class="line">        self.data_dir = f&quot;../Tonic_dir/tutorials/data/rand_by_myself/Test/0&quot;</span><br><span class="line">        if train:</span><br><span class="line">            self.filenames = [</span><br><span class="line">                # f&quot;recording&#123;i&#125;.npy&quot; 为 数据流文件</span><br><span class="line">                os.path.join(self.data_dir,f&quot;recording&#123;i&#125;.npy&quot;) for i in range(n_recordings)</span><br><span class="line">            ]</span><br><span class="line">        else:</span><br><span class="line">            raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        # 加载文件中的事件流</span><br><span class="line">        events = np.load(self.filenames[index])</span><br><span class="line"></span><br><span class="line">        if self.transform is not None:</span><br><span class="line">            events = self.transform(events)</span><br><span class="line"></span><br><span class="line">        return events</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.filenames)</span><br></pre></td></tr></table></figure><p><strong>使用程序读取事件流数据</strong>，程序并为对事件流数据进行人工标注</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataset = MyRecordings(train=True)</span><br><span class="line">events = dataset[<span class="number">0</span>]</span><br><span class="line">print(events)</span><br><span class="line"></span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, shuffle=True)</span><br><span class="line">events = next(iter(dataloader))</span><br><span class="line">print(events)</span><br></pre></td></tr></table></figure><h3 id="3-加载事件流数据并对其进行人工标注"><a href="#3-加载事件流数据并对其进行人工标注" class="headerlink" title="3.加载事件流数据并对其进行人工标注"></a>3.加载事件流数据并对其进行人工标注</h3><p>该程序改编自 <code>tonic.datasets.NMNIST</code>数据集加载的源码，将其网络下载的部分进行删剪得到</p><p><strong>程序只能对以<code>bin</code>为后缀的事件流数据进行人工标注</strong>，因为程序中<code>getitem()</code>中得到对应索引的事件流数据使用的是<code>tonic.io.read_minst_file()</code>函数，函数经过测试得到无法对以<code>npy</code>为后缀的事件流数据进行解析，其中的解析算法会报错</p><p><a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.io.read_mnist_file.html">tonic.io.read_minst_file()函数网址链接</a></p><p>程序中需要修改的为<code>classes</code>数组，根据自己数据集的标签进行修改</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    对本地的事件流数据使用Tonic进行标注</span></span><br><span class="line"><span class="string">        有关网络下载的内容可以删除</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        save_to (string): Location to save files to on disk.</span></span><br><span class="line"><span class="string">        train (bool): If True, uses training subset, otherwise testing subset.</span></span><br><span class="line"><span class="string">        first_saccade_only (bool): If True, only work with events of the first of three saccades.</span></span><br><span class="line"><span class="string">                                   Results in about a third of the events overall.</span></span><br><span class="line"><span class="string">                                   如果为True，则只处理三个扫视中的第一个事件。大约三分之一的事件的结果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        transform (callable, optional): A callable of transforms to apply to the data.</span></span><br><span class="line"><span class="string">        target_transform (callable, optional): A callable of transforms to apply to the targets/labels.</span></span><br><span class="line"><span class="string">                                               一个可调用的转换应用到目标/标签。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        transforms (callable, optional): A callable of transforms that is applied to both data and</span></span><br><span class="line"><span class="string">                                         labels at the same time.</span></span><br><span class="line"><span class="string">                                         同时应用于数据和标签的转换可调用对象</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">from typing import Callable, Optional</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tonic.dataset import Dataset</span><br><span class="line">from tonic.io import read_mnist_file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class <span class="title function_">MYDATASET</span><span class="params">(Dataset)</span>:</span><br><span class="line">    train_filename = <span class="string">&quot;train.zip&quot;</span></span><br><span class="line">    train_folder = <span class="string">&quot;Train&quot;</span></span><br><span class="line">    test_filename = <span class="string">&quot;test.zip&quot;</span></span><br><span class="line">    test_folder = <span class="string">&quot;Test&quot;</span></span><br><span class="line"></span><br><span class="line">    classes = [</span><br><span class="line">        <span class="string">&quot;0 - zero&quot;</span>,</span><br><span class="line">        <span class="string">&quot;1 - one&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    sensor_size = (<span class="number">34</span>, <span class="number">34</span>, <span class="number">2</span>)</span><br><span class="line">    dtype = np.dtype([(<span class="string">&quot;x&quot;</span>, <span class="type">int</span>), (<span class="string">&quot;y&quot;</span>, <span class="type">int</span>), (<span class="string">&quot;t&quot;</span>, <span class="type">int</span>), (<span class="string">&quot;p&quot;</span>, <span class="type">int</span>)])</span><br><span class="line">    ordering = dtype.names</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">            Callable 作为函数参数使用，其实只是做一个类型检查的作用，检查传入的参数值 get_func 是否为可调用对象</span></span><br><span class="line"><span class="string">                函数是可以调用的，变量是不可以调用的</span></span><br><span class="line"><span class="string">            Optional 可选类型 可选参数具有默认值，具有默认值的可选参数不需要在其类型批注上使用 Optional，因为它是可选的</span></span><br><span class="line"><span class="string">                Optional[int] 等价于 Union[int, None] 意味着：既可以传指定的类型 int，也可以传 None</span></span><br><span class="line"><span class="string">                如:transform: Optional[Callable] </span></span><br><span class="line"><span class="string">                    tranform 的 参数是一个 列表，其中参数可以为 Callable(可调用对象) 类型 也可以为 None</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">            self,</span><br><span class="line">            save_to: str,</span><br><span class="line">            train: <span class="type">bool</span> = True,</span><br><span class="line">            first_saccade_only: <span class="type">bool</span> = False,</span><br><span class="line">            transform: Optional[Callable] = None,</span><br><span class="line">            target_transform: Optional[Callable] = None,</span><br><span class="line">            transforms: Optional[Callable] = None,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__(</span><br><span class="line">            save_to,</span><br><span class="line">            transform=transform,</span><br><span class="line">            target_transform=target_transform,</span><br><span class="line">            transforms=transforms,</span><br><span class="line">        )</span><br><span class="line">        self.train = train</span><br><span class="line">        self.first_saccade_only = first_saccade_only</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            self.filename = self.train_filename</span><br><span class="line">            self.folder_name = self.train_folder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.filename = self.test_filename</span><br><span class="line">            self.folder_name = self.test_folder</span><br><span class="line"></span><br><span class="line">        # 在父类DataSet 中 self.location_on_system = os.path.join(save_to, self.__class__.__name__)</span><br><span class="line">            <span class="meta"># self.__class__.__name__  当前类类名 类名需要与本地的事件流数据集的外层名录名一致</span></span><br><span class="line">        # file_path = os.path.join(self.location_on_system, self.folder_name)</span><br><span class="line">        file_path = save_to</span><br><span class="line"></span><br><span class="line">        <span class="meta"># os.walk扫描test文件夹下所有的子目录和文件</span></span><br><span class="line">        # 二进制事件流文件的后缀名 --- 根据自己 文件 后缀名进行修改</span><br><span class="line">        events_file_end = <span class="string">&quot;bin&quot;</span></span><br><span class="line">        # 事件流标签的数据类型的别名 如 NMNIST 的标签为 <span class="number">0</span>到<span class="number">9</span> 为 <span class="type">int</span></span><br><span class="line">        label_type = <span class="type">int</span></span><br><span class="line">        <span class="keyword">for</span> path, dirs, files in os.walk(file_path):</span><br><span class="line">            # 排序</span><br><span class="line">            files.sort()</span><br><span class="line">            <span class="meta"># file 是 最底层目录下的 二进制数据流文件</span></span><br><span class="line">            <span class="keyword">for</span> file in files:</span><br><span class="line">                <span class="keyword">if</span> file.endswith(events_file_end):</span><br><span class="line">                    <span class="meta"># path 事件流上层目录的路径</span></span><br><span class="line">                    <span class="meta"># self.data 列表存储了所有的 事件流二进制文件</span></span><br><span class="line">                    self.data.append(path + <span class="string">&quot;/&quot;</span> + file)</span><br><span class="line">                    <span class="meta"># path 路径的 最后一个名录名称</span></span><br><span class="line">                    label_number = label_type(path[<span class="number">-1</span>])</span><br><span class="line">                    <span class="meta"># self.targets 列表存储了所有的 事件流二进制文件 对应的标签</span></span><br><span class="line">                    self.targets.append(label_number)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                Returns:</span></span><br><span class="line"><span class="string">                    a tuple of (events, target) where target is the index of the target class.</span></span><br><span class="line"><span class="string">                    (events, target)的元组，其中target是目标类的索引</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="meta"># events 得到 self.data 对应 index 索引下的 事件流文件</span></span><br><span class="line">        events = read_mnist_file(self.data[index], dtype=self.dtype)</span><br><span class="line">        <span class="keyword">if</span> self.first_saccade_only:</span><br><span class="line">            events = events[events[<span class="string">&quot;t&quot;</span>] &lt; <span class="number">1e5</span>]</span><br><span class="line">        <span class="meta"># target 得到 self.targets 对应 index 索引下的 事件流文件所对应的标签</span></span><br><span class="line">        target = self.targets[index]</span><br><span class="line">        <span class="keyword">if</span> self.transform is not None:</span><br><span class="line">            events = self.transform(events)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform is not None:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line">        # 对事件流与标签同时进行transform</span><br><span class="line">        <span class="keyword">if</span> self.transforms is not None:</span><br><span class="line">            events, target = self.transforms(events, target)</span><br><span class="line">        <span class="keyword">return</span> events, target</span><br><span class="line"></span><br><span class="line">    # 得到文件中事件流的文件的个数</span><br><span class="line">    def __len__(self) -&gt; <span class="type">int</span>:</span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br></pre></td></tr></table></figure><p><strong>测试程序：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    对 MYDATASET 进行测试</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tonic</span><br><span class="line">import tonic.transforms as transforms</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">from MYDATASET import MYDATASET</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = MYDATASET(save_to=<span class="string">&quot;/home/zxz/Proj/DP/Do/demo_03/Tonic_dir/tutorials/data/rand_by_myself&quot;</span>,train=True)</span><br><span class="line"></span><br><span class="line">print(dataset[<span class="number">7</span>])</span><br><span class="line">events,target = dataset[<span class="number">7</span>]</span><br><span class="line">print(len(events))</span><br><span class="line">print(events)</span><br><span class="line">print(target)</span><br></pre></td></tr></table></figure><p><strong>测试效果:</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302071955413.png" alt="image-20230207195517384"></p><p><strong>注意：</strong>数据集的目录文件结构如下，其中<code>0、1</code>为标签</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202302071957630.png" alt="image-20230207195739596"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> SNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SNN工具链 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用numpy创建三层神经网络</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pytorch/%E4%BD%BF%E7%94%A8numpy%E5%88%9B%E5%BB%BA%E4%B8%89%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pytorch/%E4%BD%BF%E7%94%A8numpy%E5%88%9B%E5%BB%BA%E4%B8%89%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="numpy网络编程"><a href="#numpy网络编程" class="headerlink" title="numpy网络编程"></a>numpy网络编程</h2><p>本文参考于：书本<a href="https://www.amazon.com/Make-Your-Own-Neural-Network/dp/1530826608/r">《Make Your Own Neural Network》</a>  中文《Python神经网络编程》</p><p><a href="https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork">书中源代码</a></p><p><a href="https://github.com/maxswordsman/Create_Neural_Network">本文代码托管github</a></p><p>通过此次实践对于神经网络的认识更加深刻，通过数学推导到代码实践让自己有了比较大的收获</p><h3 id="一、数学推导"><a href="#一、数学推导" class="headerlink" title="一、数学推导"></a>一、数学推导</h3><p><strong>构造三层神经网络</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163043.png" alt="image-20230303163035955"></p><h4 id="1-前向传播"><a href="#1-前向传播" class="headerlink" title="1.前向传播"></a>1.前向传播</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163105.png" alt="image-20230303163104956"></p><h4 id="2-反向传播"><a href="#2-反向传播" class="headerlink" title="2.反向传播"></a>2.反向传播</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163132.png" alt="image-20230303163132514"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163226.png" alt="image-20230303163154712"></p><h4 id="3-更新权重"><a href="#3-更新权重" class="headerlink" title="3.更新权重"></a>3.更新权重</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163744.png" alt="image-20230303163743986"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163802.png" alt="image-20230303163802729"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163823.png" alt="image-20230303163822906"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303163838.png" alt="image-20230303163838120"></p><h3 id="二、代码实现"><a href="#二、代码实现" class="headerlink" title="二、代码实现"></a>二、代码实现</h3><h4 id="1-网络实现以及训练过程（Network-py）"><a href="#1-网络实现以及训练过程（Network-py）" class="headerlink" title="1.网络实现以及训练过程（Network.py）"></a>1.网络实现以及训练过程（<code>Network.py</code>）</h4><p>该部分对上述数学推导过程进行代码实现，构建一个三层的神经网络，其中隐含层的节点<code>self.hnodes</code>可以自己进行设置，因为网络主要使用的MNIST手写数字数据集，输入为（28，28）的灰度图片，但是要对其进行展平，变为784个输入数据；输出为10个节点，因为预测结果有0到9 十种可能</p><p><strong>网络实现：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    创建三层神经网络模型</span></span><br><span class="line"><span class="string">    用于训练  MNIST 数据集</span></span><br><span class="line"><span class="string">    书中的源码:https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part2_neural_network_mnist_data.ipynb</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import numpy</span><br><span class="line"># Sigmod() 函数定义在scipy包里面，其输入可以直接 为矩阵</span><br><span class="line">import scipy.special</span><br><span class="line"># 绘图</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">neuralNetwork</span>:</span></span><br><span class="line">    def __init__(self,inputnodes,hiddennodes,outputnodes,learningrate):</span><br><span class="line">        self.inodes = inputnodes</span><br><span class="line">        self.hnodes = hiddennodes</span><br><span class="line">        self.onodes = outputnodes</span><br><span class="line"></span><br><span class="line">        <span class="meta"># learning rate</span></span><br><span class="line">        self.lr = learningrate</span><br><span class="line"></span><br><span class="line">        # 隐藏层与输入层之间wih  以及 隐藏层与输出层之间 的初始权重矩阵</span><br><span class="line">        self.wih = numpy.random.normal(<span class="number">0.0</span>,<span class="built_in">pow</span>(self.inodes,<span class="number">-0.5</span>),(self.hnodes,self.inodes))</span><br><span class="line">        self.who = numpy.random.normal(<span class="number">0.0</span>,<span class="built_in">pow</span>(self.hnodes,<span class="number">-0.5</span>),(self.onodes,self.hnodes))</span><br><span class="line"></span><br><span class="line">        # 激活函数</span><br><span class="line">        self.activation_function = lambda x: scipy.special.expit(x)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># train the neural network</span></span><br><span class="line">    def train(self,inputs_list,targets_list):</span><br><span class="line">        # 将输入转为<span class="number">2</span>d 矩阵</span><br><span class="line">        inputs = numpy.<span class="built_in">array</span>(inputs_list,ndmin=<span class="number">2</span>).T</span><br><span class="line">        targets = numpy.<span class="built_in">array</span>(targets_list,ndmin=<span class="number">2</span>).T</span><br><span class="line"></span><br><span class="line">        # 计算隐藏层的信号加权和</span><br><span class="line">        hidden_inputs = numpy.dot(self.wih,inputs)</span><br><span class="line">        # 对加权和的值 使用激活函数</span><br><span class="line">        hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line"></span><br><span class="line">        # 计算输出层的信号加权和</span><br><span class="line">        final_inputs = numpy.dot(self.who,hidden_outputs)</span><br><span class="line">        # 对加权和的值 使用激活函数</span><br><span class="line">        final_outputs = self.activation_function(final_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="meta"># output layer <span class="keyword">error</span> is the  (target - actual)</span></span><br><span class="line">        output_errors = targets - final_outputs</span><br><span class="line">        # 隐藏层的误差 是对输出层误差按照权重进行分割重组得到的</span><br><span class="line">        hidden_errors = numpy.dot(self.who.T,output_errors)</span><br><span class="line"></span><br><span class="line">        # 更新隐含层与输出层之间的权重</span><br><span class="line">        self.who += self.lr * numpy.dot((output_errors * final_outputs * (<span class="number">1.0</span> - final_outputs)),</span><br><span class="line">                                        numpy.transpose(hidden_outputs))</span><br><span class="line">        # 更新隐含层与输入层之间的权重</span><br><span class="line">        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (<span class="number">1.0</span> - hidden_outputs)),</span><br><span class="line">                                       numpy.transpose(inputs))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br><span class="line"></span><br><span class="line">    # 测试网络</span><br><span class="line">    def query(self,inputs_list):</span><br><span class="line">        # 将输入转化为<span class="number">2</span>D 矩阵</span><br><span class="line">        inputs = numpy.<span class="built_in">array</span>(inputs_list,ndmin=<span class="number">2</span>).T</span><br><span class="line"></span><br><span class="line">        # 将输入信号的加权和 输入之隐藏层</span><br><span class="line">        hidden_inputs = numpy.dot(self.wih,inputs)</span><br><span class="line">        # 是 加权使用 激活函数</span><br><span class="line">        hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line"></span><br><span class="line">        # 对隐藏层的输出信号进行加权和</span><br><span class="line">        final_inputs = numpy.dot(self.who,hidden_outputs)</span><br><span class="line">        # 对加权和的信号  使用激活函数</span><br><span class="line">        final_outputs = self.activation_function(final_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br></pre></td></tr></table></figure><p><strong>主程序</strong></p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="meta"># 输入为 28*28 = 784 单通道</span></span><br><span class="line">    input_nodes = <span class="number">784</span></span><br><span class="line">    hidden_nodes = <span class="number">200</span></span><br><span class="line">    <span class="meta"># 输出为 0-9 10 个预测数字</span></span><br><span class="line">    output_nodes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">#  学习率</span></span><br><span class="line">    learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="meta"># 创建网络实例</span></span><br><span class="line">    n = neuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># 加载 MNIST 训练的数据集</span></span><br><span class="line">    training_data_file = open(<span class="string">&quot;/home/zxz/Proj/deeplearning/Create_neural_network/Mnist_dateset/mnist_train.csv&quot;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="meta"># 在csv 类型文本中 mnist 数据集 每一行 代表一张单通道图片 其中的内容为 图片像素矩阵的各个像素值，并且存入列表</span></span><br><span class="line">    training_data_list = training_data_file.readlines()</span><br><span class="line">    training_data_file.close()</span><br><span class="line"></span><br><span class="line">    <span class="meta"># 加载mnist 测试数据集</span></span><br><span class="line">    test_data_file = open(<span class="string">&quot;/home/zxz/Proj/deeplearning/Create_neural_network/Mnist_dateset/mnist_test.csv&quot;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    test_data_list = test_data_file.readlines()</span><br><span class="line">    test_data_file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta"># 训练神经网络</span></span><br><span class="line">    print(<span class="string">&quot;Training ...................................&quot;</span>)</span><br><span class="line">    epochs = <span class="number">5</span></span><br><span class="line">    <span class="function"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="title">range</span>(<span class="params">epochs</span>):</span></span><br><span class="line"><span class="function">        scorecard</span> = []  <span class="meta"># 用于存储每一轮的训练正确与否的结果</span></span><br><span class="line">        <span class="meta"># 遍历训练数据(列表)并对其进行 数据处理  --- 对数据仅仅训练了一轮</span></span><br><span class="line">        <span class="keyword">for</span> recode <span class="keyword">in</span> training_data_list:</span><br><span class="line">            <span class="meta"># 每一行数据 之间的像素值 以 “,“ 分割开来</span></span><br><span class="line">            <span class="meta"># 以 “,” 将每一行的 像素矩阵的值进行分割，并且将值存入列表</span></span><br><span class="line">            all_values = recode.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            <span class="meta"># 正确的标签是 数组中的第一个元素</span></span><br><span class="line">            correct_label = <span class="built_in">int</span>(all_values[<span class="number">0</span>])</span><br><span class="line">            <span class="meta"># 对数据进行归一化以及偏移0.01 防止0输入导致权值无法更新</span></span><br><span class="line">            inputs = (numpy.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br><span class="line">            <span class="meta"># 创建期望输出值 （所有的都为0.01 只有期望的标签对应的值 为 0.99）</span></span><br><span class="line">            targets = numpy.zeros(output_nodes) + <span class="number">0.01</span></span><br><span class="line">            <span class="meta"># 列表的第一个元素all_values[0]为 每一个图片的 标签</span></span><br><span class="line">            targets[<span class="built_in">int</span>(all_values[<span class="number">0</span>])] = <span class="number">0.99</span></span><br><span class="line">            <span class="meta"># 使用训练函数进行训练</span></span><br><span class="line">            outputs = n.train(inputs, targets)</span><br><span class="line">            <span class="meta"># 得到输出结果中得分最好的索引</span></span><br><span class="line">            result = numpy.argmax(outputs)</span><br><span class="line">            <span class="keyword">if</span>(result == correct_label):</span><br><span class="line">                scorecard.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                scorecard.append(<span class="number">0</span>)</span><br><span class="line">        <span class="meta"># 计算本轮训练中的正确率</span></span><br><span class="line">        scorecard_array = numpy.asarray(scorecard)</span><br><span class="line">        print(r<span class="string">&quot;Epoch &#123;&#125;  Training performance = &#123;&#125;&quot;</span>.format((epoch+<span class="number">1</span>),scorecard_array.sum() / scorecard_array.size))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;Testing ................................&quot;</span>)</span><br><span class="line">    <span class="meta"># 测试神经网络</span></span><br><span class="line">    scorecard = []   <span class="meta"># 用于存储每一轮的预测正确与否的结果</span></span><br><span class="line">    <span class="keyword">for</span> recode <span class="keyword">in</span> test_data_list:</span><br><span class="line">        all_values = recode.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="meta"># 正确的标签是 数组中的第一个元素</span></span><br><span class="line">        correct_label = <span class="built_in">int</span>(all_values[<span class="number">0</span>])</span><br><span class="line">        inputs = (numpy.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br><span class="line">        <span class="meta"># 测试网络</span></span><br><span class="line">        outputs = n.query(inputs)</span><br><span class="line">        <span class="meta"># 得到输出结果中 得分最高的索引位置</span></span><br><span class="line">        label = numpy.argmax(outputs)</span><br><span class="line">        <span class="keyword">if</span>(label == correct_label):</span><br><span class="line">            scorecard.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scorecard.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># 计算 该论测试中的 正确率</span></span><br><span class="line">    scorecard_array = numpy.asarray(scorecard)</span><br><span class="line">    print(<span class="string">&quot;Testing performance = &quot;</span>,scorecard_array.sum() / scorecard_array.size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string"># 可视化数据集中的图片</span></span><br><span class="line"><span class="string">    all_v = test_data_list[0].split(&quot;</span>,<span class="string">&quot;)</span></span><br><span class="line"><span class="string">    image_array = ((np.asfarray(all_values[1:])/255.0 * 0.99) + 0.01).reshape(28,28)</span></span><br><span class="line"><span class="string">    plt.imshow(image_array,cmap=&#x27;Greys&#x27;,interpolation=&#x27;None&#x27;)</span></span><br><span class="line"><span class="string">    plt.show()</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>运行结果：</strong>测试的正确率可以达到97%左右</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230303165100.png" alt="image-20230303165059981"></p><h4 id="2-构建自己的手写数字数据集"><a href="#2-构建自己的手写数字数据集" class="headerlink" title="2.构建自己的手写数字数据集"></a>2.构建自己的手写数字数据集</h4><p>将自己手写的数字，进行数据处理变为合适网络的输入</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    将自己手写的数字(PNG存储在my_image中)进行处理并且进行数据存储(存储到training_data_list中)</span></span><br><span class="line"><span class="string">        1.书中的源码:https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part3_load_own_images.ipynb</span></span><br><span class="line"><span class="string">        2.将自己创建的数字图片存储在 my_image，每张图片对应的label为图片的上层文件夹，图片格式为xxx.PNG</span></span><br><span class="line"><span class="string">        文件结构;</span></span><br><span class="line"><span class="string">         my_image</span></span><br><span class="line"><span class="string">            ├── 0</span></span><br><span class="line"><span class="string">                └── 0.png</span></span><br><span class="line"><span class="string">            ├── 1</span></span><br><span class="line"><span class="string">                └── 1.png</span></span><br><span class="line"><span class="string">            ├── 2</span></span><br><span class="line"><span class="string">                └── 2.png</span></span><br><span class="line"><span class="string">            ├── 3</span></span><br><span class="line"><span class="string">                └── 3.png</span></span><br><span class="line"><span class="string">            ├── 4</span></span><br><span class="line"><span class="string">                └── 4.png</span></span><br><span class="line"><span class="string">            ├── 5</span></span><br><span class="line"><span class="string">                └── 5.png</span></span><br><span class="line"><span class="string">            ├── 6</span></span><br><span class="line"><span class="string">                └── 6.png</span></span><br><span class="line"><span class="string">            ├── 7</span></span><br><span class="line"><span class="string">                └── 7.png</span></span><br><span class="line"><span class="string">            ├── 8</span></span><br><span class="line"><span class="string">                └── 8.png</span></span><br><span class="line"><span class="string">            └── 9</span></span><br><span class="line"><span class="string">                └── 9.png</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">import numpy</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import PIL</span><br><span class="line">import os</span><br><span class="line">import torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def <span class="title function_">my_image</span><span class="params">(file_path)</span>:</span><br><span class="line">    # 将图片放缩至 （28，28）</span><br><span class="line">    ReSize = torchvision.transforms.Resize((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    # 用于存储数据的列表</span><br><span class="line">    training_data_list = []</span><br><span class="line">    # 将路径下的文件转化 列表 --- 图片的标签列表</span><br><span class="line">    num_class = [cla <span class="keyword">for</span> cla in os.listdir(file_path)]</span><br><span class="line">    <span class="keyword">for</span> cla in num_class:</span><br><span class="line">        # 每一个标签文件夹</span><br><span class="line">        cla_path = os.path.join(file_path,cla)</span><br><span class="line">        # 每一个标签文件夹下的图片列表</span><br><span class="line">        images = os.listdir(cla_path)</span><br><span class="line">        <span class="keyword">for</span> image in images:</span><br><span class="line">            # 每一张图片的路径</span><br><span class="line">            image_path = os.path.join(cla_path,image)</span><br><span class="line"></span><br><span class="line">            # 开始对数据进行处理</span><br><span class="line">            # 将图片转为灰度图片</span><br><span class="line">            image_array = PIL.Image.open(image_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">            # 将图片的大小缩放到 （<span class="number">28</span>，<span class="number">28</span>）</span><br><span class="line">            image_array_crop = ReSize(image_array)</span><br><span class="line">            # 将 PIL.Image.Image 数据类型变为 ndarry 二维数组类型 ---- 并将其展平为一维数组(<span class="number">784</span>)列 ---网络中输入数据的固定格式</span><br><span class="line">            image_data = numpy.asfarray(image_array_crop).reshape(<span class="number">784</span>)</span><br><span class="line">            # 像素值<span class="number">0</span>表示黑色  <span class="number">255</span>表示白色 但是 MNIST 中相反因此需要用 <span class="number">255</span>-image_data</span><br><span class="line">            image_data = <span class="number">255</span> - image_data</span><br><span class="line">            # 数据归一化并且进行偏移<span class="number">0.01</span>，防止<span class="number">0</span>输入造成梯度消失</span><br><span class="line">            image_data = (image_data/<span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br><span class="line">            record = numpy.append(<span class="type">float</span>(cla),image_data)</span><br><span class="line">            train_data_list.append(record)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> training_data_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 与 Network.py中的 train_data_list 效果一致</span><br><span class="line">training_data_list = my_image(<span class="string">&quot;/home/zxz/Proj/deeplearning/Create_neural_network/my_image&quot;</span>)</span><br><span class="line">print(training_data_list[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string"># 可视化图片</span></span><br><span class="line"><span class="string">    plt.imshow(image_data.reshape(28,28), cmap=&#x27;Greys&#x27;, interpolation=&#x27;None&#x27;)</span></span><br><span class="line"><span class="string">    plt.show()</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlexNet</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/AlexNET/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/AlexNET/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="AlexNET"><a href="#AlexNET" class="headerlink" title="AlexNET"></a>AlexNET</h2><h3 id="一、预备知识"><a href="#一、预备知识" class="headerlink" title="一、预备知识"></a>一、预备知识</h3><h4 id="1-网络可视化"><a href="#1-网络可视化" class="headerlink" title="1.网络可视化"></a>1.网络可视化</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchsummary</span><br></pre></td></tr></table></figure><p><strong>输入：</strong>为模型、输入尺寸、批数量、设备 </p><p><strong>输出：</strong>模型的参数信息</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torchsummary import summary</span><br><span class="line"></span><br><span class="line">def <span class="title function_">summary</span><span class="params">(model, input_size, batch_size=<span class="number">-1</span>, device=<span class="string">&quot;cuda&quot;</span>)</span>  # 函数默认是cuda，若是在cpu下就需要修改</span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchsummary import summary</span><br><span class="line">from torchvision.models import vgg16  # 以 vgg16 为例</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">myNet = vgg16()  # 实例化网络，可以换成自己的网络</span><br><span class="line"># 将模型移动到gpu上</span><br><span class="line">myNet = myNet.to(device)</span><br><span class="line">summary(myNet, (<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>))  # 输出网络结构</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305062235519.png" alt="image-20230506223506480"></p><h4 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2.数据集"></a>2.数据集</h4><p>百度网盘下载地址：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">链接: https:<span class="comment">//pan.baidu.com/s/1Uro6RuEbRGGCQ8iXvF2SAQ 密码: hl31</span></span><br></pre></td></tr></table></figure><p><code>ImageNet</code> 数据集太大了1000类别，而且达到100多G的大小，因此换成<code>Mini-ImageNet</code>测试网络</p><p><code>Mini-ImageNet</code>数据集大约3G左右，100个类别，每一个类别均有600张图片左右，共60000张图片，而且图片都是可变分辨率的（图片大小尺寸不固定）</p><p>数据集的结构：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── mini-imagenet: 数据集根目录</span><br><span class="line">     ├── images: 所有的图片都存在这个文件夹中</span><br><span class="line">     ├── train.csv: 对应训练集的标签文件</span><br><span class="line">     ├── val.csv: 对应验证集的标签文件</span><br><span class="line">     └── test.csv: 对应测试集的标签文件</span><br></pre></td></tr></table></figure><p><code>Mini-Imagenet</code>数据集中包含了<code>train.csv</code>、<code>val.csv</code>以及<code>test.csv</code>三个文件,但是提供的标签文件并不是从每个类别中进行采样的，因此无法直接用于训练分类，</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.csv 包含<span class="number">38400</span>张图片，共<span class="number">64</span>个类别</span><br><span class="line">val.csv   包含<span class="number">9600</span>张图片，共<span class="number">16</span>个类别</span><br><span class="line">test.csv  包含<span class="number">12000</span>张图片，共<span class="number">20</span>个类别</span><br></pre></td></tr></table></figure><p>按照上述链接下载文件之后，对images进行解压，在使用<code>panads</code>对数据集进行分割，需要自己构建一个新的<code>new_train.csv</code>与<code>new_val.csv</code>以<code>new_test.val</code>，代码中<code>imagenet_class_index.json</code>的下载地址为：<a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/pytorch_classification/mini_imagenet/imagenet_class_index.json">json</a></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.将train.csv与val.csv以及test.csv 进行合并(乱序)之后再按照比例进行分割为</span></span><br><span class="line"><span class="string">        new_train.csv与new_val.csv以及new_test.csv</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">print(BASE_DIR)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    读取csv下的分类</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def read_csv_classes(csv_dir: str, csv_name: str):</span><br><span class="line">    # 读取mini-imagenet 下的csv文件</span><br><span class="line">    data = pd.read_csv(os.path.join(csv_dir, csv_name))</span><br><span class="line"></span><br><span class="line">    # 得到csv文件下的label列的元素  并对其进行去重 drop_duplicates()</span><br><span class="line">    label_set = <span class="built_in">set</span>(data[<span class="string">&quot;label&quot;</span>].drop_duplicates().values)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;&#123;&#125; have &#123;&#125; images and &#123;&#125; classes.&quot;</span>.format(csv_name,</span><br><span class="line">                                                     data.shape[<span class="number">0</span>],</span><br><span class="line">                                                     len(label_set)))</span><br><span class="line">    <span class="keyword">return</span> data, label_set</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    进行分割数据集  6:2:2</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def calculate_split_info(path: str, save_path:str,label_dict: dict, rate1: <span class="type">float</span> = <span class="number">0.2</span>,rate2: <span class="type">float</span> = <span class="number">0.2</span>):</span><br><span class="line">    # image_dir 为mini-imagenet 下的images路径 存放的是所有图片</span><br><span class="line">    image_dir = os.path.join(path, <span class="string">&quot;images&quot;</span>)</span><br><span class="line">    # 得到image_dir路径下以jpg为后缀的文件的列表</span><br><span class="line">    images_list = [i <span class="keyword">for</span> i in os.listdir(image_dir) <span class="keyword">if</span> i.endswith(<span class="string">&quot;.jpg&quot;</span>)]</span><br><span class="line">    # 输出数据集中的图片数量</span><br><span class="line">    print(<span class="string">&quot;find &#123;&#125; images in dataset.&quot;</span>.format(len(images_list)))</span><br><span class="line"></span><br><span class="line">    train_data, train_label = read_csv_classes(path, <span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line">    val_data, val_label = read_csv_classes(path, <span class="string">&quot;val.csv&quot;</span>)</span><br><span class="line">    test_data, test_label = read_csv_classes(path, <span class="string">&quot;test.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    # 得到 train test val 三个数据集中的标签  总共为 <span class="number">100</span>类</span><br><span class="line">    labels = (train_label | val_label | test_label)</span><br><span class="line">    labels = <span class="built_in">list</span>(labels)</span><br><span class="line">    labels.sort()</span><br><span class="line">    print(<span class="string">&quot;all classes: &#123;&#125;&quot;</span>.format(len(labels)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #  得到类似于 <span class="string">&#x27;n01532829&#x27;</span>: [<span class="number">0</span>, <span class="string">&#x27;house_finch&#x27;</span>] 这样格式的字典</span><br><span class="line">    classes_label = dict([(label, [index, label_dict[label]]) <span class="keyword">for</span> index, label in enumerate(labels)])</span><br><span class="line">    # 将得到的字典写入json文件中</span><br><span class="line">    json_str = json.dumps(classes_label, indent=<span class="number">4</span>)</span><br><span class="line">    with open(<span class="string">&#x27;./Data/classes_name.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) as json_file:</span><br><span class="line">        json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">    # 将train.csv  test.csv  val.csv 三个文件的内容拼接到一起 （得到所有数据的csv文件  里面的样本数量总共为<span class="number">60000</span>）</span><br><span class="line">    <span class="meta"># pd.concat()函数可以沿着指定的轴将多个dataframe或者series拼接到一起</span></span><br><span class="line">    data = pd.concat([train_data, val_data, test_data], axis=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">&quot;total data shape: &#123;&#125;&quot;</span>.format(data.shape))</span><br><span class="line"></span><br><span class="line">    # 在每一个类别中分割数据集</span><br><span class="line">    num_every_classes = []</span><br><span class="line">    split_train_data = []</span><br><span class="line">    split_val_data = []</span><br><span class="line">    split_test_data = []</span><br><span class="line">    <span class="keyword">for</span> label in labels:</span><br><span class="line">        # class_data 为每个类 对应的图片的图片的DataFrame</span><br><span class="line">        # 每个类别的图片数量为 <span class="number">600</span></span><br><span class="line">        class_data = data[data[<span class="string">&quot;label&quot;</span>] == label]</span><br><span class="line"></span><br><span class="line">        num_every_classes.append(class_data.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #  乱序</span><br><span class="line">        # DataFrame.sample 可用来对DataFrame进行随机抽样</span><br><span class="line">        <span class="meta"># frac 参数用于指定抽取的样本条数的比例（这里的代码表示全部进行抽取即乱序）</span></span><br><span class="line">        # random_state 参数 可以复现抽样结果 第二次与第一次抽取的结果一致</span><br><span class="line">        shuffle_data = class_data.sample(frac=<span class="number">1</span>, random_state=<span class="number">1</span>)</span><br><span class="line">        # 分割比例</span><br><span class="line">        num_train_sample = <span class="type">int</span>(class_data.shape[<span class="number">0</span>] * (<span class="number">1</span> - rate1 - rate2))</span><br><span class="line">        num_val_sample = <span class="type">int</span>(class_data.shape[<span class="number">0</span>] * rate1)</span><br><span class="line">        new_test_sample = <span class="type">int</span>(class_data.shape[<span class="number">0</span>] * rate2)</span><br><span class="line">        # 因为每个类别为<span class="number">600</span>张  对每个类别求的分割的比例</span><br><span class="line">        split_train_data.append(shuffle_data[:num_train_sample])</span><br><span class="line">        split_val_data.append(shuffle_data[num_train_sample:(num_val_sample + num_train_sample)])</span><br><span class="line">        split_test_data.append(shuffle_data[(num_val_sample + num_train_sample):<span class="type">int</span>(class_data.shape[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">        <span class="meta"># imshow</span></span><br><span class="line">        imshow_flag = False</span><br><span class="line">        <span class="keyword">if</span> imshow_flag:</span><br><span class="line">            img_name, img_label = shuffle_data.iloc[<span class="number">0</span>].values</span><br><span class="line">            img = Image.open(os.path.join(image_dir, img_name))</span><br><span class="line">            plt.imshow(img)</span><br><span class="line">            plt.title(<span class="string">&quot;class: &quot;</span> + classes_label[img_label][<span class="number">1</span>])</span><br><span class="line">            plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="meta"># plot classes distribution</span></span><br><span class="line">    plot_flag = False</span><br><span class="line">    <span class="keyword">if</span> plot_flag:</span><br><span class="line">        plt.bar(range(<span class="number">1</span>, <span class="number">101</span>), num_every_classes, align=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="meta"># concatenate data 将分割的数据集 创建一个新的csv文件  并将其内容进行拼接</span></span><br><span class="line">    new_train_data = pd.concat(split_train_data, axis=<span class="number">0</span>)</span><br><span class="line">    new_val_data = pd.concat(split_val_data, axis=<span class="number">0</span>)</span><br><span class="line">    new_test_data = pd.concat(split_test_data, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># save new csv data</span></span><br><span class="line">    new_train_data.to_csv(os.path.join(save_path, <span class="string">&quot;new_train.csv&quot;</span>))</span><br><span class="line">    new_val_data.to_csv(os.path.join(save_path, <span class="string">&quot;new_val.csv&quot;</span>))</span><br><span class="line">    new_test_data.to_csv(os.path.join(save_path, <span class="string">&quot;new_test.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    data_dir = <span class="string">&quot;/media/zxz/新加卷/DataSET/mini-imagenet/&quot;</span>  # 指向数据集的根目录</span><br><span class="line">    json_path = <span class="string">&quot;./Data/imagenet_class_index.json&quot;</span>  # 指向imagenet的索引标签文件</span><br><span class="line">    save_path = <span class="string">&quot;./Data&quot;</span>   # 创建的new_train.csv  与  ne_val.csv 需要保留的地址</span><br><span class="line"></span><br><span class="line">    <span class="meta"># load imagenet labels</span></span><br><span class="line">    label_dict = json.load(open(json_path, <span class="string">&quot;r&quot;</span>))</span><br><span class="line">    # 得到一个字典，其中键代表 label  而值代表label对应的事物的英语单词 如： <span class="string">&#x27;n01440764&#x27;</span>: <span class="string">&#x27;tench&#x27;</span></span><br><span class="line">    label_dict = dict([(v[<span class="number">0</span>], v[<span class="number">1</span>]) <span class="keyword">for</span> k, v in label_dict.items()])</span><br><span class="line"></span><br><span class="line">    calculate_split_info(data_dir, save_path,label_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>一般训练集、验证集、测试集按照<code>6:2:2</code>的比例进行分割,分割后得到的<code>csv</code>文件如下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_train.csv 包含<span class="number">36000</span>张图片，共<span class="number">100</span>个类别</span><br><span class="line">new_val.csv   包含<span class="number">12000</span>张图片，共<span class="number">100</span>个类别</span><br><span class="line">new_test.csv  包含<span class="number">12000</span>张图片，共<span class="number">100</span>个类别</span><br></pre></td></tr></table></figure><p>根据创建的csv文件划分为原始如下形式：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Mini-imageNet</span><br><span class="line">    new_train</span><br><span class="line">    class1_dir</span><br><span class="line">   class2_dir</span><br><span class="line">    ...</span><br><span class="line">   new_val</span><br><span class="line">    class1_dir</span><br><span class="line">   class2_dir</span><br><span class="line">    ...</span><br><span class="line">   new_test</span><br><span class="line">    class1_dir</span><br><span class="line">   class2_dir</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    根据split_dataset1 新创建的csv文件   对原始的数据集进行分割</span></span><br><span class="line"><span class="string">        1.根据new_train.csv 文件中的图片 创建new_train文件夹 将图片复制到 new_train文件夹的对应的label下的文件夹下</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def <span class="title function_">copy_to_move</span><span class="params">(base_path:str,root_path:str,csv_path:str,move_to_dir:str)</span>:</span><br><span class="line">    # 读取csv文件  获取所有的img文件名称</span><br><span class="line">    handle_csv = os.path.join(base_path, csv_path)</span><br><span class="line">    data = pd.read_csv(handle_csv)</span><br><span class="line"></span><br><span class="line">    # 将csv文件中的图片名字  装入列表</span><br><span class="line">    handle_filename = <span class="built_in">list</span>(data[<span class="string">&quot;filename&quot;</span>].values)</span><br><span class="line">    handle_label = <span class="built_in">list</span>(data[<span class="string">&quot;label&quot;</span>].values)   <span class="meta">#  classes = 100</span></span><br><span class="line">    print(<span class="string">&quot;the train_cav data num is &#123;&#125; classes is &#123;&#125;&quot;</span>.format(len(handle_filename), len(<span class="built_in">set</span>(handle_label))))</span><br><span class="line"></span><br><span class="line">    dst = move_to_dir   #  提前创建一个new_train or new_test or new_val 文件夹，将CSV对应的img 复制到文件夹中</span><br><span class="line">    <span class="keyword">for</span> i, name in enumerate(handle_filename):</span><br><span class="line">        imgx = os.path.join(root_path, name)</span><br><span class="line">        print(f<span class="string">&quot;第&#123;i&#125;张图片已经copy完成&quot;</span>)</span><br><span class="line">        print(imgx)</span><br><span class="line">        shutil.copy(imgx, dst)</span><br><span class="line"></span><br><span class="line">    files = os.listdir(dst)  # 上一步创建的文件夹</span><br><span class="line">    pre = dst</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, img in enumerate(files):</span><br><span class="line">        # <span class="number">1.</span> 首先遍历每个文件，创建文件夹</span><br><span class="line">        #  n0153282900000138.jpg</span><br><span class="line">        dir_name = img.split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>][:<span class="number">9</span>]  # 这里就是为了截取label，根据img name 前<span class="number">9</span>个为label</span><br><span class="line">        dir_path = os.path.join(pre,dir_name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> not os.path.exists(dir_path):</span><br><span class="line">            os.mkdir(dir_path)  # 创建该类文件夹</span><br><span class="line"></span><br><span class="line">        #  直接判断该文件，归类</span><br><span class="line">        img_path = os.path.join(pre,img)</span><br><span class="line">        <span class="keyword">if</span> not os.path.isdir(img_path):</span><br><span class="line">            <span class="keyword">if</span> img[:<span class="number">9</span>] == dir_name:  # 由于每个类包含很多img文件，判断该文件是否属于该类</span><br><span class="line">                shutil.move(img_path, dir_path)  <span class="meta"># true的话，移动到该类目录</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    # 当前程序文件所在的目录</span><br><span class="line">    BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">    print(BASE_DIR)</span><br><span class="line"></span><br><span class="line">    <span class="meta"># mini-imagenet数据集原始的images文件下</span></span><br><span class="line">    root_path_images = <span class="string">&quot;/media/zxz/新加卷/DataSET/mini-imagenet/images&quot;</span></span><br><span class="line">    new_train_csv_path = <span class="string">&quot;./Mini-ImageNet/new_train.csv&quot;</span></span><br><span class="line">    new_val_csv_path = <span class="string">&quot;./Mini-ImageNet/new_val.csv&quot;</span></span><br><span class="line">    new_test_csv_path = <span class="string">&quot;./Mini-ImageNet/new_test.csv&quot;</span></span><br><span class="line"></span><br><span class="line">    new_train_dir = <span class="string">&quot;./Mini-ImageNet/new_train&quot;</span></span><br><span class="line">    new_val_dir = <span class="string">&quot;./Mini-ImageNet/new_val&quot;</span></span><br><span class="line">    new_test_dir = <span class="string">&quot;./Mini-ImageNet/new_test&quot;</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;创建new_train文件夹.....&quot;</span>)</span><br><span class="line">    copy_to_move(BASE_DIR,root_path_images,new_train_csv_path,new_train_dir)</span><br><span class="line">    print()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;创建new_val文件夹.....&quot;</span>)</span><br><span class="line">    copy_to_move(BASE_DIR, root_path_images, new_val_csv_path, new_val_dir)</span><br><span class="line">    print()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;创建new_test文件夹.....&quot;</span>)</span><br><span class="line">    copy_to_move(BASE_DIR, root_path_images, new_test_csv_path, new_test_dir)</span><br><span class="line">    print()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>划分的数据集图片：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305081527871.png" alt="image-20230508152703849"></p><h3 id="二、论文细节"><a href="#二、论文细节" class="headerlink" title="二、论文细节"></a>二、论文细节</h3><h4 id="1-ReLu与Tanh收敛速度的比较"><a href="#1-ReLu与Tanh收敛速度的比较" class="headerlink" title="1.ReLu与Tanh收敛速度的比较"></a>1.<code>ReLu</code>与<code>Tanh</code>收敛速度的比较</h4><p>在论文的3.1节中提到ReLu相较于Tanh收敛速度更快，且ReLu无需对输入数据进行归一化防止饱和，在不对数据进行归一化的情况下，比较如下：</p><h5 id="（1）代码"><a href="#（1）代码" class="headerlink" title="（1）代码"></a>（1）代码</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">代码功能:</span></span><br><span class="line"><span class="string">    复现论文中3.1的部分比较</span></span><br><span class="line"><span class="string">        对于特定的四层卷积神经网络 达到25%的训练误差 所迭代的论轮数</span></span><br><span class="line"><span class="string">        没有对数据进行任何的正规化</span></span><br><span class="line"><span class="string">        每个网络的学习速率都是独立选择的，以使训练尽可能快</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.下载数据集</span></span><br><span class="line"><span class="string">        len(train_data) = 50000</span></span><br><span class="line"><span class="string">        len(test_data) = 10000</span></span><br><span class="line"><span class="string">        pic shape = [2,32,32]</span></span><br><span class="line"><span class="string">        classes = 10</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../Cifar-10&quot;</span>,transform=torchvision.transforms.PILToTensor(),train=True,download=True)</span><br><span class="line"></span><br><span class="line"># 利用DataLoader加载数据集</span><br><span class="line">train_dataloader = DataLoader(train_data,batch_size=<span class="number">128</span>,shuffle=True)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    3.搭建四层卷积网络</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 搭建神经网络</span><br><span class="line">class Module(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            # Layer1</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  #  <span class="number">32</span> <span class="number">32</span> <span class="number">32</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>) , # <span class="number">32</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            # Layer2</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            #Layer3</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            # # Layer4</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   # <span class="number">128</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=<span class="number">2</span>),  # <span class="number">128</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            # Linear Layer</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">2</span>*<span class="number">2</span>,<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,input):</span><br><span class="line">        input = self.model(input)</span><br><span class="line">        <span class="keyword">return</span> input</span><br><span class="line"></span><br><span class="line">class Module2(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            # Layer1</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  #  <span class="number">32</span> <span class="number">32</span> <span class="number">32</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>) , # <span class="number">32</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            # Layer2</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">16</span> <span class="number">16</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            #Layer3</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  # <span class="number">64</span> <span class="number">8</span> <span class="number">8</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>,<span class="number">2</span>),stride=<span class="number">2</span>),  # <span class="number">64</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            # # Layer4</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   # <span class="number">128</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=<span class="number">2</span>),  # <span class="number">128</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            # Linear Layer</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">2</span>*<span class="number">2</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,input):</span><br><span class="line">        input = self.model(input)</span><br><span class="line">        <span class="keyword">return</span> input</span><br><span class="line"></span><br><span class="line"><span class="meta"># <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span></span><br><span class="line"><span class="meta">#     x = torch.rand([1, 3, 32, 32])</span></span><br><span class="line"><span class="meta">#     model = Module()</span></span><br><span class="line"><span class="meta">#     y = model(x)</span></span><br><span class="line"><span class="meta">#     print(y.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    4.训练模型</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 定义程序运行设备，若无法使用GPU则在CPU上进行运算</span><br><span class="line">device_1 = <span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">device = torch.device(device_1)</span><br><span class="line"></span><br><span class="line"># 创建网络模型（Relu）</span><br><span class="line">model1 = Module()</span><br><span class="line">model1 = model1.to(device=device)</span><br><span class="line"># （Tanh）</span><br><span class="line">model2 = Module2()</span><br><span class="line">model2 = model2.to(device=device)</span><br><span class="line"></span><br><span class="line"># 定义损失函数</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn = loss_fn.to(device=device)</span><br><span class="line"></span><br><span class="line"># 定义优化器</span><br><span class="line">learn_rate = <span class="number">0.01</span></span><br><span class="line">optimizer1 = torch.optim.SGD(model1.parameters(),lr=learn_rate,momentum=<span class="number">0.9</span>,weight_decay=<span class="number">0.0005</span>)</span><br><span class="line">optimizer2 = torch.optim.SGD(model2.parameters(),lr=learn_rate,momentum=<span class="number">0.9</span>,weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">def train(lun,dataloader,model,loss_fn,optimizer):</span><br><span class="line">    # 将模型转化为训练模式</span><br><span class="line">    model.train()</span><br><span class="line">    loss,acc,step,epoch_error_rate = <span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0</span>,<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data in dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        imgs = imgs.<span class="type">float</span>()</span><br><span class="line">        <span class="meta"># imgs = torch.tensor(np.array(imgs))</span></span><br><span class="line">        <span class="meta"># targets = torch.tensor(np.array(targets))</span></span><br><span class="line">        # 对数据进行GPU加速</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        # 将数据传入网路模型</span><br><span class="line">        output = model(imgs) # 分别得到每一张图片为那一个target的概率值</span><br><span class="line">        # 求解当前损失值(当前批次的损失)</span><br><span class="line">        cur_loss = loss_fn(output,targets)</span><br><span class="line">        # 求解当前训练批次的正确率</span><br><span class="line">        _, pred = torch.max(output, axis=<span class="number">1</span>)</span><br><span class="line">        cur_acc = torch.sum(targets == pred) / output.shape[<span class="number">0</span>]</span><br><span class="line">        # 求解当前训练批次的错误率</span><br><span class="line">        batch_error = torch.sum(targets != pred) / output.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        # 反向传播</span><br><span class="line">        optimizer.zero_grad() # 优化前将梯度清<span class="number">0</span></span><br><span class="line">        cur_loss.backward()        # 反向传播，求得每一个节点的梯度</span><br><span class="line">        optimizer.step()       # 对模型的每一个参数进行优化</span><br><span class="line"></span><br><span class="line">        # 将训练集下的每一轮的每一个批次的的错误率累加（跳出<span class="keyword">for</span>循环最后得到这一轮的总错误率）</span><br><span class="line">        epoch_error_rate += batch_error.item()</span><br><span class="line">        <span class="meta"># step 该训练集目前训练到多少批次</span></span><br><span class="line">        step = step +<span class="number">1</span>  # 本轮样本的训练次数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 本轮训练的每一批次的平均损失</span><br><span class="line">    train_error_rate = epoch_error_rate / step # 本轮训练的平均损失</span><br><span class="line">    print(<span class="string">&quot;train_error_rate: &#123;&#125;&quot;</span>.format(train_error_rate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_error_rate</span><br><span class="line"></span><br><span class="line"># 定义画图函数</span><br><span class="line">def matplot_loss(train_error_relu, train_error_tanh):</span><br><span class="line">    plt.plot(train_error_relu, label=<span class="string">&#x27;error_relu&#x27;</span>)  # 画一个折线名字named = error_relu</span><br><span class="line">    plt.plot(train_error_tanh, label=<span class="string">&#x27;error_tanh&#x27;</span>)      # 画一个折线名字named = error_tanh</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)                    # （说明那条曲线是什么的标签）指定图例的位置。默认为loc=best 左上方</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;train_error_rate&#x27;</span>)                        # 二维图形的y轴名称</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)                       # 二维图形的X轴名称</span><br><span class="line">    plt.title(<span class="string">&quot;train_error_relu vs train_error_tanh&quot;</span>)       # 图的标题</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建列表用于存储数据画图</span><br><span class="line">relu_train_error_rate = []</span><br><span class="line">tanh_train_error_rate = []</span><br><span class="line"></span><br><span class="line"># 训练轮数实现</span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> i in range(epoch):</span><br><span class="line">        print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;-------Relu 第 &#123;&#125; 轮训练开始------&quot;</span>.format(i + <span class="number">1</span>))</span><br><span class="line">        train_error_relu = train(i + <span class="number">1</span>, train_dataloader, model1, loss_fn, optimizer1)</span><br><span class="line">        relu_train_error_rate.append(train_error_relu)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i in range(epoch):</span><br><span class="line">        print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;-------Tanh 第 &#123;&#125; 轮训练开始------&quot;</span>.format(i + <span class="number">1</span>))</span><br><span class="line">        train_error_tanh = train(i + <span class="number">1</span>, train_dataloader, model2, loss_fn, optimizer2)</span><br><span class="line">        tanh_train_error_rate.append(train_error_tanh)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(relu_train_error_rate)</span><br><span class="line">    print(tanh_train_error_rate)</span><br><span class="line"></span><br><span class="line">    matplot_loss(relu_train_error_rate,tanh_train_error_rate)</span><br></pre></td></tr></table></figure><h5 id="（2）结果"><a href="#（2）结果" class="headerlink" title="（2）结果"></a>（2）结果</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091445057.png" alt="image-20230509144434172"></p><p>对上面的结果进行比较可以得到，<code>ReLu</code>的收敛速度在17轮之前确实是优于<code>tanh</code></p><h4 id="2-LRN"><a href="#2-LRN" class="headerlink" title="2.LRN"></a>2.<code>LRN</code></h4><p>论文的3.3接提到的局部响应标准化（<code>LRN</code>）有助于AlexNet泛化能力的提升，受真实的神经元<strong>侧抑制</strong>启发</p><p>**侧抑制:**细胞分化变为不同时，会对周围细胞产生抑制信号，组织他们像相同的方向分化，最终表现为细胞命运的不同</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091459835.png" alt="image-20230509145958807"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091500540.png" alt="image-20230509150018496"></p><ul><li>a 表示<strong>卷积层（包括卷积操作和激活操作）后的输出结果</strong>。这个输出的结果是一个四维数组 [batch,height,width,channel]。这个输出结构中的一个位置 [a,b,c,d]，可以理解成在某一张特征图中的某一个通道下的某个高度和某个宽度位置的点，即<strong>第 a 张特征图的第 d 个通道下的高度为 b 宽度为 c 的点。</strong></li><li><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091502049.png" alt="image-20230509150204010">表示第 i 个通道的特征图在位置（x,y)运用激活函数 ReLU 后的输出。n 是同一位置上临近的 feature map 的数目，N 是特征图的总数。</li></ul><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091504802.png" alt="image-20230509150421758"></p><p>即公式中的分母，若此处的分母越大即表示对该处的像素值抑制程度越大。若<img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091507434.png" alt="image-20230509150754414">周围的deepth_radius范围存在较大的像素值，那么对于<img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091509677.png">的输出存在较大的抑制</p><p>论文中提到使用<code>LRN</code>分别减少了<code>top-1</code>和<code>top-5</code>的1.4%与1.2%的错误率</p><h5 id="（1）Pyotrch中LRN的实现"><a href="#（1）Pyotrch中LRN的实现" class="headerlink" title="（1）Pyotrch中LRN的实现"></a>（1）Pyotrch中LRN的实现</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LocalResponseNorm(size, alpha=<span class="number">0.0001</span>, beta=<span class="number">0.75</span>, k=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091514922.png" alt="image-20230509151431897"></p><p>在2014年的<code>《Very Deep Convolutional Networks for Large-Scale Image Recognition》</code>提到<code>LRN</code>技术实际用处不大</p><h4 id="3-Overall-architecture"><a href="#3-Overall-architecture" class="headerlink" title="3. Overall architecture"></a>3. Overall architecture</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305091538588.png" alt="image-20230509153832557"></p><ul><li>首先输入的是一张<code>224x224x3</code>（因为是彩色&#96;RGB三通道的图）</li><li>第一层用的卷积核的大小是 <code>11∗11∗3 </code>，卷积核的个数是<code>48+48=96</code>，从这一层开始两个<code>GPU</code>开始分开运行，现在定义处理上半层特征图的叫<code>GPU_A</code>，处理下半层特征图的叫<code>GPU_B</code>，每个<code>GPU</code>负责48个卷积核的运算，上半层<code>GPU_A</code>生成48张特征图，下半层<code>GPU_B</code>生成48张特征图。这一层卷积结束之后，还需要<code>LRN</code>（<code>Local Response Normalization </code>局部响应归一化）和<code>Max_Pooling</code>（最大池化）</li><li>第二层和第一层同理，两个<code>GPU</code>分别处理自己上一层传来的<code>output</code>（那48张特征图），卷积核的大小是 <code>5∗5∗48</code> ，然后一共有<code>128+128=256</code>个卷积核，所以两个<code>GPU</code>各自利用自己上一层的<code>output</code>生成<code>128</code>张特征图。这一层的卷积结束之后还需要<code>LRN</code>（<code>Local Response Normalization </code>局部响应归一化）和<code>Max_Pooling</code>（最大池化）</li><li>第三层和前两层不同，这一层两个<code>GPU</code>都要是将两个<code>GPU</code>的上一层的全部输出<code>output</code>作为输入<code>input</code>，所以这一层的卷积核大小是 <code>3∗3∗ （128[来自GPU_A]+128[来自GPU_B]）</code>，也就是这层的卷积核是 <code>3∗3∗256 </code>，而不是像前两层那样只是把自己上一层的输出当成输入，这层一共有<code>192+192=384</code>个卷积核，<code>GPU_A</code>负责前192个卷积核的生成的特征图，<code>GPU_B</code>负责后<code>192</code>个卷积核生成的特征图</li><li>第四层和第五层同第三层</li><li>第六层，接了一个全连接层<code>(FC)</code>，首先将<code>128[来自GPU_A]和128[来自GPU_B]</code>的一共256张特征图拉直成一个超长的向量，连接到一个大小为4096的全连接层中，其中4096个神经元的前2048个神经元由<code>GPU_A</code>运算，后2048个神经元由<code>GPU_B</code>来运算</li><li>第七层和第六层同理</li><li>第八层是再连接到一个大小为1000的全连接层中，用softmax，来算1000种分类的分布</li></ul><h5 id="（1）pytorch代码实现"><a href="#（1）pytorch代码实现" class="headerlink" title="（1）pytorch代码实现"></a>（1）pytorch代码实现</h5><p><strong><code>AlexNet.py</code></strong>,需要注意的是，目前网络全是在一块GPU上进行加速运算的，因此与原来的架构不一样</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    搭建AlexNet网络模型</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.搭建网络模型</span></span><br><span class="line"><span class="string">        # 输入为 224*224*3</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">class <span class="title function_">AlexNet</span><span class="params">(nn.Module)</span>:</span><br><span class="line">    def __<span class="title function_">init__</span><span class="params">(self, num_classes: <span class="type">int</span> = <span class="number">100</span>, dropout: <span class="type">float</span> = <span class="number">0.5</span>)</span> -&gt; None:</span><br><span class="line">        <span class="title function_">super</span><span class="params">()</span>.__<span class="title function_">init__</span><span class="params">()</span></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            # Layer1</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.LocalResponseNorm(size=<span class="number">5</span>,alpha=<span class="number">10e-4</span>,beta=<span class="number">0.75</span>,k=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            # Layer2</span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.LocalResponseNorm(size=<span class="number">5</span>, alpha=<span class="number">10e-4</span>, beta=<span class="number">0.75</span>, k=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            # Layer3</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Layer4</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Layer5</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), # <span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span></span><br><span class="line">        )</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            # Linear1</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Linear2</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            # Linear3</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x= self.flatten(x)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta"># <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span></span><br><span class="line"><span class="meta">#     x = torch.rand([1, 3, 224, 224])</span></span><br><span class="line"><span class="meta">#     model = AlexNet()</span></span><br><span class="line"><span class="meta">#     y = model(x)</span></span><br><span class="line"><span class="meta">#     print(y.shape)</span></span><br></pre></td></tr></table></figure><h4 id="4-Data-Augmentation-数据增强"><a href="#4-Data-Augmentation-数据增强" class="headerlink" title="4. Data Augmentation(数据增强)"></a>4. Data Augmentation(数据增强)</h4><p>论文在训练阶段使用两种数据增强的方式减少数据的过拟合，都允许用很少的计算从原始图像生成转换</p><h5 id="（1）第一种"><a href="#（1）第一种" class="headerlink" title="（1）第一种"></a>（1）第一种</h5><p>从<code>256x256</code>的图像中随机扣下<code>224x224</code>大小的图片，并进行随机的水平翻转，这样相当于将数据增加了<code>2048倍(32x32x2)</code></p><ul><li>（数据保证符合网络期望的输入数据）将短边减少到256，长边也保证高宽比往下降，长边多出来的以中心为界将两个边进行裁剪在第二节 <code>2  The Dataset</code>中提到过，<code>ImageNet</code>是一个可变分辨率的数据集因此，</li></ul><p>实现代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line">    </span><br><span class="line"># 标准化所求的数据集的均值与方差</span><br><span class="line">norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    </span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">256</span>)),      # (<span class="number">256</span>, <span class="number">256</span>) 区别 按照长宽比进行缩放</span><br><span class="line">        transforms.CenterCrop(<span class="number">256</span>),    # 将长边多余的地方进行裁剪</span><br><span class="line">        transforms.RandomCrop(<span class="number">224</span>),    # 随机裁剪<span class="number">224</span>*<span class="number">224</span></span><br><span class="line">        transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),  # 以<span class="number">50</span>%的概率进行水平翻转</span><br><span class="line">        transforms.ToTensor(),                   # 转变为tensor()数据</span><br><span class="line">        transforms.Normalize(norm_mean, norm_std),  # 标准化</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><h5 id="（2）第二种"><a href="#（2）第二种" class="headerlink" title="（2）第二种"></a>（2）第二种</h5><p>第二种方法改变训练图像中<code>RGB</code>通道的强度，对整个<code>ImageNet</code>训练集的<code>RGB</code>像素值集执行<code>PCA</code>主成分分析,然后对主成分上的数进行微小的扰动，以此<strong>图像色彩</strong>就会发生微小的变化，增加图像的丰富性多样性 </p><p><strong>暂时不清楚如何对其进行操作…..</strong>,在<code>AlexNet</code>实现时候效果有限</p><p>同时在测试阶段也有对数据进行的操作：</p><h5 id="（3）测试阶段数据处理"><a href="#（3）测试阶段数据处理" class="headerlink" title="（3）测试阶段数据处理"></a>（3）测试阶段数据处理</h5><p>在测试时，网络通过提取<code>5个224 × 224</code>的patch(四个角斑和中心斑)及其水平反射(共10个patch)进行预测，并将网络的<code>softmax</code>层对这10个patch的预测取平均</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">valid_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">        transforms.TenCrop(<span class="number">224</span>, vertical_flip=False),</span><br><span class="line">        transforms.Lambda(lambda crops: torch.<span class="built_in">stack</span>([normalizes(transforms.ToTensor()(crop)) <span class="keyword">for</span> crop in crops])),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><h4 id="5-Dropout"><a href="#5-Dropout" class="headerlink" title="5. Dropout"></a>5. Dropout</h4><p>在网络架构的前两个全链接层后添加<code>Dropout</code>，防止了过拟合（论文作者最开始的理解是<code>Dropout</code>是做模型融合，实际上是在正则化，之后本文作者写了一篇<code>JMLR</code>文章说明Dropout实际等价一个<code>L2</code>的正则,使用<code>Dopout</code></p><p><strong>可以提高模型的泛化性</strong></p><h4 id="6-Details-of-learning"><a href="#6-Details-of-learning" class="headerlink" title="6. Details of learning"></a>6. Details of learning</h4><p>在论文中使用<code>SGD</code>随机梯度下降法作为优化函数进行权重参数优化,其中<code>dataloader</code>中的<code>batch_size = 128</code>,<code>momentum = 0.9</code> ,<code>weight decay = 0.0005</code></p><p>训练细节：</p><ul><li>权重参数初始化，标准差&#x3D;0.01  均值&#x3D;0   的高斯正太分布</li><li>有关学习率的调整，所有层的学习率相同，但是在验证的正确率随着当前学习率停止提高时，将学习率除以10继续训练；学习率<code>learn_rate = 0.01</code>  初始值</li><li>训练拟合时，第一层卷积的可视化，也需要进行演示</li></ul><h3 id="三、实现"><a href="#三、实现" class="headerlink" title="三、实现"></a>三、实现</h3><h5 id="1-模型训练"><a href="#1-模型训练" class="headerlink" title="1.模型训练"></a>1.模型训练</h5><p>模型使用<code>Mini-ImageNet</code>数据集对网络模型进行训练拟合</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    对网络模型进行训练拟合，并且保存模型最好的验证正确率的参数权重</span></span><br><span class="line"><span class="string">        训练数据集 使用MINI-ImageNet</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line">from AlexNet import AlexNet</span><br><span class="line">from torch.optim import lr_scheduler</span><br><span class="line">import logging</span><br><span class="line">import colorlog</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.定义相关全局变量</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 当前Train.py文件所在的目录位置</span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 得到数据集所在文件目录</span><br><span class="line">data_dir = os.path.join(BASE_DIR,<span class="string">&quot;../Mini-ImageNet/&quot;</span>)</span><br><span class="line">train_data_dir = os.path.join(data_dir,<span class="string">&quot;./new_train&quot;</span>)</span><br><span class="line">test_data_dir = os.path.join(data_dir,<span class="string">&quot;./new_test&quot;</span>)</span><br><span class="line">num_classes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">MAX_EPOCH = <span class="number">1</span>    # 最大训练epoch</span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">classes = <span class="number">100</span></span><br><span class="line">start_epoch = <span class="number">-1</span></span><br><span class="line">log_interval = <span class="number">1</span></span><br><span class="line">val_interval = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># 设置<span class="built_in">log</span>输出--控制台输出并保存到文件中</span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">logger.setLevel(level=logging.INFO)</span><br><span class="line">handler = logging.FileHandler(<span class="string">&quot;../Log/log.txt&quot;</span>,mode=<span class="string">&#x27;w+&#x27;</span>)</span><br><span class="line">handler.setLevel(logging.INFO)</span><br><span class="line">formatter = logging.Formatter(<span class="string">&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;</span>)</span><br><span class="line">handler.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line">log_colors_config = &#123;</span><br><span class="line">    <span class="string">&#x27;INFO&#x27;</span>: <span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">console_formatter = colorlog.ColoredFormatter(log_colors=log_colors_config)</span><br><span class="line">console = logging.StreamHandler()</span><br><span class="line">console.setFormatter(console_formatter)</span><br><span class="line">console.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line"># 重复日志问题：</span><br><span class="line"># <span class="number">1</span>、防止多次addHandler；</span><br><span class="line"># <span class="number">2</span>、loggername 保证每次添加的时候不一样；</span><br><span class="line"># <span class="number">3</span>、显示完<span class="built_in">log</span>之后调用removeHandler</span><br><span class="line"><span class="keyword">if</span> not logger.handlers:</span><br><span class="line">    logger.addHandler(handler)</span><br><span class="line">    logger.addHandler(console)</span><br><span class="line"></span><br><span class="line">handler.close()</span><br><span class="line">console.close()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    3.加载训练以及验证数据集</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>)),      # (<span class="number">256</span>, <span class="number">256</span>) 区别</span><br><span class="line">    transforms.CenterCrop(<span class="number">256</span>),</span><br><span class="line">    transforms.RandomCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(norm_mean, norm_std),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">normalizes = transforms.Normalize(norm_mean, norm_std)</span><br><span class="line">valid_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">    transforms.TenCrop(<span class="number">224</span>, vertical_flip=False), #  一张图片会得到<span class="number">10</span>张图片 <span class="number">10</span>张图片会以<span class="built_in">list</span>形式存储</span><br><span class="line">    # 将<span class="built_in">list</span>中的图片依次去取出做normalizes()  torch.<span class="built_in">stack</span>就将<span class="number">10</span>张图片进行拼接得到一个<span class="number">4</span>D张量  [B  C  H  W]  B = <span class="number">10</span></span><br><span class="line">    transforms.Lambda(lambda crops: torch.<span class="built_in">stack</span>([normalizes(transforms.ToTensor()(crop)) <span class="keyword">for</span> crop in crops])),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 构建Dataset实例</span><br><span class="line">train_dataset = ImageFolder(train_data_dir, transform=train_transform)</span><br><span class="line">test_dataset = ImageFolder(test_data_dir, transform=valid_transform)</span><br><span class="line"></span><br><span class="line"># 构建DataLoder</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)</span><br><span class="line">valid_loader = DataLoader(dataset=test_dataset, batch_size=<span class="number">4</span>,shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    4.导入网络模型</span></span><br><span class="line"><span class="string">        配置损失函数</span></span><br><span class="line"><span class="string">        配置优化器</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"># 模型</span><br><span class="line">alexnet_model = AlexNet(num_classes=<span class="number">100</span>,dropout=<span class="number">0.5</span>)</span><br><span class="line">alexnet_model.to(device)</span><br><span class="line"></span><br><span class="line"># 损失函数</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 优化器 （对最后一层使用了softmax）--- 卷积层的学习率可以小一些  在线性层学习率可以大一些 --- trick</span><br><span class="line">flag = <span class="number">0</span></span><br><span class="line"><span class="meta"># flag = 1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="meta"># map() 会根据提供的函数对指定序列(可以迭代对象)做映射</span></span><br><span class="line">    <span class="meta"># id() 函数返回指定对象的唯一 id    id 是对象的内存地址</span></span><br><span class="line">    # 该模型有三个线性层  每个线性层对应 一个输入参数与权重的乘法  以及一个加法（偏置）对应六个id</span><br><span class="line">    fc_params_id = <span class="built_in">list</span>(<span class="built_in">map</span>(id, alexnet_model.classifier.parameters()))  # 返回的是parameters的 内存地址</span><br><span class="line">    # 如 lambda x: x ** <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]   x**<span class="number">2</span> 是函数表达式  x 参数  取值范围是  [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    <span class="meta"># lambda p: id(p) not in fc_params_id, alexnet_model.parameters()</span></span><br><span class="line">    # 得到的id是模型参数中 不属于 fc_params_id(线性层)列表中的id</span><br><span class="line">    <span class="meta"># filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表  filter(function, iterable)</span></span><br><span class="line">    base_params = filter(lambda p: id(p) not in fc_params_id, alexnet_model.parameters())</span><br><span class="line">    optimizer = optim.SGD([</span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: base_params, <span class="string">&#x27;lr&#x27;</span>: LR * <span class="number">0.1</span>&#125;,  # <span class="number">0</span></span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: alexnet_model.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: LR&#125;], momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=<span class="number">0.9</span>)  # 选择优化器</span><br><span class="line"></span><br><span class="line"># 学习率每隔<span class="number">10</span>轮变为原来的<span class="number">0.1</span></span><br><span class="line">lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    train_curve = <span class="built_in">list</span>()</span><br><span class="line">    train_ACC_curve = <span class="built_in">list</span>()</span><br><span class="line">    valid_curve = <span class="built_in">list</span>()</span><br><span class="line">    valid_ACC_curve = <span class="built_in">list</span>()</span><br><span class="line">    min_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch in range(start_epoch + <span class="number">1</span>, MAX_EPOCH):</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                5.训练网络</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        loss_mean = <span class="number">0.</span></span><br><span class="line">        correct = <span class="number">0.</span></span><br><span class="line">        total = <span class="number">0.</span></span><br><span class="line">        train_ACC = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">&quot;----------------Train: Epoch &#123;&#125;----------------------&quot;</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line">        alexnet_model.train()</span><br><span class="line">        <span class="keyword">for</span> i, data in enumerate(train_loader):</span><br><span class="line"></span><br><span class="line">            <span class="meta"># forward</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">            outputs = alexnet_model(inputs)</span><br><span class="line"></span><br><span class="line">            <span class="meta"># backward</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            <span class="meta"># update weights</span></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            # 统计分类情况</span><br><span class="line">            _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)   # 累加这一轮 每一批次的样本数量 = 这一轮的总样本数量</span><br><span class="line">            correct += (predicted == labels).squeeze().cpu().sum().numpy()  # 将该轮中每一个批次预测正确的样本数量进行累加</span><br><span class="line"></span><br><span class="line">            # 打印训练信息</span><br><span class="line">            loss_mean += loss.item()  # 将该轮中每一批次的损失进行累积  得到本轮的总损失</span><br><span class="line">            train_curve.append(loss.item())</span><br><span class="line">            train_ACC = correct / total</span><br><span class="line">            train_ACC_curve.append(train_ACC)</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % log_interval == <span class="number">0</span>:   # 在本轮训练中 当训练了log_interval的批次时名就打印一次训练信息</span><br><span class="line">                loss_mean = loss_mean / log_interval</span><br><span class="line"></span><br><span class="line">                logger.info(</span><br><span class="line">                    <span class="string">&quot;Training:Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Train_Loss: &#123;:.4f&#125; Train_Acc:&#123;:.2%&#125;&quot;</span>.format(</span><br><span class="line">                        epoch + <span class="number">1</span>, MAX_EPOCH, i + <span class="number">1</span>, len(train_loader), loss_mean, train_ACC)</span><br><span class="line">                )</span><br><span class="line">                loss_mean = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        lr_scheduler.step()  # 更新学习率</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">            6.验证网络</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % val_interval == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            correct_val = <span class="number">0.</span></span><br><span class="line">            total_val = <span class="number">0.</span></span><br><span class="line">            loss_val = <span class="number">0.</span></span><br><span class="line">            Valid_Acc = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">            logger.info(<span class="string">&quot;----------------Valid: Epoch &#123;&#125;----------------------&quot;</span>.format(epoch + <span class="number">1</span>))</span><br><span class="line">            alexnet_model.eval()</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> j, data in enumerate(valid_loader):</span><br><span class="line">                    inputs, labels = data</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">                    bs, ncrops, c, h, w = inputs.size()  # [<span class="number">4</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>]</span><br><span class="line">                    outputs = alexnet_model(inputs.view(<span class="number">-1</span>, c, h, w))  # [<span class="number">40</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>]</span><br><span class="line">                    # 论文中对于验证的相关操作 --- 对于网络的softmax层输出的<span class="number">10</span>个patch 预测取平均</span><br><span class="line">                    <span class="meta"># outputs.view(bs, ncrops, -1)  [4,10,100]</span></span><br><span class="line">                    <span class="meta"># torch.mean(x,dim)  dim表示对于输入x的那一个维度求平均 [bs,ncrops,100].mean(1) 对dim=1求平均</span></span><br><span class="line">                    outputs_avg = outputs.view(bs, ncrops, <span class="number">-1</span>).mean(<span class="number">1</span>)  #  outputs_avg.shape = [<span class="number">4</span>,<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">                    loss = criterion(outputs_avg, labels)</span><br><span class="line"></span><br><span class="line">                    _, predicted = torch.max(outputs_avg.data, <span class="number">1</span>)  # 该批次验证预测的结果</span><br><span class="line">                    total_val += labels.size(<span class="number">0</span>)  # 本轮累积批次验证的样本总数</span><br><span class="line">                    correct_val += (predicted == labels).squeeze().cpu().sum().numpy()  #  本轮累积批次验证正确的样本数</span><br><span class="line"></span><br><span class="line">                    loss_val += loss.item()</span><br><span class="line"></span><br><span class="line">                loss_val_mean = loss_val / len(valid_loader)  # 本轮验证 每一个批次的平均损失</span><br><span class="line">                valid_curve.append(loss_val_mean)</span><br><span class="line">                Valid_Acc = correct_val/total_val  # 本轮的平均正确率</span><br><span class="line">                valid_ACC_curve.append(Valid_Acc)</span><br><span class="line"></span><br><span class="line">                logger.info(</span><br><span class="line">                    <span class="string">&quot;Valid:\t Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Valid_Loss: &#123;:.4f&#125; Valid_Acc:&#123;:.2%&#125;&quot;</span>.format(</span><br><span class="line">                        epoch+<span class="number">1</span>, MAX_EPOCH, j + <span class="number">1</span>, len(valid_loader), loss_val_mean, Valid_Acc)</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                    7.保存网络模型</span></span><br><span class="line"><span class="string">                &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">                <span class="keyword">if</span> Valid_Acc &gt; min_acc:</span><br><span class="line">                    folder = <span class="string">&#x27;../Models&#x27;</span></span><br><span class="line">                    <span class="keyword">if</span> not os.path.exists(folder):  # 当前目录不存在则进行创建</span><br><span class="line">                        os.mkdir(folder)</span><br><span class="line">                    min_acc = Valid_Acc</span><br><span class="line">                    logger.info(<span class="string">&quot;save best model Epoch : &#123;&#125;&quot;</span>.format(epoch + <span class="number">1</span>))</span><br><span class="line">                    # 保存权重文件</span><br><span class="line">                    torch.save(alexnet_model.state_dict(), <span class="string">&#x27;../Models/best_model_AlexNet.pth&#x27;</span>)</span><br><span class="line">                # 保存最后一轮的权重文件</span><br><span class="line">                <span class="keyword">if</span> epoch+<span class="number">1</span> == MAX_EPOCH:</span><br><span class="line">                    torch.save(alexnet_model.state_dict(), <span class="string">&#x27;../Models/last_model_AlexNet.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            alexnet_model.train()</span><br><span class="line">            logger.info(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        7.结果可视化</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    train_x = range(len(train_curve))</span><br><span class="line">    train_y = train_curve</span><br><span class="line">    train_acc_x = range(len(train_ACC_curve))</span><br><span class="line">    train_acc_y = train_ACC_curve</span><br><span class="line"></span><br><span class="line">    train_iters = len(train_loader)</span><br><span class="line">    #  由于valid中记录的是epoch_loss，需要对记录点进行转换到iterations</span><br><span class="line">    valid_x = np.arange(<span class="number">1</span>, len(valid_curve) + <span class="number">1</span>) * train_iters * val_interval</span><br><span class="line">    valid_y = valid_curve</span><br><span class="line">    valid_acc_x = np.arange(<span class="number">1</span>, len(valid_ACC_curve) + <span class="number">1</span>) * train_iters * val_interval</span><br><span class="line">    valid_acc_y = valid_ACC_curve</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(train_x, train_y, label=<span class="string">&#x27;Train_loss&#x27;</span>)</span><br><span class="line">    plt.plot(valid_x, valid_y, label=<span class="string">&#x27;Valid_loss&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss value&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(train_acc_x, train_acc_y, label=<span class="string">&#x27;Train_acc&#x27;</span>)</span><br><span class="line">    plt.plot(valid_acc_x, valid_acc_y, label=<span class="string">&#x27;Valid_acc&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;acc value&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and Validation acc&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">&#x27;End....&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练结束之后，会将最优以及最后训练的模型进行保存，此外还会将训练的日志在控制台输出以及存储至<code>log.txt</code>文件中</p><p>最后会得到<code>训练损失vs验证损失</code>  以及  <code>训练正确率vs验证正确率</code>的可视化曲线</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121048551.png" alt="image-20230512104741717"></p><p>训练得到的结果出现过拟合的现象，大概在迭代的10000次（10000&#x2F;282&#x3D;35）轮左右出现过拟合….</p><p>训练损失在降低，但是验证损失在增加，而且此时的测试正确率不再有变化…</p><h5 id="（1）过拟合解决方法"><a href="#（1）过拟合解决方法" class="headerlink" title="（1）过拟合解决方法"></a>（1）过拟合解决方法</h5><p>过拟合出现的主要原因是因为：<strong>数据太少+模型太复杂</strong></p><ul><li>增加数据量<ul><li>多收集数据集，扩大数据集的量</li><li>数据增强（通过图片的旋转、平移、亮度、切割），增加数据的多样性</li></ul></li><li>正则化方法<ul><li><code>L1</code>正则、<code>L2</code>正则（使得某些权重<code>w</code>不会过大）</li><li>Dropout</li></ul></li><li>多模型组合</li><li>贝叶斯方法</li></ul><h5 id="（2）Pytorch实现正则化"><a href="#（2）Pytorch实现正则化" class="headerlink" title="（2）Pytorch实现正则化"></a>（2）Pytorch实现正则化</h5><p>在<code>pytorch</code>中进行<code>L2</code>正则化，最直接的方式可以直接用优化器自带的<code>weight_decay</code>选项指定权值衰减率，相当于<code>L2</code>正则化中的<code>λ</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=<span class="number">0.9</span>,weight_decay=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><p>对上述模型加入<code>L2</code>的正则，并加载之前过拟合的训练的权重参数，发现其训练正确率在下降….</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121557715.png" alt="image-20230512155714678"></p><h4 id="2-模型测试"><a href="#2-模型测试" class="headerlink" title="2.模型测试"></a>2.模型测试</h4><p>测试集数据与验证集数据的数量是一致的均为<code>12000</code>张</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    用于测试模型的正确率</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line">from AlexNet import AlexNet</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.得到有关100分类标签的列表(按照正确的顺序)</span></span><br><span class="line"><span class="string">        参数 class_name.json的路径</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">def <span class="title function_">get_classes_name</span><span class="params">(json_path)</span>:</span><br><span class="line">    classes_name_list = <span class="built_in">list</span>()</span><br><span class="line">    with <span class="title function_">open</span><span class="params">(json_path, <span class="string">&quot;r&quot;</span>)</span> as f:</span><br><span class="line">        class_names_dict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k,v in class_names_dict.items():</span><br><span class="line">        classes_name_list.append(v[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> classes_name_list</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    3.定义相关全局变量</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data_dir = os.path.join(BASE_DIR,<span class="string">&quot;../Mini-ImageNet/&quot;</span>)</span><br><span class="line">test_data_dir = os.path.join(data_dir,<span class="string">&quot;./new_test&quot;</span>)</span><br><span class="line"></span><br><span class="line">path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;../Models/best_model_AlexNet.pth&quot;</span>)</span><br><span class="line">num_classes=<span class="number">100</span></span><br><span class="line"></span><br><span class="line">classes_name = <span class="built_in">list</span>()   # 用于存储 Mini-ImageNet <span class="number">100</span>分类名字的列表</span><br><span class="line">classes_name_json_path = os.path.join(BASE_DIR, <span class="string">&quot;../Mini-ImageNet/classes_name.json&quot;</span>)  # 模型参数路径</span><br><span class="line">classes_name = get_classes_name(classes_name_json_path)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    4.测试数据集</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line">test_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>), #  一张图片会得到<span class="number">10</span>张图片 <span class="number">10</span>张图片会以<span class="built_in">list</span>形式存储</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(norm_mean,norm_std),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_dataset = ImageFolder(test_data_dir, transform=test_transform)</span><br><span class="line">test_loader = DataLoader(dataset=test_dataset, batch_size=<span class="number">1</span>,shuffle=True)  # batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    5.导入网络模型</span></span><br><span class="line"><span class="string">        加载模型参数</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">alexnet_model = AlexNet(num_classes=<span class="number">100</span>,dropout=<span class="number">0.5</span>)</span><br><span class="line">pretrained_state_dict = torch.load(path_state_dict)</span><br><span class="line">alexnet_model.load_state_dict(pretrained_state_dict)</span><br><span class="line">alexnet_model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    6.验证</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">alexnet_model.eval()</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    sum = len(test_dataset)</span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data in test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs,targets = imgs.to(device),targets.to(device)</span><br><span class="line"></span><br><span class="line">        output = alexnet_model(imgs)</span><br><span class="line">        _, pred = torch.max(output, axis=<span class="number">1</span>)</span><br><span class="line">        predicted_point = pred[<span class="number">0</span>].item()</span><br><span class="line">        <span class="keyword">if</span> predicted_point == <span class="type">int</span>(targets.item()):</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        predicted = classes_name[predicted_point]</span><br><span class="line">        actual = classes_name[<span class="type">int</span>(targets.item())]</span><br><span class="line">        print(f<span class="number">&#x27;</span>the predicted_point is <span class="string">&quot;&#123;predicted_point&#125; &quot;</span>predicted:<span class="string">&quot;&#123;predicted&#125;&quot;</span>, Actual:<span class="string">&quot;&#123;actual&#125;&quot;</span><span class="string">&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print(f&quot;the Number of samples is &#123;sum&#125;&quot;)</span></span><br><span class="line"><span class="string">print(f&quot;the Accuracy  is &#123;float(right/sum)*100&#125; %&quot;)</span></span><br></pre></td></tr></table></figure><p>测试结果与训练输出的曲线结果类似，测试正确率与验证正确率都是在 <code>50%</code> 左右（这是用过拟合的模型进行测试的）—- 第一次过拟合的模型放在<code>My_Proj/Models/model_overfitting</code>目录下</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121119943.png" alt="image-20230512111954917"></p><h4 id="3-卷积核的可视化"><a href="#3-卷积核的可视化" class="headerlink" title="3.卷积核的可视化"></a>3.卷积核的可视化</h4><p>在论文第五节中提到，将第一层的卷积核提取出来可以看到第一层96个卷积核（<code>GPU0  GPU1</code>）分别48个卷积核</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    训练拟合后对第一层的卷积核进行可视化</span></span><br><span class="line"><span class="string">        Web端 可视化的结果存储在 Visualization_Log目录下</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    1.导入库</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from PIL import Image</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">import torchvision.models as models</span><br><span class="line">import torchvision.utils as vutils</span><br><span class="line"></span><br><span class="line">from AlexNet import AlexNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    2.定义相关全局变量</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    log_dir = os.path.join(BASE_DIR, <span class="string">&quot;../Visualization_Log&quot;</span>)   #  用于存储 web 可视化结果的目录</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(log_dir=log_dir, filename_suffix=<span class="string">&quot;_kernel&quot;</span>)  #  创建对象  filename_suffix文件名后缀</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        3.导入模型以及预训练参数</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;../Models/best_model_AlexNet.pth&quot;</span> )</span><br><span class="line">    <span class="meta"># alexnet = models.alexnet()</span></span><br><span class="line">    alexnet  = AlexNet()</span><br><span class="line">    pretrained_state_dict = torch.load(path_state_dict)</span><br><span class="line">    alexnet.load_state_dict(pretrained_state_dict)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        4.卷积核可视化</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    kernel_num = <span class="number">-1</span></span><br><span class="line">    vis_max = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sub_module in alexnet.modules():  <span class="meta"># model.modules()迭代遍历模型的所有子层</span></span><br><span class="line">        <span class="keyword">if</span> not isinstance(sub_module, nn.Conv2d):   # 判断当前迭代的层是否是卷积层</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        kernel_num += <span class="number">1</span>  # 当前迭代的子层是卷积层</span><br><span class="line">        <span class="keyword">if</span> kernel_num &gt; vis_max:   # 大于需要可视化最大层数量时  <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        kernels = sub_module.weight</span><br><span class="line">        c_out, c_int, k_h, k_w = tuple(kernels.shape)  # c_out与下一层的特征映射图数量一致(卷积核的数量)  c_int卷积核的通道数</span><br><span class="line"></span><br><span class="line">        # 拆分channel---将单个卷积核心的每一个通道的图像可视化</span><br><span class="line">        <span class="keyword">for</span> o_idx in range(c_out):</span><br><span class="line">            kernel_idx = kernels[o_idx, :, :, :]  # 获得(C, h, w)</span><br><span class="line">            # make_grid需要 BCHW，这里拓展C维度变为（C，<span class="number">1</span>， h, w）  unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度</span><br><span class="line">            # 这样一个卷积核的 多个通道就被拆开了</span><br><span class="line">            kernel_idx = kernel_idx.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            kernel_grid = vutils.make_grid(kernel_idx,normalize=True, scale_each=True, nrow=<span class="number">8</span>) <span class="meta"># normalize=True 将值缩放值 [0-1] 之间</span></span><br><span class="line">            writer.add_image(<span class="string">&#x27;&#123;&#125;_Convlayer_split_in_channel&#x27;</span>.format(kernel_num), kernel_grid, global_step=o_idx)</span><br><span class="line"></span><br><span class="line">        # 将单个卷积核 进行可视化 第一层卷积核 有<span class="number">64</span>个</span><br><span class="line">        kernel_all = kernels.view(<span class="number">-1</span>, <span class="number">3</span>, k_h, k_w)</span><br><span class="line">        kernel_grid = vutils.make_grid(kernel_all, normalize=False, scale_each=True, nrow=<span class="number">8</span>)  <span class="meta"># c, h, w</span></span><br><span class="line">        writer.add_image(<span class="string">&#x27;&#123;&#125;_all&#x27;</span>.format(kernel_num), kernel_grid, global_step=<span class="number">620</span>)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">&quot;&#123;&#125;_convlayer shape:&#123;&#125;&quot;</span>.format(kernel_num, tuple(kernels.shape)))</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        5.可视化第一层卷积后的特征映射图</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    writer = SummaryWriter(log_dir=log_dir, filename_suffix=<span class="string">&quot;_feature map&quot;</span>)</span><br><span class="line"></span><br><span class="line">    # 输入数据</span><br><span class="line">    path_img = os.path.join(BASE_DIR, <span class="string">&quot;../../deep_eyes/A_alexnet/data/tiger cat.jpg&quot;</span>)</span><br><span class="line">    normMean = [<span class="number">0.49139968</span>, <span class="number">0.48215827</span>, <span class="number">0.44653124</span>]</span><br><span class="line">    normStd = [<span class="number">0.24703233</span>, <span class="number">0.24348505</span>, <span class="number">0.26158768</span>]</span><br><span class="line">    norm_transform = transforms.Normalize(normMean, normStd)</span><br><span class="line">    img_transforms = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        norm_transform</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    img_pil = Image.open(path_img).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    img_tensor = img_transforms(img_pil)</span><br><span class="line">    img_tensor.unsqueeze_(<span class="number">0</span>)  <span class="meta"># chw --&gt; bchw</span></span><br><span class="line"></span><br><span class="line">    # 前向传播</span><br><span class="line">    convlayer1 = alexnet.features[<span class="number">0</span>]   #  拿到模型的第一层卷积</span><br><span class="line">    fmap_1 = convlayer1(img_tensor)    # 得到特征映射图  shape = [<span class="number">1</span>,<span class="number">64</span>,<span class="number">55</span>,<span class="number">55</span>]</span><br><span class="line"></span><br><span class="line">    # 预处理 transpose方法的作用是交换矩阵的两个维度</span><br><span class="line">    fmap_1.transpose_(<span class="number">0</span>, <span class="number">1</span>)  <span class="meta"># bchw=(1, 64, 55, 55) --&gt; (64, 1, 55, 55)</span></span><br><span class="line">    fmap_1_grid = vutils.make_grid(fmap_1, normalize=False, scale_each=True, nrow=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    writer.add_image(<span class="string">&#x27;feature map in conv1&#x27;</span>, fmap_1_grid, global_step=<span class="number">620</span>)</span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure><p>在<code>pycharm</code>控制台的终端,进入存放web可视化文件的目录下<code>My_Proj/Visualization_Log</code>目录下</p><p>终端输入:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=./</span><br></pre></td></tr></table></figure><p>终端输出一个主机<code>host</code></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121133692.png" alt="image-20230512113315661"></p><p>点击，会在web端的<code>0_all</code>出现可视化的第一层卷积核：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305121135618.png" alt="image-20230512113525576"></p><p>后面几层卷积核，特征相对高级，很抽象。而且卷积核尺寸小，看不懂卷积学习的内容是什么</p><p>第一层卷积最为低级，而且卷积核尺寸为<code>11x11</code>，可以看到第一层卷积学习了图片数据的颜色、纹理、边缘这些较为低级的特征。</p><p>神经网络对数据的特征进行提取符合由低级至高级特征提取的过程</p><h3 id="四、结论"><a href="#四、结论" class="headerlink" title="四、结论"></a>四、结论</h3><h4 id="1-重点"><a href="#1-重点" class="headerlink" title="1.重点"></a>1.重点</h4><ul><li><code>AlexNet</code>本质是一个更大更深的<code>LeNet</code>,主要改进有,<code>Dropout</code>丢弃法、<code>ReLu</code>激活函数、<code>maxpooling</code>重叠池化；<code>ReLu</code>与<code>sigmoid</code>相比梯度更大，且<code>ReLu</code>在零点处一阶导更好（减缓梯度消失），<code>maxpooling</code>取最大池化，取最大值，输出梯度更大</li><li>局部响应标准化（<code>LRN</code>）有助于<code>AlexNet</code>泛化能力的提升，受真实的神经元<strong>侧抑制</strong>启发；但是在论文<code>VGG</code>说该方法作用不大，而且有更好的<code>Batch Normalization</code></li><li><code>PCA</code>对数据集图片的颜色进行扰动，对于模型的性能提升并不大，而且实现相对复杂，目前就不实现了…</li></ul><h4 id="2-启发性"><a href="#2-启发性" class="headerlink" title="2.启发性"></a>2.启发性</h4><ul><li>初始的<code>224x224x3</code>的图片经过五层卷积之后，最后会被展平成一个<code>256x6x6</code>的向量进入线性层，直到最后一个分类层（输出层）之前，向量长度为<code>4096</code>，则一张图片会表示为<code>4096</code>的维度，<strong>这个长度为4096的向量非常好的抓住了输入图片的语义信息。若两个图片最后的4096的向量的距离（欧几里德距离非常相近的话，那么这两张图片很有可能是同一个物体的图片）————-深度学习设计的网络可以通过中间的各种隐含层的操作将一张图片最后压缩为一个特征向量（知识的压缩），而这个向量可以很好的将中间的语义信息表示出来（变成了一个机器可以理解的东西）</strong>    （论文的6.1 Qualitative  Evaluations 中提及）</li><li>将神经网络在倒数第二层的输出拿出来，得到一个长向量。然后将每个图片均拿出来，然后给定一张图片看一下和我这个向量最近的图片是谁（欧几里德距离），<strong>如果两幅图像产生的特征激活向量具有小的欧几里德距离，我们可以说神经网络认为它们是相似的（注意原始的图像之间的距离是不相近的，但是通过神经网络提取得到的高级特征向量之后，欧几里德距离是相近的）</strong>—<strong>深度神经网络的图片训练出来的最后那个向量，在语义空间的表示特别好（非常好的特征）</strong>，相似的图片会将其放在一起</li><li><code>6.1 Qualitative  Evaluations </code>最后提出可以使用<code>AlexNet</code>做图像检索、图像聚类、图像编码，利用两个4096维实值向量之间的欧氏距离来计算相似度是低效的，但通过<strong>训练自动编码器</strong>将这些向量压缩成简短的二进制码可以提高效率。这将产生一种比对原始像素应用自动编码器好得多的图像检索方法，后者不使用图像标签，因此倾向于检索具有相似边缘模式的图像，无论它们在语义上是否相似。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 论文复现 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MAKEFILE（4）</title>
      <link href="/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/MAKEFILE%EF%BC%884%EF%BC%89/"/>
      <url>/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/MAKEFILE%EF%BC%884%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="MAKEFILE"><a href="#MAKEFILE" class="headerlink" title="MAKEFILE"></a>MAKEFILE</h2><ul><li><p>在Linux中，有一个叫make的东西，就相当于C语言的集成开发环境，我们只需要在make里面创建文件，写代码，make会帮我们管理这些文件</p></li><li><p>我们创建的项目不叫project，而是称为Makefile，打开一个make源程序包，发现很多Makefile的文件，说明里面有很多的项目</p></li><li><p>源程序包里面，也有名为makefile的文件（m是小写），两个命名同时存在，这是合理的，在开发一个项目的时候，工程师一般都会命名为Makefile然后打包交给用户，用户觉得某个Makefile需要改动，用户改动后或者新建后的项目定义为makefile，并且在运行时候，先执行makefile，再执行Makefile文件。</p></li><li><p>快捷操作：</p><ul><li><pre><code class="c++">vim * -p :可以vim你所创建的文件gcc *.c ：编译该文件夹下的所有.c文件在非插入的模式下：    x可以删除光标前的内容    u可以撤销前一个操作    ctrl+r 反撤销    yy 复制光标所在一行的内容    v 进入可视模式，直接移动光标选中内容，按y复制内容，然后按p/P粘贴内容。    dd 删除一整行的内容    ggVG:选中全部   y复制   d全部删除<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 一、为何使用Makefile</span><br><span class="line"></span><br><span class="line">![image-20230311145137250](https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230311145144.png)</span><br><span class="line"></span><br><span class="line">* 这个树形图展示了一个项目中的层级关系，如果我们需要变动3号文件，会发现，牵一发而动全身，改动一个被迫需要改动一堆，为了解放我们，make中编写Makefile就不再需要考虑这些，你把每个文件的依赖关系以指令的形式说明清楚并且保存下来，改动一个即可，会自动帮你修改关联到的其他文件。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 二、创建相关文件</span><br><span class="line"></span><br><span class="line">#### 1.创建工程文件</span><br><span class="line"></span><br><span class="line">* ```c++</span><br><span class="line">  touch main.c tool1.c tool1.h tool2.c tool2.h</span><br><span class="line">  ls</span><br><span class="line">  gedit main.c tool1.c tool1.h tool2.c tool2.h  或者  vim * -p</span><br><span class="line">      </span><br><span class="line">  vim * -p 打开该文件下所有文件   gt用于切换文件</span><br><span class="line">  ：wqa 可在vim 中方退出所有文件</span><br></pre></td></tr></table></figure></code></pre></li></ul></li></ul><ul><li><p>编写代码</p><ul><li><p>tool1.h</p><ul><li><pre><code class="c++">#ifndef TOOL1_H#define TOOL1_Hvoid mytool1(void);# endif<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* tool2.h</span><br><span class="line"></span><br><span class="line">  * ```c++</span><br><span class="line">    #ifndef TOOL2_H</span><br><span class="line">    #define TOOL2_H</span><br><span class="line">    </span><br><span class="line">    void mytool2(void);</span><br><span class="line">    </span><br><span class="line">    # endif</span><br></pre></td></tr></table></figure></code></pre></li></ul></li><li><p>tool1.c</p><ul><li><pre><code class="c++">#include &lt;stdio.h&gt;#include &quot;tool1.h&quot;void mytool1()&#123;    printf(&quot;tool1 print\n&quot;);&#125;<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* tool2.c</span><br><span class="line"></span><br><span class="line">  * ```c++</span><br><span class="line">    #include &lt;stdio.h&gt;</span><br><span class="line">    #include &quot;tool2.h&quot;</span><br><span class="line">    </span><br><span class="line">    void mytool2()</span><br><span class="line">    &#123;</span><br><span class="line">    printf(&quot;tool2 print\n&quot;);               </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></code></pre></li></ul></li><li><p>main.c</p><ul><li><pre><code class="c++">#include &quot;tool1.h&quot;#include &quot;tool2.h&quot;int main()&#123;    mytool1();    mytool2();    return 0;&#125;<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 编译代码</span><br><span class="line"></span><br><span class="line">  * ```c++</span><br><span class="line">    gcc *.c</span><br></pre></td></tr></table></figure></code></pre></li></ul></li><li><pre><code class="c">ls<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    * ![image-20230311150527032](https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230311150527.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 2.文件依赖关系</span><br><span class="line"></span><br><span class="line">**.o是可执行文件**</span><br><span class="line"></span><br><span class="line">  ```c++</span><br><span class="line">  a.out -&gt; main.o tool1.o tool2.o   # 可执行文件a.out依赖于后面的三个执行文件</span><br><span class="line">  main.o -&gt; main.c</span><br><span class="line">  tool1.o -&gt; tool1.c</span><br><span class="line">  tool2.o -&gt; tool2.c</span><br></pre></td></tr></table></figure></code></pre></li></ul></li></ul><h4 id="3-编写makefile文件"><a href="#3-编写makefile文件" class="headerlink" title="3.编写makefile文件"></a>3.编写makefile文件</h4><ul><li><pre><code class="c++">终端输入：vim makefile<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* makefile文件内容</span><br><span class="line"></span><br><span class="line">  * ```c++</span><br><span class="line">    mytool:main.o tool1.o tool2.o</span><br><span class="line">    [TAB键]gcc main.o tool1.o tool2.o -o mytool</span><br><span class="line">    </span><br><span class="line">    main.o:main.c</span><br><span class="line">    gcc main.c -c -Wall -g -o main.o</span><br><span class="line">    tool1.o:tool1.c</span><br><span class="line">    gcc tool1.c -c -Wall -g -o tool1.o</span><br><span class="line">    tool2.o:tool2.c</span><br><span class="line">    gcc tool2.c -c -Wall -g -o tool2.o</span><br><span class="line">        </span><br><span class="line">    clean:</span><br><span class="line">    rm *.o mytool -rf   </span><br></pre></td></tr></table></figure>* 上述文件内容说明了可执行文件，以及其依赖和对应编译过程  * ```c    mytool:main.o tool1.o tool2.o               # mytool依赖于main.o tool1.o tool2.o三个可执行文件        gcc main.o tool1.o tool2.o -o mytool    # gcc使得main.o tool1.o tool2.o 生成可执行文件mytool -o 使得可执行文件命名mytool            main.o:main.c                               # main.o依赖于main.c        gcc main.c -c -Wall -g -o main.o# gcc 编译main.c -c(编译) -Wall(显示警告信息) -g(gdb调试) -o(如上)            clean:        rm *.o mytool -rf                       # 递归的删除make生成的所有.o文件以及mytool文件    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">* ```c</span><br><span class="line">  终端输入：make</span><br></pre></td></tr></table></figure>* 显示结果：![image-20230311151934404](https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230311151934.png)</code></pre></li></ul><h4 id="4-执行mytool可执行文件"><a href="#4-执行mytool可执行文件" class="headerlink" title="4.执行mytool可执行文件"></a>4.执行mytool可执行文件</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mytool</span><br></pre></td></tr></table></figure><p>在终端输入：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make clean     #  可将makefile文件中定义 rm *.o mytool -rf指令执行 递归伤处make生成的所有.o文件以及mytool文件</span><br></pre></td></tr></table></figure><h4 id="5-makefile第二个版本"><a href="#5-makefile第二个版本" class="headerlink" title="5.makefile第二个版本"></a>5.makefile第二个版本</h4><p><strong>利用关键词去替换重复部分减少工工作量，实际过程中操作：$(简写代码)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">OBJS = main.o tool1.o tool2.o</span><br><span class="line">CC = gcc</span><br><span class="line">RM = rm -f</span><br><span class="line"></span><br><span class="line">mytool:$(OBJS)</span><br><span class="line">$(CC) $(OBJS) -o mytool</span><br><span class="line"></span><br><span class="line">main.o:main.c</span><br><span class="line">$(CC) main.c -c -Wall -g -o main.o</span><br><span class="line">tool1.o:tool1.c</span><br><span class="line">$(CC) tool1.c -c -Wall -g -o tool1.o</span><br><span class="line">tool2.o:tool2.c</span><br><span class="line">$(CC) tool2.c -c -Wall -g -o tool2.o</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">$(RM) *.o mytool -r</span><br></pre></td></tr></table></figure><h4 id="6-makefile第三个版本"><a href="#6-makefile第三个版本" class="headerlink" title="6.makefile第三个版本"></a>6.makefile第三个版本</h4><p><strong>将编译选项定义为CFLAGS代替</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">OBJS = main.o tool1.o tool2.o</span><br><span class="line">CC = gcc</span><br><span class="line">RM = rm -f</span><br><span class="line">CFLAGS += -c -Wall -g</span><br><span class="line"></span><br><span class="line">mytool:$(OBJS)</span><br><span class="line">$(CC) $(OBJS) -o mytool</span><br><span class="line"></span><br><span class="line">main.o:main.c</span><br><span class="line">$(CC) main.c $(CFLAGS) -o main.o</span><br><span class="line">tool1.o:tool1.c</span><br><span class="line">$(CC) tool1.c $(CFLAGS) -o tool1.o</span><br><span class="line">tool2.o:tool2.c</span><br><span class="line">$(CC) tool2.c $(CFLAGS) -o tool2.o</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">$(RM) *.o mytool -r</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="7-makefile第四个版本"><a href="#7-makefile第四个版本" class="headerlink" title="7.makefile第四个版本"></a>7.makefile第四个版本</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">OBJS = main.o tool1.o tool2.o</span><br><span class="line">CC = gcc</span><br><span class="line">RM = rm -f</span><br><span class="line">CFLAGS += -c -Wall -g</span><br><span class="line"></span><br><span class="line">mytool:$(OBJS)</span><br><span class="line">$(CC) $^ -o $@</span><br><span class="line"></span><br><span class="line">main.o:main.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o main.o</span><br><span class="line">tool1.o:tool1.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o tool1.o</span><br><span class="line">tool2.o:tool2.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o tool2.o</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">$(RM) *.o mytool -r</span><br></pre></td></tr></table></figure><ul><li><p>上述中:</p><ul><li><pre><code class="c++">mytool:$(OBJS)    $(CC) $^ -o $@    $^ 代表上述的依赖 $(OBJS)  $@ 代表上述需要生成的目标文件   $(CC) $^ -o $@ 整体意思：GCC将上述需要依赖的文件进行-o生成上一句当中的目标文件<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 8.第五个版本（重点）</span><br><span class="line"></span><br><span class="line">**将格式一致的部分利用通用公式代替：**</span><br><span class="line"></span><br><span class="line">```c++</span><br><span class="line">OBJS = main.o tool1.o tool2.o</span><br><span class="line">CC = gcc</span><br><span class="line">RM = rm -f</span><br><span class="line">CFLAGS += -c -Wall -g</span><br><span class="line"></span><br><span class="line">mytool:$(OBJS)</span><br><span class="line">$(CC) $^ -o $@</span><br><span class="line"></span><br><span class="line">%.o:%.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o $@</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">$(RM) *.o mytool -r</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"># 以下部分都是格式一致：</span><br><span class="line">main.o:main.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o main.o</span><br><span class="line">tool1.o:tool1.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o tool1.o</span><br><span class="line">tool2.o:tool2.c</span><br><span class="line">$(CC) $^ $(CFLAGS) -o tool2.o</span><br></pre></td></tr></table></figure></code></pre></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 嵌入式 </category>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 嵌入式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态内存管理（3）</title>
      <link href="/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%883%EF%BC%89/"/>
      <url>/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%883%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="动态内存管理"><a href="#动态内存管理" class="headerlink" title="动态内存管理"></a>动态内存管理</h2><h3 id="一、相关函数原型"><a href="#一、相关函数原型" class="headerlink" title="一、相关函数原型"></a>一、相关函数原型</h3><p><strong>原则-谁申请谁释放</strong></p><p>这类函数返回值都是 void * 可以与其他指针直接赋值</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">SYNOPSIS</span><br><span class="line">       <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span>  <span class="comment">// 此类函数的头文件</span></span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 动态分配一个内存为 size 大小的空间---指针函数---返回一个地址（开辟的内存的地址）</span></span><br><span class="line">       <span class="type">void</span> *<span class="title function_">malloc</span><span class="params">(<span class="type">size_t</span> size)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 释放</span></span><br><span class="line">       <span class="type">void</span> <span class="title function_">free</span><span class="params">(<span class="type">void</span> *ptr)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 动态分配 内存为 nmenb * sizeof(size) 大小的空间，可用于数组（连续的内存空间）</span></span><br><span class="line">       <span class="type">void</span> *<span class="title function_">calloc</span><span class="params">(<span class="type">size_t</span> nmemb, <span class="type">size_t</span> size)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重新将指针ptr指向的地址 分配内存为 size 大小的空间 --- ptr必须是malloc或者calloc调用后返回的某一个指针</span></span><br><span class="line"><span class="comment">// 当原来使用malloc或者calloc开辟的内存空间不够时，若原来的地址后续内存足够，则在原来的地址的基础上进行扩充</span></span><br><span class="line"><span class="comment">// 若原来的地址后续内存不够，则会将原来malloc或者calloc开辟的内存空间进行释放重新找一块内存空间</span></span><br><span class="line">       <span class="type">void</span> *<span class="title function_">realloc</span><span class="params">(<span class="type">void</span> *ptr, <span class="type">size_t</span> size)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重新分配连续的空间</span></span><br><span class="line">       <span class="type">void</span> *<span class="title function_">reallocarray</span><span class="params">(<span class="type">void</span> *ptr, <span class="type">size_t</span> nmemb, <span class="type">size_t</span> size)</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="二、函数使用与注意事项"><a href="#二、函数使用与注意事项" class="headerlink" title="二、函数使用与注意事项"></a>二、函数使用与注意事项</h3><h4 id="1-malloc"><a href="#1-malloc" class="headerlink" title="1.malloc()"></a>1.<code>malloc()</code></h4><p><strong>实例程序–<code>malloc()</code></strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span>  </span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line">        p = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>));   <span class="comment">// 强制转换(int *)可以省略，malloc()返回值为void *</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(p==<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;malloc error&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);   <span class="comment">// 程序异常时，提前终止程序运行</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        *p = <span class="number">10</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p --&gt; %d\n&quot;</span>,p,*p);</span><br><span class="line">        <span class="comment">// 释放内存</span></span><br><span class="line">        <span class="built_in">free</span>(p);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>实例程序(分配连续的空间内存，用于数组)</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> *p;</span><br><span class="line">        <span class="type">int</span> num = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建连续的空间内存</span></span><br><span class="line">        p = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>) * num);</span><br><span class="line">        <span class="type">int</span> i;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>;i&lt;<span class="number">5</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,p+i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>;i &lt; <span class="number">5</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>,p[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-内存泄漏"><a href="#2-内存泄漏" class="headerlink" title="2.内存泄漏"></a>2.内存泄漏</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">func</span><span class="params">(<span class="type">int</span> *p,<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line">        p = <span class="built_in">malloc</span>(n);</span><br><span class="line">        <span class="keyword">if</span>(p == <span class="literal">NULL</span>)</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> num = <span class="number">100</span>;</span><br><span class="line">        <span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        func(p,num);  </span><br><span class="line">        <span class="built_in">free</span>(p);  <span class="comment">// 此时释放p相当于释放一个空指针</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>该程序存在内存泄漏，<code>func()</code>函数中的形式参数<code>int *p </code> 为局部变量，当<code>main()</code>中调用该函数结束后，p的指针就会被释放，但是<code>malloc(n)</code>是动态开辟的内存，只能进行主动释放，此时p指针已经被释放，不会指向此动态开辟的内存，因此这块动态开辟内存就会丢失</strong></p><p>修改程序</p><p>方法1:<strong>二级指针</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">func</span><span class="params">(<span class="type">int</span> **p,<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line">        *p = <span class="built_in">malloc</span>(n);</span><br><span class="line">        <span class="keyword">if</span>(*p == <span class="literal">NULL</span>)</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> num = <span class="number">100</span>;</span><br><span class="line">        <span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        func(&amp;p,num);  <span class="comment">// 函数传入的是 main()函数中定义的int *p的地址，func函数调用结束也不会对其存在影响，因此*p仍然直线malloc(n)开辟的内存</span></span><br><span class="line">        <span class="built_in">free</span>(p);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法2: <strong>返回指针</strong>—-<strong>指针函数</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> 0</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">func</span><span class="params">(<span class="type">int</span> **p,<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line">        *p = <span class="built_in">malloc</span>(n);</span><br><span class="line">        <span class="keyword">if</span>(*p == <span class="literal">NULL</span>)</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> *<span class="title function_">func</span><span class="params">(<span class="type">int</span> *p,<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line">        p = <span class="built_in">malloc</span>(n);</span><br><span class="line">        <span class="keyword">if</span>(p == <span class="literal">NULL</span> )</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> num = <span class="number">100</span>;</span><br><span class="line">        <span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        func(p,num);</span><br><span class="line">        <span class="built_in">free</span>(p);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-野指针的使用"><a href="#3-野指针的使用" class="headerlink" title="3.野指针的使用"></a>3.野指针的使用</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line">        p = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">        <span class="keyword">if</span>(p==<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;malloc() error! \n&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        *p = <span class="number">10</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p----&gt;%d\n&quot;</span>,p,*p);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">free</span>(p);</span><br><span class="line"></span><br><span class="line">        *p = <span class="number">123</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p----&gt;%d\n&quot;</span>,p,*p);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>运行结果:</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230308220924.png" alt="image-20230308220924668"></p><p>在上述的运行结果，虽然<code>free（p）</code>之后的结果，对<code>*p</code>进行赋值，输出<code>*p</code>的值以及指针变量p里面存储的地址值，但是实际此时的<code>p</code>已经是一个<strong>野指针</strong>，可能由于编译器的问题里面对应的地址值并没有变化，若程序在释放p之后在该地址出存储了一个特别重要的变量，这样操作将会造成很危险的结果，因此需要在释放的之后在该指针有下一个合法的指向之前对指针<code>p</code>赋值为<code>NULL</code></p><p><strong>修改</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line">        p = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">        <span class="keyword">if</span>(p==<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;malloc() error! \n&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        *p = <span class="number">10</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p----&gt;%d\n&quot;</span>,*p);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">free</span>(p);</span><br><span class="line">    p = <span class="literal">NULL</span>;  <span class="comment">// 在指针P有下一个合法的指向之前指向为NULL </span></span><br><span class="line"></span><br><span class="line">        *p = <span class="number">123</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p----&gt;%d\n&quot;</span>,*p);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>运行结果:</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230308221752.png" alt="image-20230308221752696"></p><p><strong>上述结果出现段错误，因为free()实际并没有将该段地址中的值清除掉，而是变量p对于开辟的内存空间再也没有引用的权限，此时的指针<code>p</code>以及为NULL，无法对其在进行赋值</strong></p><h4 id="4-传入的指针参数，作为临时（局部）变量"><a href="#4-传入的指针参数，作为临时（局部）变量" class="headerlink" title="4. 传入的指针参数，作为临时（局部）变量"></a>4. 传入的指针参数，作为临时（局部）变量</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdbool.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 取出数组中第 i 个元素 ， 使用指针p 指向arr[i-1]</span></span><br><span class="line"><span class="type">bool</span> <span class="title function_">GetElem</span><span class="params">(<span class="type">int</span> arr[],<span class="type">int</span> i,<span class="type">int</span> *p)</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> length = <span class="keyword">sizeof</span>(arr) / <span class="keyword">sizeof</span>(arr[<span class="number">0</span>]);  <span class="comment">// 得到传入数组的长度</span></span><br><span class="line">        <span class="keyword">if</span>(length==<span class="number">0</span> || i&lt;<span class="number">1</span> || i&gt;length)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// 使得指针p 指向arr[i-1]</span></span><br><span class="line">        p = &amp;arr[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> <span class="built_in">list</span>[<span class="number">5</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">        <span class="type">int</span> i = <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> *p;</span><br><span class="line">        GetElem(<span class="built_in">list</span>,i,p);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,*p);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>编译结果</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230308224213.png"></p><p><strong>原因:</strong> C语言中如果一个函数接受一个数组作为参数，那么数组将会被退化为指针，正确的用法不在其他函数内部使用<code>sizeof()</code>函数</p><p>而是在<code>main()</code>函数中将数组的长度计算出来得到&#96;length,在进一步将length作为参数传入值函数中</p><p><strong>运行结果:</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230308224626.png" alt="image-20230308224626590"></p><p><strong>最后得到的结果,p指针指向的是一个不确定的区块，可以将p仍然看作为野指针</strong></p><p><strong>原因：在调用函数GetElem()之后，传入的指针作为局部变量，函数调用完，指针p中就被释放，又被指向不确定区块</strong></p><p><strong>改正：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdbool.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span>* <span class="title function_">GetElem</span><span class="params">(<span class="type">int</span> arr[],<span class="type">int</span> i,<span class="type">int</span> *p)</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> length = <span class="keyword">sizeof</span>(arr) / <span class="keyword">sizeof</span>(arr[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">if</span>(length==<span class="number">0</span> || i&lt;<span class="number">1</span> || i&gt;length)</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        p = &amp;arr[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> <span class="built_in">list</span>[<span class="number">5</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">        <span class="type">int</span> i = <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> *p;</span><br><span class="line">        p = GetElem(<span class="built_in">list</span>,i,p);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,*p);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后得到期望的值</span></span><br></pre></td></tr></table></figure><p><strong>例程2：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">swap</span><span class="params">(<span class="type">int</span> *p,<span class="type">int</span> *q,<span class="type">int</span>* c,<span class="type">int</span>* d)</span></span><br><span class="line">&#123;</span><br><span class="line">        p = c;</span><br><span class="line">        q = d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> a = <span class="number">3</span>;</span><br><span class="line">        <span class="type">int</span> b = <span class="number">4</span>;</span><br><span class="line">        <span class="type">int</span> c=<span class="number">5</span>;</span><br><span class="line">        <span class="type">int</span> d=<span class="number">6</span>;</span><br><span class="line">        <span class="type">int</span> *p = &amp;a;</span><br><span class="line">        <span class="type">int</span> *q = &amp;b;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;*p:%d----*q:%d\n&quot;</span>,*p,*q);</span><br><span class="line">        swap(p,q,&amp;c,&amp;d);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;*p:%d----*q:%d\n&quot;</span>,*p,*q);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>在函数swap()中使得指针p，q重新指向c,d，此操作无效，因为传入的指针，作为局部变量，在函数中对指针进行重新指向的操作，在函数调用结束之后，指针将指向原来的值</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230308225342.png" alt="image-20230308225342832"></p>]]></content>
      
      
      <categories>
          
          <category> 嵌入式 </category>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 嵌入式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>构造类型（2）</title>
      <link href="/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/%E6%9E%84%E9%80%A0%E7%B1%BB%E5%9E%8B%EF%BC%882%EF%BC%89/"/>
      <url>/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/%E6%9E%84%E9%80%A0%E7%B1%BB%E5%9E%8B%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="构造类型"><a href="#构造类型" class="headerlink" title="构造类型"></a>构造类型</h2><h3 id="一、结构体"><a href="#一、结构体" class="headerlink" title="一、结构体"></a>一、结构体</h3><p>将不同类型的数据存储在同一个内存空间</p><p><strong>类型描述</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> 结构体名字</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    数据类型 成员<span class="number">1</span>;</span><br><span class="line">    数据类型 成员<span class="number">2</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 结构体的类型描述（只是表明这种数据结构由哪些类型的数据组成）不占用存储空间，无法直接用等号初始化</span></span><br><span class="line">如;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Student</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">    <span class="type">char</span> name[<span class="number">10</span>];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义结构体</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> 结构体名字 变量名</span></span><br><span class="line"><span class="class">如：</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Student</span> <span class="title">zxz</span>;</span>    </span><br></pre></td></tr></table></figure><h4 id="1-结构体的内存（地址对齐）"><a href="#1-结构体的内存（地址对齐）" class="headerlink" title="1.结构体的内存（地址对齐）"></a>1.结构体的内存（地址对齐）</h4><p><strong>对于内存中放置不同类型的数据的对齐信息都有考量—因为有了该对齐信息—可以让CPU很快的找到数据</strong>，地址对齐方便硬件取地址</p><p>代码1</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> NAMESIZE = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student_st</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        <span class="type">int</span> i;</span><br><span class="line">    <span class="type">char</span> ch;</span><br><span class="line">        <span class="type">float</span> f;</span><br><span class="line">        </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> <span class="title">st</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> *<span class="title">p</span> =</span> &amp;st;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(p) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(p));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(st) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(st));</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果 <strong>因为当前运行程序的机器环境为64位，所以指针大小为8个字节，而结构体<code>st</code>的大小为12字节</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230305095244.png" alt="image-20230305095236954"></p><p>代码2</p><p>在结构体中，添加一个字节 <code>ch2</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> NAMESIZE = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student_st</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        <span class="type">int</span> i;</span><br><span class="line">    <span class="type">char</span> ch;</span><br><span class="line">    <span class="type">char</span> ch2;</span><br><span class="line">        <span class="type">float</span> f;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> <span class="title">st</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> *<span class="title">p</span> =</span> &amp;st;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(p) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(p));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(st) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(st));</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果，与代码1结果相同</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230305095900.png" alt="image-20230305095900069"></p><p>代码3</p><p>将<code>float f</code>放到<code>ch</code>与<code>ch1</code>之间</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> NAMESIZE = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student_st</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        <span class="type">int</span> i;</span><br><span class="line">        <span class="type">char</span> ch;</span><br><span class="line">        <span class="type">float</span> f;</span><br><span class="line">        <span class="type">char</span> ch2;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> <span class="title">st</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> *<span class="title">p</span> =</span> &amp;st;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(p) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(p));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(st) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(st));</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果存在不同，<strong>指针仍是8字节，但是结构体不同，为16字节</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230305100140.png" alt="image-20230305100140476"></p><p>出现上面的情况是由于<strong>机器的地址对齐</strong>造成的</p><p><strong>参考公式</strong>： <code>addr % sizeof(i) </code> 如可以整除则变量i存储在该地址处</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230305102810.png" alt="image-20230305102810599"><strong>让机器不地址对齐</strong></p><p><strong>在结构体后声明关键词<code>__attributr__((packed))</code></strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> NAMESIZE = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student_st</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        <span class="type">int</span> i;</span><br><span class="line">        <span class="type">char</span> ch;</span><br><span class="line">        <span class="type">float</span> f;</span><br><span class="line">        <span class="type">char</span> ch2;</span><br><span class="line">&#125;__attributr__((packed));</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> <span class="title">st</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">student_st</span> *<span class="title">p</span> =</span> &amp;st;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(p) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(p));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;struct(st) = %ld\n&quot;</span>,<span class="keyword">sizeof</span>(st));</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果，<strong>结构体内存大小发生了变化</strong>，为10字节，即4+1+4+1 &#x3D; 12</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230305102157.png" alt="image-20230305102157819"></p><h3 id="二、共用体"><a href="#二、共用体" class="headerlink" title="二、共用体"></a>二、共用体</h3><h4 id="1-共用体"><a href="#1-共用体" class="headerlink" title="1. 共用体"></a>1. 共用体</h4><p><strong>形式：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">un</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="type">int</span> a;</span><br><span class="line">    <span class="type">char</span> ch;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 该共用体a为4字节  ch为1字节   因此un大小为4字节</span></span><br><span class="line"><span class="comment">// 创建对象 共用体a</span></span><br><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">un</span> <span class="title">a</span></span></span><br><span class="line"><span class="class"> </span></span><br></pre></td></tr></table></figure><p><strong>共用体的所占内存为：共用体内存大小为 共用体所包含的数据中所占内存最大的数据的内存大小</strong></p><p>实例程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line"></span><br><span class="line">union test_un</span><br><span class="line">&#123;</span><br><span class="line">        int i;</span><br><span class="line">        float f;  // 4字节</span><br><span class="line">        double d; // 共用体中内存最大的8字节</span><br><span class="line">        char ch;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">        union test_un a;</span><br><span class="line">        a.f = 345.678;</span><br><span class="line">        printf(&quot;%f\n&quot;,a.f);</span><br><span class="line">        printf(&quot;%ld\n&quot;,sizeof(a));</span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果   <strong>该共用体变量此时的内存大小为8字节，即double变量的内存大小</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230306092733.png" alt="image-20230306092726474"></p><h4 id="2-硬件存储"><a href="#2-硬件存储" class="headerlink" title="2. 硬件存储"></a>2. 硬件存储</h4><p>硬件存储数据两种方式：</p><p>大端：数据的第一位存在高地址中</p><p>小端：数据的第一位存在低地址中</p><h3 id="三、枚举"><a href="#三、枚举" class="headerlink" title="三、枚举"></a>三、枚举</h3><p>枚举类型可以视作<strong>有值的宏</strong>进行使用</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">day</span>&#123;</span></span><br><span class="line">        MON = <span class="number">1</span>,</span><br><span class="line">        TUS,</span><br><span class="line">        THR,</span><br><span class="line">        WES</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="class"><span class="keyword">enum</span> <span class="title">day</span> <span class="title">a</span> =</span> MON;</span><br><span class="line">        <span class="class"><span class="keyword">enum</span> <span class="title">day</span> <span class="title">b</span> =</span> WES;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,a);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,b);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>运行结果</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230307213550.png" alt="image-20230307213542982"></p>]]></content>
      
      
      <categories>
          
          <category> 嵌入式 </category>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 嵌入式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow 入门教程（2）</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Tensorflow/Tensorflow%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Tensorflow/Tensorflow%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="Tensorflow教程（2）"><a href="#Tensorflow教程（2）" class="headerlink" title="Tensorflow教程（2）"></a>Tensorflow教程（2）</h2><p><strong>通过Tensorflow实现经典的卷积神经网络</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305050935919.png" alt="image-20230505093533845"></p><h3 id="一、LeNet"><a href="#一、LeNet" class="headerlink" title="一、LeNet"></a>一、LeNet</h3><p>卷积神经网络的开山之作(1998)</p><p>文章链接：<a href="https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition">Gradient-Based Learning Applied to Document Recognition</a></p><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/273247515">Lenet5经典论文解读</a></p><h4 id="1-代码实现："><a href="#1-代码实现：" class="headerlink" title="1.代码实现："></a>1.代码实现：</h4><h5 id="（1）网络架构"><a href="#（1）网络架构" class="headerlink" title="（1）网络架构"></a>（1）网络架构</h5><p>LeNet时代没有<code>BN</code>以及<code>Dropout</code></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041146781.png" alt="image-20230504114615690"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041147270.png" alt="image-20230504114719184"></p><h5 id="（2）代码"><a href="#（2）代码" class="headerlink" title="（2）代码"></a>（2）代码</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Tensorflow实现LeNet</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line"># 加载数据集</span><br><span class="line">cifar10 = tf.keras.datasets.cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line"># 对输入数据进行皈依化</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LeNet5(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(LeNet5, self).__init__()</span><br><span class="line">        self.c1 = tf.keras.layers.Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">                         activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.p1 = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.c2 = tf.keras.layers.Conv2D(filters=<span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">                         activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.p2 = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.flatten = tf.keras.layers.Flatten()</span><br><span class="line">        self.f1 = tf.keras.layers.Dense(<span class="number">120</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.f2 = tf.keras.layers.Dense(<span class="number">84</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">        self.f3 = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    def call(self, x):</span><br><span class="line">        x = self.c1(x)</span><br><span class="line">        x = self.p1(x)</span><br><span class="line"></span><br><span class="line">        x = self.c2(x)</span><br><span class="line">        x = self.p2(x)</span><br><span class="line"></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = self.f2(x)</span><br><span class="line">        y = self.f3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LeNet5()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/LeNet5.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=True,</span><br><span class="line">                                                 save_best_only=True)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="meta"># print(model.trainable_variables)</span></span><br><span class="line">file = open(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line">##############################################<span class="meta">#    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="二、AlexNet"><a href="#二、AlexNet" class="headerlink" title="二、AlexNet"></a>二、AlexNet</h3><h4 id="1-代码实现"><a href="#1-代码实现" class="headerlink" title="1.代码实现"></a>1.代码实现</h4><p>论文原文中使用的是<code>LRN</code>操作，与<code>BN</code>操作类似</p><h5 id="（1）网络架构-1"><a href="#（1）网络架构-1" class="headerlink" title="（1）网络架构"></a>（1）网络架构</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041846834.png" alt="image-20230504184654735"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041857446.png" alt="image-20230504185755388"></p><h5 id="（2）代码-1"><a href="#（2）代码-1" class="headerlink" title="（2）代码"></a>（2）代码</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense</span><br><span class="line">from tensorflow.keras import Model</span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line">cifar10 = tf.keras.datasets.cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class AlexNet8(Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(AlexNet8, self).__init__()</span><br><span class="line">        self.c1 = Conv2D(filters=<span class="number">96</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.b1 = BatchNormalization()</span><br><span class="line">        self.a1 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p1 = MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.c2 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.b2 = BatchNormalization()</span><br><span class="line">        self.a2 = Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p2 = MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.c3 = Conv2D(filters=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">                         activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                         </span><br><span class="line">        self.c4 = Conv2D(filters=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">                         activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                         </span><br><span class="line">        self.c5 = Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">                         activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.p3 = MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.f1 = Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d1 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.f2 = Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d2 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.f3 = Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    def call(self, x):</span><br><span class="line">        x = self.c1(x)</span><br><span class="line">        x = self.b1(x)</span><br><span class="line">        x = self.a1(x)</span><br><span class="line">        x = self.p1(x)</span><br><span class="line"></span><br><span class="line">        x = self.c2(x)</span><br><span class="line">        x = self.b2(x)</span><br><span class="line">        x = self.a2(x)</span><br><span class="line">        x = self.p2(x)</span><br><span class="line"></span><br><span class="line">        x = self.c3(x)</span><br><span class="line"></span><br><span class="line">        x = self.c4(x)</span><br><span class="line"></span><br><span class="line">        x = self.c5(x)</span><br><span class="line">        x = self.p3(x)</span><br><span class="line"></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = self.d1(x)</span><br><span class="line">        x = self.f2(x)</span><br><span class="line">        x = self.d2(x)</span><br><span class="line">        y = self.f3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = AlexNet8()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/AlexNet8.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=True,</span><br><span class="line">                                                 save_best_only=True)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="meta"># print(model.trainable_variables)</span></span><br><span class="line">file = open(<span class="string">&#x27;./weight_txt/weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line">##############################################<span class="meta">#    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="3-VGGNet"><a href="#3-VGGNet" class="headerlink" title="3.VGGNet"></a>3.VGGNet</h4>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数组-函数-指针（1）</title>
      <link href="/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/%E6%95%B0%E7%BB%84-%E5%87%BD%E6%95%B0-%E6%8C%87%E9%92%88%EF%BC%881%EF%BC%89/"/>
      <url>/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C/%E6%95%B0%E7%BB%84-%E5%87%BD%E6%95%B0-%E6%8C%87%E9%92%88%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="数组-函数-指针"><a href="#数组-函数-指针" class="headerlink" title="数组-函数-指针"></a>数组-函数-指针</h2><h3 id="1-注释方法"><a href="#1-注释方法" class="headerlink" title="1. 注释方法"></a>1. 注释方法</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> 0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure><h3 id="2-空指针与野指针"><a href="#2-空指针与野指针" class="headerlink" title="2. 空指针与野指针"></a>2. 空指针与野指针</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 空指针</span></span><br><span class="line"><span class="type">int</span> *p = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 野指针 : 指针无确定指向，就发生了指针的使用</span></span><br><span class="line">如何杜绝： 定义指针开始就应该初始化，或者使其指向<span class="literal">NULL</span></span><br><span class="line">        如:<span class="type">int</span> *p = <span class="number">123</span>; <span class="comment">// 野指针</span></span><br></pre></td></tr></table></figure><h3 id="3-空类型指针"><a href="#3-空类型指针" class="headerlink" title="3. 空类型指针"></a>3. 空类型指针</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *p; <span class="comment">// 可以赋给任意类型的指针</span></span><br></pre></td></tr></table></figure><h3 id="4-指针与数组"><a href="#4-指针与数组" class="headerlink" title="4. 指针与数组"></a>4. 指针与数组</h3><h4 id="（1）指针与一维数组"><a href="#（1）指针与一维数组" class="headerlink" title="（1）指针与一维数组"></a>（1）指针与一维数组</h4><p><strong>可以使用一级指针指向一维数组</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> a[<span class="number">3</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">        <span class="type">int</span> i;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>;i&lt;<span class="keyword">sizeof</span>(a)/<span class="keyword">sizeof</span>(a[<span class="number">0</span>]);i++)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%p--&gt;%d\n&quot;</span>,&amp;a[i],a[i]);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义指针指向一维数组(数组名a代表数组第一个元素地址（a为地址常量，右值）)</span></span><br><span class="line">        <span class="type">int</span> *p = a;</span><br><span class="line">        <span class="comment">// 此时指针指向数组的第一个元素</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;第一个元素地址: %p--&gt; 第一个元素为: %d\n&quot;</span>,p,*p);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>;i&lt;<span class="keyword">sizeof</span>(a)/<span class="keyword">sizeof</span>(a[<span class="number">0</span>]);i++)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%p--&gt;%d\n&quot;</span>,(p+i),*(p+i));</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h4 id="（2）指针与二维数组"><a href="#（2）指针与二维数组" class="headerlink" title="（2）指针与二维数组"></a>（2）指针与二维数组</h4><p><strong>Warning:</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> a[<span class="number">2</span>][<span class="number">2</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;;</span><br><span class="line">        <span class="type">int</span> *p = a;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p   %p\n&quot;</span>,a,a+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>;i &lt; <span class="number">2</span>; i++)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>;j &lt; <span class="number">2</span>; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;%p --&gt; %d\n&quot;</span>,&amp;a[i][j],a[i][j]);</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译之后会出现以下Warning:</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230304155807.png" alt="image-20230304155807863"></p><p>运行之后发现：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230304155820.png" alt="image-20230304155820132"></p><p><strong>数组名a是：a[0] [0]的地址 a,a+1是：a[1] [0]的地址，a+1 是在行间进行跳跃，而p指向的是二维数组首地址，p+1指向的是a[0] [1],p是在列之间进行跳跃</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> *p = a;</span><br><span class="line"><span class="comment">// 上述代码进行修改：</span></span><br><span class="line"><span class="type">int</span> *p = *(a+<span class="number">0</span>);   <span class="comment">// 对行地址取星，相当于降级为列地址 如此运行不会有Warning</span></span><br></pre></td></tr></table></figure><hr><p>对于二维数组也可以如此遍历：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> a[<span class="number">2</span>][<span class="number">2</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;;</span><br><span class="line">        <span class="type">int</span> *p = *a;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p   %p\n&quot;</span>,a,a+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>;i &lt; <span class="number">2</span>; i++)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>;j &lt; <span class="number">2</span>; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// *(a+i) 变为列地址在后续进行+j，地址在列上进行移动</span></span><br><span class="line">                    <span class="comment">// *(*(a+i)+j) 得到的是a[i][j] 的元素 </span></span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;%p --&gt; %d\n&quot;</span>,*(a+i)+j,*(*(a+i)+j));</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><p><strong>通过列指针p进行遍历二维数组</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a[<span class="number">2</span>][<span class="number">2</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;;</span><br><span class="line"><span class="type">int</span> *p = *a;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%p   %p\n&quot;</span>,a,a+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> i;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i&lt;<span class="number">4</span>;i++)</span><br><span class="line">&#123;</span><br><span class="line">        <span class="comment">// p指针为列指针，在列之间进行跳跃</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%p --&gt; %d\n&quot;</span>,p+i,*(p+i));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><hr><h5 id="数组指针-重点"><a href="#数组指针-重点" class="headerlink" title="数组指针(重点)"></a>数组指针(重点)</h5><p><strong>指向数组的指针</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据类型 (*指针名)[] = 二维数组首地址;</span><br></pre></td></tr></table></figure><p><strong>例如：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a[<span class="number">2</span>][<span class="number">3</span>] = &#123;&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;,&#123;<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>&#125;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义一个数组指针，指向数组a，指针 p+1 移动大小为 3个整型字节大小，类似于行指针，通过“*”号 *(p+i) 进行降级，变为列指针，在列之间进行操作</span></span><br><span class="line"><span class="comment">// 数组指针p 指向的对象为 int[3] 存有三个整型字节的一维度数组</span></span><br><span class="line"><span class="type">int</span> (*p)[<span class="number">3</span>] = a; </span><br></pre></td></tr></table></figure><p><strong>例程：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> a[<span class="number">2</span>][<span class="number">3</span>] = &#123;&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;,&#123;<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>&#125;&#125;;</span><br><span class="line">        <span class="comment">// 定义一个指针数组</span></span><br><span class="line">        <span class="type">int</span> (*p)[<span class="number">3</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// p 指向的内存为大小为三个整型字节的一维数组地址，此时不会上一个程序有Warning,因为p+1移动大小为3个整型字节的大小，类似于行指针。而a+1同样如此移动三个整型字节的大小</span></span><br><span class="line">        <span class="comment">// 同样可以通过 *(p+1)对行指针进行降级 变为列指针</span></span><br><span class="line">        p = a;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p----&gt;%d---&gt;%p----&gt;%d\n&quot;</span>,p,(*p)[<span class="number">0</span>],p+<span class="number">1</span>,*(p+<span class="number">1</span>)[<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p----&gt;%d----&gt;%p---&gt;%d\n&quot;</span>,a,(*a)[<span class="number">0</span>],a+<span class="number">1</span>,*(a+<span class="number">1</span>)[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;<span class="number">2</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;<span class="number">3</span>;j++)</span><br><span class="line">                &#123;</span><br><span class="line">                        <span class="comment">// *(p+i),用*号是的p降级从行指针变为列指针，可在列之间操作</span></span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;%p --&gt; %d\n&quot;</span>,*(p+i)+j,*(*(p+i)+j));</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>运行结果</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230304163440.png" alt="image-20230304163440412"></p><h3 id="5-指针数组"><a href="#5-指针数组" class="headerlink" title="5.指针数组"></a>5.指针数组</h3><p><strong>定义一个数组，里面的元素为指针</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据类型 *数组名 [长度];</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> a = <span class="number">1</span>,b = <span class="number">2</span>, c = <span class="number">3</span>;</span><br><span class="line"><span class="comment">// 指针数组p，里面的元素均为int *,地址</span></span><br><span class="line"><span class="type">int</span> *p[<span class="number">3</span>] = &#123;&amp;a,&amp;b,&amp;c&#125;;</span><br></pre></td></tr></table></figure><p><strong>例程：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a = <span class="number">1</span>,b = <span class="number">2</span>, c = <span class="number">3</span>;</span><br><span class="line"><span class="comment">// 指针数组p，里面的元素均为int *,地址</span></span><br><span class="line"><span class="type">int</span> *p[<span class="number">3</span>] = &#123;&amp;a,&amp;b,&amp;c&#125;;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%p---%p---%p\n&quot;</span>,&amp;a,&amp;b,&amp;c);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d---%d---%d\n&quot;</span>,*p[<span class="number">0</span>],*p[<span class="number">1</span>],*p[<span class="number">2</span>]);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-指针函数与函数指针"><a href="#6-指针函数与函数指针" class="headerlink" title="6.指针函数与函数指针"></a>6.指针函数与函数指针</h3><h4 id="（1）指针函数"><a href="#（1）指针函数" class="headerlink" title="（1）指针函数"></a>（1）指针函数</h4><p>指针函数本质还是一个函数，返回为一个指针，地址</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ret *<span class="title function_">func</span><span class="params">(args, ...)</span>;</span><br></pre></td></tr></table></figure><p><code>func</code>是一个函数，<code>args</code>是形参列表，<code>ret *</code>作为一个整体，是 <code>func</code>函数的返回值，是一个指针的形式</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义指针函数</span></span><br><span class="line"><span class="type">int</span> *<span class="title function_">sum</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">static</span> <span class="type">int</span> result;  <span class="comment">//此处变量为静态局部变量</span></span><br><span class="line"><span class="type">int</span> *p;</span><br><span class="line">result = a + b;</span><br><span class="line">p = &amp;result;</span><br><span class="line"><span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> b = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> *p = sum(a, b);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;the result is %d&quot;</span>,*p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>示例二:</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义指针函数</span></span><br><span class="line"><span class="type">int</span> *<span class="title function_">sum</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> result;      <span class="comment">//此时变量为，普通局部变量</span></span><br><span class="line"><span class="type">int</span> *p;</span><br><span class="line">result = a + b;</span><br><span class="line">p = &amp;result;</span><br><span class="line"><span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> b = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> *p = sum(a, b);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;the result is %d&quot;</span>,*p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230304165649.png" alt="image-20230304165649028"></p><p><strong>上述两个实例，结果输出，并无差别，但是如果我们在main函数中做一些修改如下：</strong></p><p><strong>示例三：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义指针函数</span></span><br><span class="line"><span class="type">int</span> *<span class="title function_">sum</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> result;      <span class="comment">//此时变量为，普通局部变量</span></span><br><span class="line"><span class="type">int</span> *p;</span><br><span class="line">result = a + b;</span><br><span class="line">p = &amp;result;</span><br><span class="line"><span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> b = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> *p = sum(a, b);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;wait for a while...\n&quot;</span>);    <span class="comment">//此处加一句打印</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;the result is %d&quot;</span>,*p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>运行结果</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/20230304165818.png" alt="image-20230304165818314"></p><p><strong>出现以上结果的原因：</strong></p><ul><li>一般的<strong>局部变量存在在栈区</strong>，当函数结束，栈区变量就会释放，倘若在函数内部定义一个局部变量，在使用指针指向该变量,当函数调用结束，这个变量的空间就会被释放。这是放回该地址的指针，也不一定得到正确的结果，上述<strong>示例一、示例二</strong>，在返回指针后，立马访问也是巧合。<strong>但是如果我们等待一段时间再去访问，这是可能该地址，可能地址以及被其他变量所占用</strong></li><li><strong>解决方法：</strong><ul><li>在函数内，使用static去修饰需要返回地址的变量，该变量就会变成静态变量，<strong>静态变量存放在数据段。静态变量的生命周期为整个程序的运行周期</strong></li><li>使用全局变量，<strong>全局变量同样存放在数据段，其生命周期为整个程序的运行周期</strong>，但是不推荐！</li></ul></li></ul><h4 id="（2）函数指针"><a href="#（2）函数指针" class="headerlink" title="（2）函数指针"></a>（2）函数指针</h4><p><strong>函数指针本质还是指针，指向函数的指针</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ret (*p)(args, ...);</span><br><span class="line"><span class="comment">//ret为返回值，*p作为一个整体，代表的是指向该函数的指针，args为形参列表。其中p被称为函数指针变量 </span></span><br><span class="line"><span class="keyword">typedef</span> <span class="title function_">int</span> <span class="params">(*fun_ptr)</span><span class="params">(<span class="type">int</span>,<span class="type">int</span>)</span>; </span><br><span class="line"><span class="comment">// 声明一个指向同样参数、返回值的函数指针类型</span></span><br></pre></td></tr></table></figure><p><strong>函数指针初始化：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">函数指针变量 = 函数名;</span><br></pre></td></tr></table></figure><p><strong>实例程序</strong></p><p>以下实例声明了函数指针变量 p，指向函数 max：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"> </span><br><span class="line">int max(int x, int y)</span><br><span class="line">&#123;</span><br><span class="line">    return x &gt; y ? x : y;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">    /* p 是函数指针 */</span><br><span class="line">    int (* p)(int, int) = &amp; max; // &amp;可以省略</span><br><span class="line">    int a, b, c, d;</span><br><span class="line">    printf(&quot;请输入三个数字:&quot;);</span><br><span class="line">    scanf(&quot;%d %d %d&quot;, &amp; a, &amp; b, &amp; c);</span><br><span class="line">    /* 与直接调用函数等价，d = max(max(a, b), c) */</span><br><span class="line">    d = p(p(a, b), c); </span><br><span class="line">    printf(&quot;最大的数字是: %d\n&quot;, d);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下实例声明了函数指针变量p，指向函数sum</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">sum</span><span class="params">(<span class="type">int</span> a,<span class="type">int</span> b)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span>(*p)(<span class="type">int</span> a, <span class="type">int</span> b);<span class="comment">//定义一个函数指针---指向函数的指针</span></span><br><span class="line">p = &amp;sum;   <span class="comment">//此处的&amp;可以省略，p=sum----&gt;满足：函数指针变量=函数名</span></span><br><span class="line"><span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> b = <span class="number">2</span>;</span><br><span class="line"><span class="type">int</span> c = <span class="number">3</span>;</span><br><span class="line"><span class="type">int</span> result = p(p(a,b),c);<span class="comment">//等价sum(sum(a,b),c)</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;the result is %d&quot;</span>,result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（3）函数指针数组"><a href="#（3）函数指针数组" class="headerlink" title="（3）函数指针数组"></a>（3）函数指针数组</h4><p><strong>定义</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">类型 (*数组名[下标]) (形参)</span><br><span class="line">如：</span><br><span class="line">    <span class="type">int</span> (*arr[N])(<span class="type">int</span>)    如何理解：先看括号，首先是一个数组其次是一个指针 括号中整体就是 指针数组；最后还是一个函数；三者组合就是 函数指针数组</span><br></pre></td></tr></table></figure><h4 id="（4）回调函数"><a href="#（4）回调函数" class="headerlink" title="（4）回调函数"></a>（4）回调函数</h4><p>回调函数通过<strong>函数指针</strong>调用</p><p>的函数。<strong>其将函数指针作为一个参数，传递给另一个函数。回调函数并不是由实现方直接调用，而是在特定的事件或条件发生时由另外一方来调用的。</strong></p><p>函数指针的一个非常典型的应用就是<strong>回调函数</strong></p><p><strong>回调函数是由别人的函数执行时调用你实现的函数。</strong></p><p><strong>实例程序</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 函数功能:累加求和</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">func_sum</span><span class="params">(<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;n must be &gt; 0\n&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">                sum += i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 回调函数，其中第二个参数是一个函数指针，通过该函数指针来调用求和函数，并把结果返回给主调函数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">callback</span><span class="params">(<span class="type">int</span> n,<span class="type">int</span> (*p)(<span class="type">int</span>))</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="keyword">return</span> p(n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> n = <span class="number">5</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;thr sum is from 0 to %d is %d\n&quot;</span>,n,callback(n,func_sum));</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>使用回调函数的优势:</strong></p><p>在这个程序中，回调函数callback无需关心<code>func_sum</code>是怎么实现的，只需要去调用即可。<br>这样的好处就是，如果以后对求和函数有优化，比如新写了个<code>func_sum2</code>函数的实现，我们只需要在调用回调函数的地方将函数指针指向<code>func_sum2</code>即可，而无需去修改callback函数内部。</p><p>回调函数广泛用于开发场景中，比如信号函数、线程函数等，都使用到了回调函数的知识。</p>]]></content>
      
      
      <categories>
          
          <category> 嵌入式 </category>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 嵌入式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ Primer Plus 学习要点（1）</title>
      <link href="/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C%E5%8A%A0%E5%8A%A0/c++%20Primer%20Plus%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%20%EF%BC%881%EF%BC%89/"/>
      <url>/2023/05/16/%E5%B5%8C%E5%85%A5%E5%BC%8F/C%E5%8A%A0%E5%8A%A0/c++%20Primer%20Plus%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%20%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="一-C语言中-p与-x的区别"><a href="#一-C语言中-p与-x的区别" class="headerlink" title="一.C语言中%p与%x的区别"></a>一.C语言中%p与%x的区别</h2><p>“%p”中的p是pointer（指针）的缩写。%p是打印地址的, 而%x是以十六进制形式打印。<br>%p是打印地址（指针地址）的，是十六进制的形式，但是会全部打完，即有多少位打印多少位。</p><p>32位编译器的指针变量为4个字节(32位)，64位编译器的指针变量为8个字节(64位)。</p><p>所以，在32位编译器下，使用%p打印指针变量，则会显示32位的地址（16进制的）；在64位编译器下，使用%p打印指针变量，则会显示64位的地址（16进制的），左边空缺的会补0。</p><p>%x：无符号十六进制整数(字母小写，不像上面指针地址那样补零)</p><p>%X：无符号十六进制整数(字母大写，不像上面指针那样补零)</p><p>%x、%X和%p的相同点都是16进制，不同点是%p按编译器位数长短（32位&#x2F;64位）输出地址，不够的补零</p><h2 id="二-C-中如何打印字符的地址"><a href="#二-C-中如何打印字符的地址" class="headerlink" title="二.C++中如何打印字符的地址"></a>二.C++中如何打印字符的地址</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        <span class="type">int</span> a = <span class="number">17</span>;</span><br><span class="line">        <span class="type">char</span> c1 = <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        <span class="type">char</span> c2 = <span class="string">&#x27;b&#x27;</span>;</span><br><span class="line">        <span class="type">char</span> *ch1 = &amp;c1;</span><br><span class="line">        <span class="type">char</span> *ch2 = &amp;c2;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p\n&quot;</span>,&amp;c1);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%p\n&quot;</span>,ch1);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%x\n&quot;</span>,a);</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;static_cast&lt;void *&gt;(&amp;c1)=&quot;</span>&lt;&lt;<span class="built_in">static_cast</span>&lt;<span class="type">void</span>*&gt;(&amp;c1)&lt;&lt;endl;</span><br><span class="line">    cout &lt;&lt; (<span class="type">int</span> *)&amp;c1 &lt;&lt; endl;   <span class="comment">// 同样可以输出字符地址</span></span><br><span class="line">   cout &lt;&lt; (string *)&amp;c1 &lt;&lt; endl;   <span class="comment">// 同样可以输出字符地址</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>解析：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%p\n&quot;</span>,&amp;c1); <span class="comment">//打印c1字符的地址</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%p\n&quot;</span>,ch1); <span class="comment">//打印ch1指针变量的值</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%x\n&quot;</span>,a);   <span class="comment">//将整型a以16进制形式输出</span></span><br></pre></td></tr></table></figure><p><strong>重点</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">C++标准库中I/O类对输出操作符&lt;&lt;重载，在遇到字符型指针时会将其当做字符串名来处理，输出指针所指的字符串。既然这样，我们就别让他知道那是字符型指针，所以得进行类型转换，即：希望任何字符型的指针变量输出为地址的话，都要作一个转换，即强制<span class="type">char</span> *转换成<span class="type">void</span> *，如下所示：</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;static_cast&lt;void *&gt;(&amp;c1)=&quot;</span>&lt;&lt;static_cast&lt;<span class="type">void</span>*&gt;(&amp;c1)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// static_cast是一个强制类型转换操作符。强制类型转换，也称为显式转换</span></span><br><span class="line">链接：https:<span class="comment">//blog.csdn.net/zongyinhu/article/details/49512919</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (<span class="type">int</span> *)&amp;c1 &lt;&lt; <span class="built_in">endl</span>;   <span class="comment">// 同样可以输出字符地址</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (<span class="built_in">string</span> *)&amp;c1 &lt;&lt; <span class="built_in">endl</span>;   <span class="comment">// 同样可以输出字符地址</span></span><br></pre></td></tr></table></figure><h2 id="三-简单文件的输入输出"><a href="#三-简单文件的输入输出" class="headerlink" title="三.简单文件的输入输出"></a>三.简单文件的输入输出</h2><p>《C++ Primer Plus》第六章第八小节</p><h3 id="1-文件的输出"><a href="#1-文件的输出" class="headerlink" title="1. 文件的输出"></a>1. 文件的输出</h3><p>​将文件内容输出至文本中</p><p>​<strong>类比：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cout控制台输出---cout输出在屏幕上</span></span><br><span class="line"><span class="number">1.</span> 包含头文件 iostream</span><br><span class="line"><span class="number">2.</span> 头文件iostream定义一个用于处理输出的ostream类</span><br><span class="line"><span class="number">3.</span> 头文件iostream声明一个名为<span class="built_in">cout</span>的ostream变量（对象）</span><br><span class="line"><span class="number">4.</span> 必须指明名称空间；必须使用编译指令using或者<span class="built_in">std</span>::</span><br><span class="line"><span class="number">5.</span> 可以使用<span class="built_in">cout</span>和运算符&lt;&lt; 来显示各种类型的数据</span><br><span class="line"></span><br><span class="line"><span class="comment">// 文件输出---ofstream对象输出在对象所关联的文件中</span></span><br><span class="line"><span class="number">1.</span> 包含头文件fstream</span><br><span class="line"><span class="number">2.</span> 头文件fstream定义一个用于处理输出的ofstream类</span><br><span class="line"><span class="number">3.</span> 需要声明一个或多个ofstream变量（对象）</span><br><span class="line"><span class="number">4.</span> 需将ofstream对象与文件关联起来，例如方法：open();</span><br><span class="line"><span class="number">5.</span> 使用完文件后，用close()将其关闭</span><br><span class="line"><span class="number">6.</span> 可结合使用ofstream对象和运算符&lt;&lt;来输出各种类型的数据</span><br><span class="line">    </span><br><span class="line"><span class="comment">// outFile 可以使用 cout 可使用的任何方法</span></span><br></pre></td></tr></table></figure><h6 id="将文件与ofstream关联起来"><a href="#将文件与ofstream关联起来" class="headerlink" title="将文件与ofstream关联起来"></a>将文件与ofstream关联起来</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ofstream outFile;</span><br><span class="line"><span class="comment">// 方法1：</span></span><br><span class="line">outFile(<span class="string">&quot;fish.txt&quot;</span>);</span><br><span class="line"><span class="comment">// 方法2：</span></span><br><span class="line"><span class="type">char</span> filename[<span class="number">50</span>];</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; filename;</span><br><span class="line">outFile(filename);</span><br></pre></td></tr></table></figure><hr><h6 id="示例程序"><a href="#示例程序" class="headerlink" title="示例程序"></a>示例程序</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">char</span> automobile[<span class="number">50</span>];</span><br><span class="line"><span class="type">int</span> year;</span><br><span class="line"><span class="type">double</span> a_price;</span><br><span class="line"><span class="type">double</span> d_price;</span><br><span class="line"></span><br><span class="line">ofstream outFile;</span><br><span class="line">outFile.open(<span class="string">&quot;carinfo.txt&quot;</span>);  <span class="comment">// associate with a file</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Enter the make and model of automobile : &quot;</span>;</span><br><span class="line"><span class="built_in">cin</span>.getline(automobile , <span class="number">50</span>);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Enter the model year : &quot;</span> ;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; year;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Make the original asking price : &quot;</span>;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; a_price ;</span><br><span class="line">d_price = <span class="number">0.913</span> * a_price;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;-----------------------&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; fixed;</span><br><span class="line"><span class="built_in">cout</span>.precision(<span class="number">2</span>); <span class="comment">// 保留两位小数</span></span><br><span class="line"><span class="built_in">cout</span>.setf(ios_base::showpoint);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Make and model: &quot;</span> &lt;&lt; automobile &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Year: &quot;</span> &lt;&lt; year &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Was asking: $&quot;</span> &lt;&lt; a_price &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Now asking: $&quot;</span> &lt;&lt; d_price &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;-----------------------&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">outFile &lt;&lt; fixed;</span><br><span class="line">outFile.precision(<span class="number">2</span>); <span class="comment">// 保留两位小数</span></span><br><span class="line">outFile.setf(ios_base::showpoint);</span><br><span class="line">outFile &lt;&lt; <span class="string">&quot;Make and model: &quot;</span> &lt;&lt; automobile &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">outFile &lt;&lt; <span class="string">&quot;Year: &quot;</span> &lt;&lt; year &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       outFile &lt;&lt; <span class="string">&quot;Was asking: $&quot;</span> &lt;&lt; a_price &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">outFile &lt;&lt; <span class="string">&quot;Now asking: $&quot;</span> &lt;&lt; d_price &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">outFile.close();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf()的作用是通过设置格式标志来控制输出形式，其中ios_base::fixed表示：用正常的记数方法显示浮点数(与科学计数法相对应)；ios_base::floatfield表示小数点后保留<span class="number">6</span>位小数。</span><br><span class="line"></span><br><span class="line">setf()的第一原型：</span><br><span class="line">C++为标准输入和输出定义了一些格式标志：</span><br><span class="line">例如 ： <span class="built_in">cout</span>.setf(ios_base::left); <span class="comment">//对所有cout的输出进行左对齐调整.</span></span><br></pre></td></tr></table></figure><p><img src="C:\Users\27239\AppData\Roaming\Typora\typora-user-images\image-20220326105933847.png" alt="image-20220326105933847"></p><hr><h3 id="2-文件的读取"><a href="#2-文件的读取" class="headerlink" title="2. 文件的读取"></a>2. 文件的读取</h3><p><strong>类比</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cout 的文件输入</span></span><br><span class="line"><span class="number">1.</span> 头文件包含iostream</span><br><span class="line"><span class="number">2.</span> 头文件定义用于处理输入的istream</span><br><span class="line"><span class="number">3.</span> 头文件iostream声明了一个名为<span class="built_in">cin</span>的istream变量(对象)</span><br><span class="line"><span class="number">4.</span> 必须指明名称空间</span><br><span class="line"><span class="number">5.</span> 可结合使用<span class="built_in">cin</span>和运算符&gt;&gt; 来读取各种类型数据</span><br><span class="line"><span class="number">6.</span> 可使用<span class="built_in">cin</span>和get()方法来读取一个字符，使用<span class="built_in">cin</span>和getline()来读取一行字符</span><br><span class="line"><span class="number">7.</span> 可结合使用<span class="built_in">cin</span>和eof()、fail()判断输入是否成功</span><br><span class="line"><span class="number">8.</span> 对象<span class="built_in">cin</span>本身被用作测试条件时，若最后一个读取操作成功，它将被转换为布尔值<span class="literal">true</span> 否则 转为<span class="literal">false</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">// 文件的读取</span></span><br><span class="line"><span class="number">1.</span> 头文件fstream</span><br><span class="line"><span class="number">2.</span> 头文件fstream定义一个用于处理输入的ifstream类</span><br><span class="line"><span class="number">3.</span> 需要声明一个或多个ifstream变量（对象）</span><br><span class="line"><span class="number">4.</span> 必须指明名称空间</span><br><span class="line"><span class="number">5.</span> 需要将ifstream对象与文件关联起来,使用方法open()</span><br><span class="line"><span class="number">6.</span> 使用完文件后，使用close()方法将其关闭</span><br><span class="line"><span class="number">7.</span> 结合使用ifstream对象和运算符&gt;&gt; 来读取各种类型的数据</span><br><span class="line"><span class="number">8.</span> 可以使用ifstream对象和get()方法来读取一个字符，使用ifstream对象和getline()读取一行字符</span><br><span class="line"><span class="number">9.</span> 可结合使用ifstream和eof()、fail()判断输入是否成功</span><br><span class="line"><span class="number">10.</span> 对象ifstream本身被用作测试条件时，若最后一个读取操作成功，它将被转换为布尔值<span class="literal">true</span> 否则 转为<span class="literal">false</span></span><br></pre></td></tr></table></figure><h6 id="将文件与ofstream关联起来-1"><a href="#将文件与ofstream关联起来-1" class="headerlink" title="将文件与ofstream关联起来"></a>将文件与ofstream关联起来</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ifstream inFile;</span><br><span class="line"><span class="comment">//方法1：</span></span><br><span class="line">inFile.open(<span class="string">&quot;123.txt&quot;</span>);</span><br><span class="line"><span class="comment">//方法2：</span></span><br><span class="line"><span class="type">char</span> filename[<span class="number">50</span>];</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; filename;</span><br><span class="line">inFile.open(filename);</span><br></pre></td></tr></table></figure><h6 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">double</span> wt;</span><br><span class="line">inFile &gt;&gt; wt;  <span class="comment">// read a num from 123.txt</span></span><br><span class="line"><span class="type">char</span> line[<span class="number">80</span>];</span><br><span class="line">inFile.getline(line,<span class="number">80</span>); <span class="comment">// read a line of text</span></span><br></pre></td></tr></table></figure><h6 id="is-open"><a href="#is-open" class="headerlink" title="is_open()"></a>is_open()</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 检查文件是否成功被打开使用方法is_open()</span></span><br><span class="line">inFile.open(<span class="number">123.</span>txt);</span><br><span class="line"><span class="keyword">if</span>(!inFile.is_open())   <span class="comment">// 当文本打开失败---&gt; 不允许读写 ---&gt; 或者文件不存在时</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_FALLURE); <span class="comment">// exit()在头文件cstdlib中定义，还定义了一个同操作系统通信的参数值EXIT_FAILURE ----&gt; exit()终止程序 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="other"><a href="#other" class="headerlink" title="other"></a>other</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">eof（）方法用于判断最后一次读取数据时候是否遇到EOF，若是返回<span class="literal">true</span>  </span><br><span class="line"></span><br><span class="line">fail（）方法用于判断最后一次读取数据的时候是否遇到了类型不配的情况，若是返回<span class="literal">true</span>（如果遇到了EOF，该方法也返回<span class="literal">true</span>）</span><br><span class="line"></span><br><span class="line">bad（） 如果出现意外的问题，如文件受损或硬件故障，最后一次读取数据的时候发生了这样的问题，方法bad（）将返回<span class="literal">true</span>。</span><br><span class="line"></span><br><span class="line">good（） 该方法在没有发生任何错误的时候返回<span class="literal">true</span>。该方法也指出的最后一次读取输入的操作是否成功。</span><br></pre></td></tr></table></figure><h6 id="实例程序"><a href="#实例程序" class="headerlink" title="实例程序"></a>实例程序</h6><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> Size = <span class="number">60</span>;</span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">char</span> filename[Size];.</span><br><span class="line">        </span><br><span class="line">ifstream inFile;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Enter name of data file :&quot;</span>;</span><br><span class="line"><span class="built_in">cin</span>.getline(filename,Size);</span><br><span class="line">inFile.open(filename);  <span class="comment">// associate inFile with a file</span></span><br><span class="line"><span class="keyword">if</span>(!inFile.is_open())<span class="comment">// failed to open the file</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Could not open the file &quot;</span> &lt;&lt; filename &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Program terminating.\n&quot;</span> ;</span><br><span class="line"><span class="built_in">exit</span>(EXIT_FAILURE);  <span class="comment">// Abnormal exit</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">double</span> value;</span><br><span class="line"><span class="type">double</span> sum = <span class="number">0.0</span>;</span><br><span class="line"><span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;------ Data Reading -------&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">inFile &gt;&gt; value;</span><br><span class="line"><span class="keyword">while</span>(inFile.good())  <span class="comment">// Judge whether the last read was successful</span></span><br><span class="line">&#123;</span><br><span class="line">++count;</span><br><span class="line">sum += value;</span><br><span class="line">inFile &gt;&gt; value;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// If the last reading fails, judge the reason</span></span><br><span class="line"><span class="keyword">if</span>(inFile.eof())   <span class="comment">// the end of the file</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;End of file reached.\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(inFile.fail())</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// Determine whether the last read encountered a type mismatch</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Input terminated by data minmatch.\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Input terminated for unknown reason.\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Judge whether there is data in the file</span></span><br><span class="line"><span class="keyword">if</span>(<span class="number">0</span>==count)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;No data processed.\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Items read : &quot;</span> &lt;&lt; count &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Sum: &quot;</span> &lt;&lt; sum &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Average: &quot;</span> &lt;&lt; sum/count &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">inFile.close();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四-指针数组与数组指针"><a href="#四-指针数组与数组指针" class="headerlink" title="四.指针数组与数组指针"></a>四.指针数组与数组指针</h2><p><strong>[]的优先级高于</strong>*</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> (*arr)[<span class="number">4</span>];             <span class="comment">// 数组指针，本质为一个指针，指向一个数组，数组中有四个元素，每一个元素都是int 类型----指向二维数组</span></span><br><span class="line"><span class="type">int</span> *arr[<span class="number">4</span>];               <span class="comment">// 指针数组，本质为一个数组，有四个元素，每一个元素都是int * 类型</span></span><br></pre></td></tr></table></figure><p><strong>实例：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> date[<span class="number">3</span>][<span class="number">4</span>] = &#123;&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;,&#123;<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>&#125;,&#123;<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>&#125;&#125;;</span><br><span class="line">        <span class="type">int</span> (*arr)[<span class="number">4</span>] = date;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(date[<span class="number">1</span>]) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(arr[<span class="number">1</span>]) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; arr[<span class="number">1</span>][<span class="number">2</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 结果： 都为 sizeof结果都为16  第一个date[1],第一行的数组，为第一个元素，有4*4=16个字节</span></span><br><span class="line"><span class="comment">//                           第二个arr[1], 指向第一个数组元素，即date的第一行也为4*4=16个字节</span></span><br></pre></td></tr></table></figure><p><strong>易混淆</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">sum</span><span class="params">(<span class="type">int</span> (*arr)[<span class="number">4</span>])</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i&lt;<span class="number">4</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">                result += (*arr + i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">int</span> arr[]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">        <span class="type">int</span> result = sum(arr);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;the result is: &quot;</span> &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 编译报错：</span></span><br><span class="line"><span class="number">13.</span>cpp: In function ‘<span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>’:</span><br><span class="line">13.cpp:19:19: error: cannot convert ‘<span class="type">int</span>*’ to ‘<span class="title function_">int</span> <span class="params">(*)</span>[4]’</span><br><span class="line">   19 |  <span class="type">int</span> result = sum(arr);</span><br><span class="line">      |                   ^~~</span><br><span class="line">      |                   |</span><br><span class="line">      |                   <span class="type">int</span>*</span><br><span class="line"><span class="number">13.</span>cpp:<span class="number">6</span>:<span class="number">15</span>: note:   initializing argument <span class="number">1</span> of ‘<span class="type">int</span> <span class="title function_">sum</span><span class="params">(<span class="type">int</span> (*)[<span class="number">4</span>])</span>’</span><br><span class="line">    6 | <span class="type">int</span> <span class="title function_">sum</span><span class="params">(<span class="type">int</span> (*arr)[<span class="number">4</span>])</span></span><br><span class="line">      |         ~~~~~~^~~~~~~</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 解析：</span></span><br><span class="line">    <span class="type">int</span> arr[] 一维数组，的arr本质是一个<span class="type">int</span> * 类型的指针变量，存储的为地址。即arr数组的首地址</span><br><span class="line">    <span class="title function_">int</span> <span class="params">(*arr)</span>[4] 数组指针，本质为一个指针，指向一个数组，指向由4个<span class="type">int</span>组成的数组指针。其类型为 <span class="title function_">int</span> <span class="params">(*)</span>[4] 无法相互转换</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="五-数组替代"><a href="#五-数组替代" class="headerlink" title="五.数组替代"></a>五.数组替代</h2><h3 id="1-vector模板类"><a href="#1-vector模板类" class="headerlink" title="1.vector模板类"></a>1.vector模板类</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 头文件 #include &lt;vector&gt;</span></span><br><span class="line"><span class="comment">// vector包含在命名空间std中</span></span><br><span class="line"><span class="comment">// 声明：</span></span><br><span class="line"><span class="built_in">vector</span>&lt;typeName&gt; <span class="title function_">vt</span><span class="params">(n_elem)</span></span><br><span class="line"><span class="comment">// typeName 为类型   ， n_elem 元素个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实例：</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt; vi;   <span class="comment">// 长度为0的vector，可使用vector的方法对其进行插入或者添加值 ---&gt; 自动调整长度 ---&gt; 动态数组 </span></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="type">double</span>&gt; <span class="title function_">vd</span><span class="params">(<span class="number">5</span>)</span>;</span><br><span class="line"><span class="comment">// 功能比数组强大，但是其效率较低</span></span><br><span class="line"><span class="comment">// 是使用New创建动态数组的替代品，使用new与delete管理内存，但是该过程是自动完成的</span></span><br><span class="line"><span class="comment">// 使用堆(new) ，动态内存分配，自由存储区</span></span><br></pre></td></tr></table></figure><hr><h3 id="2-array模板类（C-11）"><a href="#2-array模板类（C-11）" class="headerlink" title="2.array模板类（C++11）"></a>2.array模板类（C++11）</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 头文件 #include &lt;array&gt;</span></span><br><span class="line"><span class="comment">// array包含在命名空间std中</span></span><br><span class="line"><span class="comment">// 声明：</span></span><br><span class="line"><span class="built_in">array</span>&lt;typeName,n_elem&gt; arr</span><br><span class="line"><span class="comment">// typeName 为类型   ， n_elem 元素个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实例：</span></span><br><span class="line">        <span class="built_in">array</span>&lt;<span class="type">int</span>,<span class="number">5</span>&gt; ai;   </span><br><span class="line"><span class="comment">// array 长度固定,效率与数组一样，安全性相对数组更高</span></span><br><span class="line"><span class="comment">// 使用栈，静态内存分配</span></span><br></pre></td></tr></table></figure><hr><h3 id="3-超界问题解决"><a href="#3-超界问题解决" class="headerlink" title="3.超界问题解决"></a>3.超界问题解决</h3><p><strong>vector 与 array 不会对于错误进行检查</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 例如</span></span><br><span class="line"><span class="built_in">array</span>&lt;<span class="type">double</span>,<span class="number">5</span>&gt; arr;</span><br><span class="line">arr[<span class="number">-2</span>] = <span class="number">2.3</span>;  <span class="comment">// 超界问题，但是系统不会报错 ---&gt; 编译器代码转换为 *(arr-2) = 2.3;</span></span><br></pre></td></tr></table></figure><p><strong>使用成员函数 at() 进行非法索引的捕获</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">arr.at(<span class="number">-2</span>);   </span><br><span class="line"><span class="comment">// 非法，at对于-2非法索引进行捕获\</span></span><br><span class="line"><span class="comment">// 报错：</span></span><br><span class="line"><span class="built_in">terminate</span> called after throwing an instance of <span class="string">&#x27;std::out_of_range&#x27;</span></span><br><span class="line">  what():  <span class="built_in">array</span>::at: __n (which is <span class="number">18446744073709551614</span>) &gt;= _Nm (which is <span class="number">5</span>)</span><br><span class="line">已放弃 (核心已转储)</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><h2 id="六-函数指针"><a href="#六-函数指针" class="headerlink" title="六. 函数指针"></a>六. 函数指针</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 函数定义</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">pf</span><span class="params">(<span class="type">int</span>)</span>   <span class="comment">// 可以不写形参变量名字</span></span><br></pre></td></tr></table></figure><p><strong>函数指针</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">double</span> (*pf)(<span class="type">int</span>) </span><br><span class="line"><span class="comment">// () 和[] 一样优先级高于*</span></span><br><span class="line"><span class="comment">// 上述定义 解释 --&gt; pf是一个指向返回值为double，形参类型为int的函数指针</span></span><br></pre></td></tr></table></figure><p><strong>区分指针函数</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">double</span> *<span class="title function_">pf</span><span class="params">(<span class="type">int</span>)</span></span><br><span class="line"><span class="comment">// 由于 () 优先级高于 *</span></span><br><span class="line"><span class="comment">// 上述定义解释 --&gt; 首先是一个函数 形参类型为 int ,函数的返回值为 double * 类型</span></span><br></pre></td></tr></table></figure><h3 id="1-使用指针来调用函数"><a href="#1-使用指针来调用函数" class="headerlink" title="1. 使用指针来调用函数"></a>1. 使用指针来调用函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">double</span> <span class="title function_">pam</span><span class="params">(<span class="type">int</span>)</span>;</span><br><span class="line"><span class="type">double</span> (*pf)(<span class="type">int</span>);</span><br><span class="line">pf = pam;         <span class="comment">// pf指针指向pam函数</span></span><br><span class="line"><span class="type">double</span> x = pam(<span class="number">5</span>);</span><br><span class="line"><span class="type">double</span> y = (*pf)(<span class="number">5</span>); <span class="comment">// 通过函数指针来调用函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// C++同样允许以下方式：</span></span><br><span class="line"><span class="type">double</span> z = pf(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><hr><h3 id="2-深入（C-primer-plus-202页）"><a href="#2-深入（C-primer-plus-202页）" class="headerlink" title="2. 深入（C++ primer plus 202页）"></a>2. 深入（C++ primer plus 202页）</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义三个指针函数，其返回值类型为 double *</span></span><br><span class="line"><span class="type">const</span> <span class="type">double</span>* <span class="title function_">f1</span><span class="params">(<span class="type">const</span> <span class="type">double</span> *ar,<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ar;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">double</span>* <span class="title function_">f2</span><span class="params">(<span class="type">const</span> <span class="type">double</span> ar[],<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ar+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">double</span>* <span class="title function_">f3</span><span class="params">(<span class="type">const</span> <span class="type">double</span> ar[],<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ar+<span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">double</span> av[<span class="number">3</span>] = &#123;<span class="number">1.1</span>,<span class="number">1.2</span>,<span class="number">1.3</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// part1</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;PART1-----------&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// 定义一个 double * 类型的函数指针，指向函数f1</span></span><br><span class="line"><span class="type">const</span> <span class="type">double</span>* (*p1)(<span class="type">const</span> <span class="type">double</span>*,<span class="type">int</span>) = f1;</span><br><span class="line"><span class="comment">// C++11 特性 --&gt; 自动推断类型</span></span><br><span class="line"><span class="keyword">auto</span> p2 = f2;</span><br><span class="line"><span class="comment">// 等价于 --&gt; double* (*p2)(const double*,int) = f2;</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Address        Value&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// 调用f1输出av数组的地址   输出av数组第一个元素的值</span></span><br><span class="line"><span class="comment">// (*p1)(av,3)  == f1(av,3)</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (*p1)(av,<span class="number">3</span>) &lt;&lt;<span class="string">&quot;   &quot;</span> &lt;&lt; *(*p1)(av,<span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// p2(av,3) == f2(av,3)  对地址使用解除引用* 得到其内存中的数据</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p2(av,<span class="number">3</span>) &lt;&lt; <span class="string">&quot;   &quot;</span> &lt;&lt; *p2(av,<span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// part2</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;PART2-----------&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Address        Value&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// 定义一个指针数组，数组有三个元素，每一个元素都为函数指针</span></span><br><span class="line"><span class="type">const</span> <span class="type">double</span>* (*pa[<span class="number">3</span>])(<span class="type">const</span> <span class="type">double</span> * ,<span class="type">int</span>) = &#123;f1,f2,f3&#125;;</span><br><span class="line"><span class="keyword">auto</span> pb = pa;</span><br><span class="line"><span class="comment">// 等价于 const double* (**pb)(const double *,int) = pa;</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i&lt;<span class="number">3</span>;i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (*pa[i])(av,<span class="number">3</span>) &lt;&lt;<span class="string">&quot;   &quot;</span> &lt;&lt; *(*pa[i])(av,<span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i&lt;<span class="number">3</span>;i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (*pb[i])(av,<span class="number">3</span>) &lt;&lt;<span class="string">&quot;   &quot;</span> &lt;&lt; *(*pb[i])(av,<span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// part3</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;PART3-----------&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Address        Value&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// pc 是一个指针，指向由三个函数指针组成的数组</span></span><br><span class="line"><span class="keyword">auto</span> pc = &amp;pa;</span><br><span class="line"><span class="comment">// 等价于 const double *(*(*pc)[3])(const double *,int) = &amp;pa;</span></span><br><span class="line"><span class="comment">// (*pc)[3] --&gt; 数组指针，pc是一个指针，指针指向由三个元素组成的数组</span></span><br><span class="line"><span class="comment">// (*(pc)[3]) 数组内的每一个元素都是指针</span></span><br><span class="line"><span class="comment">// const double *(*(*pc)[3])(const double *,int) 数组内的每一个元素都为函数指针</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (*pc)[<span class="number">0</span>](av,<span class="number">3</span>) &lt;&lt; <span class="string">&quot;   &quot;</span> &lt;&lt; *(*pc)[<span class="number">0</span>](av,<span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// (*pc) == pa  --&gt; (*pc)[0](av,3) = pa[0](av,3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>执行结果：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">PART1-----------</span><br><span class="line">Address        Value</span><br><span class="line"><span class="number">0x7fffbc4b7000</span>   <span class="number">1.1</span></span><br><span class="line"><span class="number">0x7fffbc4b7008</span>   <span class="number">1.2</span></span><br><span class="line">PART2-----------</span><br><span class="line">Address        Value</span><br><span class="line"><span class="number">0x7fffbc4b7000</span>   <span class="number">1.1</span></span><br><span class="line"><span class="number">0x7fffbc4b7008</span>   <span class="number">1.2</span></span><br><span class="line"><span class="number">0x7fffbc4b7010</span>   <span class="number">1.3</span></span><br><span class="line"><span class="number">0x7fffbc4b7000</span>   <span class="number">1.1</span></span><br><span class="line"><span class="number">0x7fffbc4b7008</span>   <span class="number">1.2</span></span><br><span class="line"><span class="number">0x7fffbc4b7010</span>   <span class="number">1.3</span></span><br><span class="line">PART3-----------</span><br><span class="line">Address        Value</span><br><span class="line"><span class="number">0x7fffbc4b7000</span>   <span class="number">1.1</span></span><br></pre></td></tr></table></figure><h2 id="七-左值引用-右值引用"><a href="#七-左值引用-右值引用" class="headerlink" title="七.左值引用 右值引用"></a>七.左值引用 右值引用</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对一个值取地址可以成功取出的为 左值，无法成功取出则为 右值</span><br></pre></td></tr></table></figure><h5 id="1-左值引用"><a href="#1-左值引用" class="headerlink" title="(1). 左值引用"></a>(1). 左值引用</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span>&amp; c = a;   <span class="comment">//左值引用，赋值运算符右侧，一定要是左值，（常）普通变量a是左值</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> b = <span class="number">20</span>;</span><br><span class="line"><span class="type">int</span>&amp; c = (a+b);   <span class="comment">// error ，因为(a+b)是一个右值</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span>&amp; d = <span class="number">10</span>; </span><br><span class="line"><span class="type">const</span> <span class="type">int</span>&amp; c = (a+b);  <span class="comment">// 常引用成功！ const 会将 10，(a+b)计算的结果，放置到内存的临时变量中，使得引用与临时变量产生关联</span></span><br><span class="line"><span class="comment">// 使用常引用后，仅能通过引用来读取数据，无法修改数据</span></span><br></pre></td></tr></table></figure><h5 id="2-右值引用"><a href="#2-右值引用" class="headerlink" title="(2).右值引用"></a>(2).右值引用</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a= <span class="number">10</span>; <span class="comment">// 常值是右值，其地址是随机的不确定的</span></span><br><span class="line"><span class="type">int</span> b = <span class="number">20</span>;</span><br><span class="line"><span class="type">int</span> &amp;&amp;x = <span class="number">10</span>;     <span class="comment">// 合法，右值引用</span></span><br><span class="line"><span class="type">int</span> &amp;&amp;y = (a+b);  <span class="comment">// 右值引用</span></span><br></pre></td></tr></table></figure><h2 id="八-函数显示具体化与实例化区别"><a href="#八-函数显示具体化与实例化区别" class="headerlink" title="八.函数显示具体化与实例化区别"></a>八.函数显示具体化与实例化区别</h2><h3 id="1-形式区别"><a href="#1-形式区别" class="headerlink" title="1.形式区别"></a>1.形式区别</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">template&lt;&gt; <span class="type">void</span> Swap&lt;typeName&gt;(typeName &amp;a,typeName &amp;b);</span><br><span class="line">template&lt;&gt; <span class="type">void</span> <span class="title function_">Swap</span><span class="params">(typeName &amp;a,typeName &amp;b)</span>;</span><br><span class="line"><span class="comment">// 显示具体化，选择以上一种 ---&gt; typeName必须指定为特定的数据类型</span></span><br><span class="line"><span class="comment">// 非函数模板 &gt; 显示具体化 &gt; 函数模板</span></span><br><span class="line"><span class="comment">// 当传入函数的类型为指定的符合显示具体化中对应的参数类型时，此时优先使用显示具体化</span></span><br><span class="line"><span class="comment">// 显示具体化，可以帮助当模板函数无法重载时</span></span><br><span class="line"></span><br><span class="line">template <span class="type">void</span> Swap&lt;typeName&gt;(typeName &amp;a,typeName &amp;b);</span><br><span class="line"><span class="comment">// 显示实例化 ---&gt; typeName必须指定为特定的数据类型</span></span><br><span class="line"><span class="comment">// 函数模板只是用于生成函数定义的方案，并非函数定义</span></span><br><span class="line"><span class="comment">// 显示实例化：使用函数模板生成指定参数类型的函数定义</span></span><br></pre></td></tr></table></figure><p><strong>上述，代码可知，显示具体化声明在关键字 template后包含&lt;&gt;,而显示实例化没有</strong></p><hr><h3 id="2-意义区别"><a href="#2-意义区别" class="headerlink" title="2.意义区别"></a>2.意义区别</h3><p><strong>显示具体化：</strong></p><ul><li>显示具体化,<strong>指定模板函数中类型</strong>，意思是不要使用函数通用的模板来生成函数定义，而是要使用指定的数据类型来生成函数定义</li><li>显示具体化，实际仍然是<strong>隐式实例化</strong>，仅在函数调用时，根据指定的参数类型生成指定的函数定义</li><li>显示具体化为<strong>函数模板的特例</strong></li></ul><p><strong>显示实例化：</strong></p><ul><li>显示实例化,直接命令编译器创建特定的实例 —&gt; <strong>无论是否调用，均会生成函数定义</strong>，即函数定义一直存在。调用函数时，函数定义直接使用，不调用时函数定义已经存在</li><li><strong>其具体用途：</strong> 先创建模板的某个具体实例，而非使用时在隐式的创建（隐式实例化）。<ul><li>显示实例化，是为了编写库文件提供的。没有实例化的模板无法放置在<strong>目标文件（源文件编译之后生成目标文件，目标文件再经过链接生成可执行文件）</strong>中。<strong>当其他文件代码的目标文件调用（或者链接）该函数时，前提是该函数已经生成了目标文件（即该函数已经生成函数定义实例化后，倘若仍然为函数模板则无法调用）</strong></li></ul></li></ul><h2 id="九、ostream控制格式输出"><a href="#九、ostream控制格式输出" class="headerlink" title="九、ostream控制格式输出"></a>九、ostream控制格式输出</h2><p><strong>注：</strong>该部分转载至:<a href="http://c.biancheng.net/view/275.html#:~:text=ostream%20%E7%B1%BB%E6%9C%89%E4%B8%80%E4%BA%9B%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0%EF%BC%8C%E9%80%9A%E8%BF%87%20cout%20%E8%B0%83%E7%94%A8%E5%AE%83%E4%BB%AC%E4%B9%9F%E8%83%BD%E7%94%A8%E4%BA%8E%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E7%9A%84%E6%A0%BC%E5%BC%8F%EF%BC%8C%E5%85%B6%E4%BD%9C%E7%94%A8%E5%92%8C%E6%B5%81%E6%93%8D%E7%BA%B5%E7%AE%97%E5%AD%90%E7%9B%B8%E5%90%8C%EF%BC%8C%E5%A6%82%E8%A1%A8%203%20%E6%89%80%E7%A4%BA%E3%80%82%20setf%20%E5%92%8C,unsetf%20%E5%87%BD%E6%95%B0%E7%94%A8%E5%88%B0%E7%9A%84%20flag%20%EF%BC%8C%E4%B8%8E%20setiosflags%20%E5%92%8C%20resetiosflags%20%E7%94%A8%E5%88%B0%E7%9A%84%E5%AE%8C%E5%85%A8%E7%9B%B8%E5%90%8C%E3%80%82">链接</a></p><h3 id="1-使用流操作算子"><a href="#1-使用流操作算子" class="headerlink" title="1.使用流操作算子"></a>1.使用流操作算子</h3><p>C++ 中常用的输出流操纵算子如表 1 所示，它们都是在头文件 iomanip 中定义的；要使用这些流操纵算子，必须包含该头文件</p><p><strong>注意：“流操纵算子”一栏中的星号<code>*</code>不是算子的一部分，星号表示在没有使用任何算子的情况下，就等效于使用了该算子。例如，在默认情况下，整数是用十进制形式输出的，等效于使用了 dec 算子。</strong></p><table><thead><tr><th>流操纵算子</th><th>作  用</th><th></th></tr></thead><tbody><tr><td>*dec</td><td>以十进制形式输出整数</td><td>常用</td></tr><tr><td>hex</td><td>以十六进制形式输出整数</td><td></td></tr><tr><td>oct</td><td>以八进制形式输出整数</td><td></td></tr><tr><td>fixed</td><td>以普通小数形式输出浮点数</td><td></td></tr><tr><td>scientific</td><td>以科学计数法形式输出浮点数</td><td></td></tr><tr><td>left</td><td>左对齐，即在宽度不足时将填充字符添加到右边</td><td></td></tr><tr><td>*right</td><td>右对齐，即在宽度不足时将填充字符添加到左边</td><td></td></tr><tr><td>setbase(b)</td><td>设置输出整数时的进制，b&#x3D;8、10 或 16</td><td></td></tr><tr><td>setw(w)</td><td>指定输出宽度为 w 个字符，或输人字符串时读入 w 个字符</td><td></td></tr><tr><td>setfill(c)</td><td>在指定输出宽度的情况下，输出的宽度不足时用字符 c 填充（默认情况是用空格填充）</td><td></td></tr><tr><td>setprecision(n)</td><td>设置输出浮点数的精度为 n。  在使用非 fixed 且非 scientific 方式输出的情况下，n 即为有效数字最多的位数，如果有效数字位数超过 n，则小数部分四舍五人，或自动变为科学计 数法输出并保留一共 n 位有效数字。  在使用 fixed 方式和 scientific 方式输出的情况下，n 是小数点后面应保留的位数。</td><td></td></tr><tr><td>setiosflags(flag)</td><td>将某个输出格式标志置为 1</td><td></td></tr><tr><td>resetiosflags(flag)</td><td>将某个输出格式标志置为 0</td><td></td></tr><tr><td>boolapha</td><td>把 true 和 false 输出为字符串</td><td>不常用</td></tr><tr><td>*noboolalpha</td><td>把 true 和 false 输出为 0、1</td><td></td></tr><tr><td>showbase</td><td>输出表示数值的进制的前缀</td><td></td></tr><tr><td>*noshowbase</td><td>不输出表示数值的进制.的前缀</td><td></td></tr><tr><td>showpoint</td><td>总是输出小数点</td><td></td></tr><tr><td>*noshowpoint</td><td>只有当小数部分存在时才显示小数点</td><td></td></tr><tr><td>showpos</td><td>在非负数值中显示 +</td><td></td></tr><tr><td>*noshowpos</td><td>在非负数值中不显示 +</td><td></td></tr><tr><td>*skipws</td><td>输入时跳过空白字符</td><td></td></tr><tr><td>noskipws</td><td>输入时不跳过空白字符</td><td></td></tr><tr><td>uppercase</td><td>十六进制数中使用 A~E。若输出前缀，则前缀输出 0X，科学计数法中输出 E</td><td></td></tr><tr><td>*nouppercase</td><td>十六进制数中使用 a~e。若输出前缀，则前缀输出 0x，科学计数法中输出 e。</td><td></td></tr><tr><td>internal</td><td>数值的符号（正负号）在指定宽度内左对齐，数值右对 齐，中间由填充字符填充。</td><td></td></tr></tbody></table><p>使用这些算子的方法是将算子用 &lt;&lt; 和 cout 连用。例如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; hex &lt;&lt; <span class="number">12</span> &lt;&lt; <span class="string">&quot;,&quot;</span> &lt;&lt; <span class="number">24</span>; <span class="comment">// 这条语句的作用是指定以十六进制形式输出后面两个数</span></span><br></pre></td></tr></table></figure><h3 id="2-setioflags-算子"><a href="#2-setioflags-算子" class="headerlink" title="2.setioflags()算子"></a>2.setioflags()算子</h3><p>setiosflags() 算子实际上是一个库函数，它以一些标志作为参数，这些标志可以是在 iostream 头文件中定义的以下几种取值，它们的含义和同名算子一样。</p><table><thead><tr><th>标 志</th><th>作 用</th></tr></thead><tbody><tr><td>ios::left</td><td>输出数据在本域宽范围内向左对齐</td></tr><tr><td>ios::right</td><td>输出数据在本域宽范围内向右对齐</td></tr><tr><td>ios::internal</td><td>数值的符号位在域宽内左对齐，数值右对齐，中间由填充字符填充</td></tr><tr><td>ios::dec</td><td>设置整数的基数为 10</td></tr><tr><td>ios::oct</td><td>设置整数的基数为 8</td></tr><tr><td>ios::hex</td><td>设置整数的基数为 16</td></tr><tr><td>ios::showbase</td><td>强制输出整数的基数（八进制数以 0 开头，十六进制数以 0x 打头）</td></tr><tr><td>ios::showpoint</td><td>强制输出浮点数的小点和尾数 0</td></tr><tr><td>ios::uppercase</td><td>在以科学记数法格式 E 和以十六进制输出字母时以大写表示</td></tr><tr><td>ios::showpos</td><td>对正数显示“+”号</td></tr><tr><td>ios::scientific</td><td>浮点数以科学记数法格式输出</td></tr><tr><td>ios::fixed</td><td>浮点数以定点格式（小数形式）输出</td></tr><tr><td>ios::unitbuf</td><td>每次输出之后刷新所有的流</td></tr><tr><td>ios::stdio</td><td>每次输出之后清除 stdout, stderr</td></tr></tbody></table><p>多个标志可以用<code>|</code>运算符连接，表示同时设置。例如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; setiosflags(ios::scientific|ios::showpos) &lt;&lt; <span class="number">12.34</span>;</span><br></pre></td></tr></table></figure><p>如果两个相互矛盾的标志同时被设置，如先设置 setiosflags(ios::fixed)，然后又设置 setiosflags(ios::scientific)，那么结果可能就是两个标志都不起作用。因此，在设置了某标志，又要设置其他与之矛盾的标志时，就应该用 resetiosflags 清除原先的标志。例如下面三条语句：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; setiosflags(ios::fixed) &lt;&lt; <span class="number">12.34</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; resetiosflags(ios::fixed) &lt;&lt; setiosflags(ios::scientific | ios::showpos) &lt;&lt; <span class="number">12.34</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; resetiosflags(ios::showpos) &lt;&lt; <span class="number">12.34</span> &lt;&lt; <span class="built_in">endl</span>;  <span class="comment">//清除要输出正号的标志</span></span><br></pre></td></tr></table></figure><h3 id="3-调用cout成员函数"><a href="#3-调用cout成员函数" class="headerlink" title="3.调用cout成员函数"></a>3.调用cout成员函数</h3><p>ostream 类有一些成员函数，通过 cout 调用它们也能用于控制输出的格式，其作用和流操纵算子相同.</p><table><thead><tr><th>成员函数</th><th>作用相同的流操纵算子</th><th>说明</th></tr></thead><tbody><tr><td>precision(n)</td><td>setprecision(n)</td><td>设置输出浮点数的精度为 n。</td></tr><tr><td>width(w)</td><td>setw(w)</td><td>指定输出宽度为 w 个字符。</td></tr><tr><td>fill(c)</td><td>setfill (c)</td><td>在指定输出宽度的情况下，输出的宽度不足时用字符 c 填充（默认情况是用空格填充）。</td></tr><tr><td>setf(flag)</td><td>setiosflags(flag)</td><td>将某个输出格式标志置为 1。</td></tr><tr><td>unsetf(flag)</td><td>resetiosflags(flag)</td><td>将某个输出格式标志置为 0。</td></tr></tbody></table><p>setf 和 unsetf 函数用到的<code>flag</code>，与 setiosflags 和 resetiosflags 用到的完全相同。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::scientific);</span><br><span class="line"><span class="built_in">cout</span>.precision(<span class="number">8</span>);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="number">12.23</span> &lt;&lt; <span class="built_in">endl</span>; <span class="comment">// 1.22300000e+001</span></span><br></pre></td></tr></table></figure><h2 id="十、类的自动类型转换与强制转换"><a href="#十、类的自动类型转换与强制转换" class="headerlink" title="十、类的自动类型转换与强制转换"></a>十、类的自动类型转换与强制转换</h2><h3 id="1-构造函数用作自动类型转换的转换函数"><a href="#1-构造函数用作自动类型转换的转换函数" class="headerlink" title="1.构造函数用作自动类型转换的转换函数"></a>1.构造函数用作自动类型转换的转换函数</h3><p><strong>只接受一个参数的构造函数才可以作为转换函数</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Stock::Stock(<span class="type">double</span> lab); <span class="comment">// 构造函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个Stock对象</span></span><br><span class="line">Stock one; </span><br><span class="line"><span class="comment">// 将创建一个Stock(double lab)的临时对象，并将19.6作为初始值，采用成员赋值的方法，将该临时对象的内容复制到one中</span></span><br><span class="line">one = <span class="number">19.6</span></span><br></pre></td></tr></table></figure><h4 id="explicit"><a href="#explicit" class="headerlink" title="explicit"></a>explicit</h4><p><strong>可通过构造函数前，显示添加explicit，关闭该自动特性</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">explicit <span class="title function_">Stock::Stock</span><span class="params">(<span class="type">double</span> lab)</span>;</span><br><span class="line"><span class="comment">// 只接受一个参数的构造函数定义了从参数类型到类类型的转换，使用explicit可以关闭这种自动的隐式转换.</span></span><br></pre></td></tr></table></figure><h3 id="2-转换函数"><a href="#2-转换函数" class="headerlink" title="2.转换函数"></a>2.转换函数</h3><p><strong>转换函数，定义了从 某种类型 到 类类型的转换</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Stock::Stock(<span class="type">double</span> lab); <span class="comment">// 构造函数</span></span><br><span class="line">Stock::Stock(<span class="type">double</span> lab，<span class="type">double</span> mab)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 若定义了从Stock到double类型的转换</span></span><br><span class="line">Stock <span class="title function_">wolf</span><span class="params">(<span class="number">16.3</span>)</span>;</span><br><span class="line">Stock <span class="title function_">solf</span><span class="params">(<span class="number">30</span>,<span class="number">5</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用转换函数</span></span><br><span class="line"><span class="type">double</span> host = <span class="type">double</span>(wolf);</span><br><span class="line"><span class="type">double</span> think = (<span class="type">double</span>) wolf;</span><br><span class="line"><span class="comment">// 使用转换函数</span></span><br><span class="line"><span class="type">double</span> host2 = <span class="type">double</span>(solf);</span><br><span class="line"><span class="type">double</span> think2 = (<span class="type">double</span>) solf;</span><br></pre></td></tr></table></figure><p><strong>创建转换函数</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">operator <span class="title function_">typeName</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意：</span></span><br><span class="line"><span class="number">1.</span>转换函数必须是类方法</span><br><span class="line">    <span class="number">2.</span>转换函数不能指定返回类型</span><br><span class="line">    <span class="number">3.</span>转换函数不能有参数</span><br><span class="line">       </span><br><span class="line"><span class="comment">// 如：</span></span><br><span class="line">        operator <span class="title function_">double</span><span class="params">()</span>; <span class="comment">//此处的typeName(double)是指定要转换成的类型</span></span><br></pre></td></tr></table></figure><h4 id="explicit-1"><a href="#explicit-1" class="headerlink" title="explicit"></a>explicit</h4><p>若类方法中，只定义了 <strong>类类型到一种类型</strong> 的转换函数，这将避免二义性，而使得可以使用隐式转换</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//类声明中</span></span><br><span class="line">Stock::Stock(<span class="type">double</span> a); <span class="comment">// 构造函数</span></span><br><span class="line">operator <span class="title function_">double</span><span class="params">()</span> ;<span class="comment">// 定义转换函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 隐式转换</span></span><br><span class="line">Stock <span class="title function_">opp</span><span class="params">(<span class="number">5</span>)</span>;</span><br><span class="line"><span class="type">double</span> lab = opp; <span class="comment">// 而非显示转换 double lab = double(opp)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 二义性</span></span><br><span class="line">operator <span class="title function_">double</span><span class="params">()</span> ;<span class="comment">// 定义转换函数</span></span><br><span class="line">operator <span class="title function_">int</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">long</span> gone = opp;</span><br><span class="line"><span class="type">long</span> gone = opp; <span class="comment">//由于上面定义了两种转换函数，这样使用隐式转换编译器报错</span></span><br></pre></td></tr></table></figure><p><strong>explicit关键词，可以防止这样的隐式转换，只允许显示转换</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">explicit operator <span class="title function_">double</span><span class="params">()</span> ;<span class="comment">// 定义转换函数</span></span><br><span class="line">explicit operator <span class="title function_">int</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">long</span> gone = (<span class="type">double</span>)opp;</span><br><span class="line"><span class="type">long</span> gone = (<span class="type">int</span>)opp;</span><br></pre></td></tr></table></figure><h2 id="十一、类的静态成员变量"><a href="#十一、类的静态成员变量" class="headerlink" title="十一、类的静态成员变量"></a>十一、类的静态成员变量</h2><p><strong>静态类成员：无论创建多少个类对象，程序只创建一个静态类变量的副本</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义在StringBad类声明中</span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> number;</span><br></pre></td></tr></table></figure><p><strong>不能在类声明中初始化静态成员变量，声明描述了如何分配内存，但是并不分配内存，若在类声明（.h头文件中）初始化静态成员变量，当将该头文件引入别的文件时，会违背单定义的原则</strong></p><p><strong>可以在类实现中（.cpp文件中）初始化静态变量</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化在类实现文件中</span></span><br><span class="line"><span class="type">int</span> StringBad::number = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过类名进行调用</span></span><br><span class="line">StringBad::number</span><br></pre></td></tr></table></figure><p><strong>若静态成员变量是const类型，则可以在类声明中初始化</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 类声明中进行初始化</span></span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> number = <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h2 id="十二、复制构造函数"><a href="#十二、复制构造函数" class="headerlink" title="十二、复制构造函数"></a>十二、复制构造函数</h2><p>复制构造函数用于将一个对象复制到新创建的对象中。即用于初始化过程中（包括按值传递参数），而非常规的赋值过程，<strong>类的复制构造函数原型通常如下：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Class_Name(<span class="type">const</span> Class_Name &amp;)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 如：</span></span><br><span class="line">    StringBad(<span class="type">const</span> StringBad &amp;)</span><br></pre></td></tr></table></figure><h3 id="1-何时调用赋值构造函数"><a href="#1-何时调用赋值构造函数" class="headerlink" title="1.何时调用赋值构造函数"></a>1.何时调用赋值构造函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>当函数按值传递类对象时（意味着创建原始变量的副本，编译器将生成临时对象，将使用复制构造函数）</span><br><span class="line"><span class="number">2.</span>函数返回类对象时（返回类对象，也会生成临时对象）</span><br></pre></td></tr></table></figure><p>按值传递对象将调用复制构造函数，因此尽可能<strong>按引用传递对象</strong>，节省调用复制构造函数的时间，节省创建副本的时间</p><h3 id="2-定义显示复制构造函数"><a href="#2-定义显示复制构造函数" class="headerlink" title="2.定义显示复制构造函数"></a>2.定义显示复制构造函数</h3><p>定义一个显示的复制构造函数进行深度复制，<strong>复制构造函数，应当复制对象的数据内容并将对象副本的地址赋给令一个对象，而不仅仅是复制对象的地址，从而去引用该对象</strong>。</p><p>仅仅复制对象的地址赋给其他对象，从而引用该对象，为<strong>浅复制</strong>，当调用析构函数进行释放内存时，则会将一个对象进行两次释放。</p><p>一般的C++标准库均有复制构造函数的重载，若一类中的成员无使用动态的内存分配，程序调用默认的复制构造函数无影响，因为程序结束时，调用析构函数无动态的内存释放，不会对同一个开辟的内存空间进行两次释放。</p><h4 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果类中包含了使用New初始化的指针成员，应当定义一个复制构造函数，以复制指向的数据，而非指针（对象地址），这被称为深度复制。</span><br><span class="line">浅复制：仅浅浅的复制指针信息，而不会深入“挖掘”以复制指针引用的结构与其内容。</span><br></pre></td></tr></table></figure><h3 id="3-赋值问题"><a href="#3-赋值问题" class="headerlink" title="3.赋值问题"></a>3.赋值问题</h3><p>将一个对象赋值给另外一个对象，也会出现与隐式复制构造函数相同的问题，因此需要<strong>提供复制赋值运算符的定义进行（深度复制）</strong></p><p>书中：P356</p><h2 id="十三、成员列表的初始化"><a href="#十三、成员列表的初始化" class="headerlink" title="十三、成员列表的初始化"></a>十三、成员列表的初始化</h2><p>对类中声明的const常量无法在类定义中对其进行赋值，因为<strong>常量只能在声明时进行初始化</strong>，因此可以使用<strong>成员初始化列表</strong>的方式，<strong>对常量进行初始化操作</strong>。成员列表初始化会在执行构造函数之前，对创建的对象进行初始化。成员列表初始化的方式只能用于构造函数。</p><p><strong>注意：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>只能用于构造函数</span><br><span class="line"><span class="number">2.</span>必须使用成员列表初始化的操作，初始化非静态<span class="type">const</span>数据成员</span><br><span class="line"><span class="number">3.</span>必须使用成员列表初始化的操作，初始化引用数据成员</span><br><span class="line">    </span><br><span class="line">    成员列表初始化，在执行构造函数的函数体之前，先创建对象并对对象进行初始化</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Queue::Queue(<span class="type">int</span> qs):qsize(qs)</span><br><span class="line">&#123;</span><br><span class="line">front = rear = <span class="literal">NULL</span>;</span><br><span class="line">items = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//qsize =qs;    // 这是非法操作，对常量无法进行赋值操作，仅能初始化</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//解析：</span></span><br><span class="line">:qsize(qs)    <span class="comment">// 该部分是对类常量qsize 初始化为qs</span></span><br></pre></td></tr></table></figure><p><strong><code>qsiz</code>e</strong> 是在类声明中定义的一个常量，无法通过 <code>**qsize =qs**</code> 对常量进行赋值操作，只能初始化，原因是从概念上来说，调用构造函数时，对象将在括号中的代码执行之前被创建，因此调用 <strong>Queue(int qs)</strong> 构造函数将导致程序首先给四个成员变量分配内存<strong>（此时分配内存相当于在做初始化）</strong>，然后程序流程进入到括号中，使用常规的赋值方式，将值存储到内存中，因此对于 <strong>const</strong> 数据成员，<strong>必须在执行构造函数体之前，即创建对象时进行初始化</strong></p><p><strong>成员初始化列表，不用写在类声明中，只需要写在类定义中</strong></p><h2 id="十四、友元"><a href="#十四、友元" class="headerlink" title="十四、友元"></a>十四、友元</h2><ul><li>友元函数</li><li>友元类</li><li>友元成员函数</li></ul><p><strong>友元函数是一种非成员函数，可以赋予该函数与类的成员函数相同的访问权限，可以访问类中的私有成员变量</strong></p><p><strong>友元函数，将普通的函数，通过<code>friend</code>修饰，但是该修饰只出现在函数原型上，即类声明中。不要在类定义中使用关键字<code>friend</code></strong></p><h2 id="十五、派生类的构造函数"><a href="#十五、派生类的构造函数" class="headerlink" title="十五、派生类的构造函数"></a>十五、派生类的构造函数</h2><h3 id="1-成员初始化列表调用基类构造函数"><a href="#1-成员初始化列表调用基类构造函数" class="headerlink" title="1.成员初始化列表调用基类构造函数"></a>1.成员初始化列表调用基类构造函数</h3><p><strong>派生类的构造函数必须使用基类的构造函数</strong></p><p><strong>创建派生类对象时，程序首先创建基类对象。从概念上说，这意味着基类对象应当在程序进入派生类构造函数之前被创建。C++使用成员初始化列表的方式创建</strong></p><p>下面是一个派生类<code>RatedPlayer</code>继承自基类<code>TableTennisPlayer</code>构造函数的代码实例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 只在类实现中这样写，类声明中不写成员列表初始化</span></span><br><span class="line">RatedPlayer::RatedPlayer(<span class="type">unsigned</span> <span class="type">int</span> r,<span class="type">const</span> <span class="built_in">string</span> &amp; fn,<span class="type">const</span> <span class="built_in">string</span> &amp; ln,<span class="type">bool</span> ht):TableTennisPlayer(fn,ln,ht)</span><br><span class="line">&#123;</span><br><span class="line">    rating = r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述实例代码，<code>:TableTennisPlayer(fn,ln,ht)</code>是成员初始化列表。它是<strong>可执行代码，调用TableTennisPlayer构造函数。通过成员初始化列表，使用基类的构造函数为基类的私有成员进行赋值。</strong></p><p><strong>先创建基类对象（通过成员初始化列表的方式，隐式创建），在通过派生类构造函数创建派生类对象</strong></p><h3 id="2-省略成员初始化列表"><a href="#2-省略成员初始化列表" class="headerlink" title="2.省略成员初始化列表"></a>2.省略成员初始化列表</h3><p>若省略成员初始化列表，实例代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RatedPlayer::RatedPlayer(<span class="type">unsigned</span> <span class="type">int</span> r,<span class="type">const</span> <span class="built_in">string</span> &amp; fn,<span class="type">const</span> <span class="built_in">string</span> &amp; ln,<span class="type">bool</span> ht)</span><br><span class="line">&#123;</span><br><span class="line">    rating = r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>必须首先创建基类对象，若不调用基类构造函数（即使不使用成员初始化列表方式调用基类的构造函数），程序也将使用默认基类构造函数</strong>，以下代码与上述等价：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 等价于调用默认的构造函数</span></span><br><span class="line">RatedPlayer::RatedPlayer(<span class="type">unsigned</span> <span class="type">int</span> r,<span class="type">const</span> <span class="built_in">string</span> &amp; fn,<span class="type">const</span> <span class="built_in">string</span> &amp; ln,<span class="type">bool</span> ht):TableTennisPlayer()</span><br><span class="line">&#123;</span><br><span class="line">    rating = r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>除非使用默认的构造函数，否则应该显示的调用正确的基类构造函数</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RatedPlayer::RatedPlayer(<span class="type">unsigned</span> <span class="type">int</span> r,<span class="type">const</span> TableTennisPlayer &amp; tp):TableTennisPlayer(tp)</span><br><span class="line">&#123;</span><br><span class="line">    rating = r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>tp</code>的类型是 <code>TableTennisPlayer &amp; </code>,因此将调用基类的复制构造函数，基类若没有定义复制构造函数，编译器将自动生成一个。</p><p>若愿意，也可以使用成员初始化列表对<strong>派生类成员进行初始化</strong>，代码实例如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RatedPlayer::RatedPlayer(<span class="type">unsigned</span> <span class="type">int</span> r,<span class="type">const</span> TableTennisPlayer &amp; tp):TableTennisPlayer(tp),rating(r)</span><br><span class="line">&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-要点"><a href="#3-要点" class="headerlink" title="3.要点"></a>3.要点</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先创建基类对象</span><br><span class="line">派生类构造函数应通过成员初始化列表将基类信息传递给基类构造函数</span><br><span class="line">派生类构造函数应初始化派生类新增的数据成员</span><br></pre></td></tr></table></figure><p><strong>注意：创建派生类对象时，程序首先调用基类构造函数，再调用派生类构造函数，基类构造函数负责初始化继承的数据成员；派生类构造函数主要用于初始化新增的数据成员。派生类构造函数总是调用几个基类的构造函数。可以使用初始化列表的语法指明要使用的基类构造函数，否则将使用默认的基类构造函数</strong></p><p><strong>派生类对象过期（应该被释放时），程序首先调用派生类的析构函数，然后再调用基类的析构函数</strong></p><h2 id="十六、派生类与基类之间的关系"><a href="#十六、派生类与基类之间的关系" class="headerlink" title="十六、派生类与基类之间的关系"></a>十六、派生类与基类之间的关系</h2><ul><li><p>派生类可以使用基类的方法，<strong>前提是方法非私有</strong></p></li><li><p><strong>基类的指针与引用均可以不在显示转换的情况下指向派生类对象</strong>，但是基类指针与方法均只能调用基类的方法。**(单向)**，派生类新增的方法与成员基类的指针与引用无法调用</p><ul><li><pre><code class="c">// ball 是 basketball 的基类basketball object(&quot;vs&quot;);ball *f_object1 = &amp;object; // 基类指针指向派生类对象ball &amp;f_object2 = object;  // 基类引用指向派生类对象<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 可以将派生类对象赋给基类对象**(会使用隐式的重载运算符)**</span><br><span class="line"></span><br><span class="line">  * ```c</span><br><span class="line">    // ball 是 basketball 的基类</span><br><span class="line">    basketball object(&quot;vs&quot;);</span><br><span class="line">    ball f_object1 = object; // 派生类对象赋值给基类对象</span><br><span class="line">    </span><br><span class="line">    // 这种情况，会使用隐式的重载运算符</span><br><span class="line">    ball &amp;f_object1 = (const ball &amp;) object;  // 基类引用指向派生类对象,object的基类部分，将被复制给f_object1</span><br></pre></td></tr></table></figure></code></pre></li></ul></li><li><p>在派生类中若定义了与基类相同名称的方法<strong>（重写）</strong>，则需要通过<code>::</code>作用域解析运算符来调用基类的方法</p><ul><li><pre><code>// ball 是 basketball 的基类// 若基类与派生类中均定义了 view()方法，在派生类中想使用基类的方法，则需要使用 基类名称：：方法名称 调用基类方法ball::view()<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 十七、虚方法</span><br><span class="line"></span><br><span class="line">在类的类声明的函数原型中通过`virtual`关键字可以将函数进行**虚函数**的声明。**`virtual`关键词只用于类声明的方法原型中**，同友元函数的关键词`friend`</span><br><span class="line"></span><br><span class="line">**在基类中将派生类会重新定义的方法，声明为虚方法，方法在基类中被声明为虚方法之后，在派生类中不用指明关键字 virtual 也会自动成为虚方法**，好习惯，还是将**派生类中的虚方法通过`virtual`声明出来，增加代码的可读性**</span><br><span class="line"></span><br><span class="line">**基类声明一个虚的析构函数，这样可以确保释放对象时，按照正确的顺序调用析构函数**</span><br><span class="line"></span><br><span class="line">### 1.重点</span><br><span class="line"></span><br><span class="line">**利用基类的指针与引用指向派生类对象，若方法没有声明为`virtual` 虚方法，程序将根据指针或者引用的类型去选择方法；若使用了`virtual`声明为了虚方法，程序则会根据指针或者引用则会根据指向对象的类型来选择方法**</span><br><span class="line"></span><br><span class="line">* 若`view()`没有被声明为虚方法,`view()`方法在基类与派生类中均有定义</span><br><span class="line"></span><br><span class="line">  * ```c</span><br><span class="line">    // ball 是 basketball 的基类</span><br><span class="line">    basketball object(&quot;vs&quot;);</span><br><span class="line">    ball opp(&quot;str&quot;);</span><br><span class="line">    ball &amp;f_object1 = opp; // 基类引用指向基类对象</span><br><span class="line">    ball &amp;f_object2 = object;  // 基类引用指向派生类对象</span><br><span class="line">    </span><br><span class="line">    ball.view(); // 引用类型为基类引用，因此均是调用基类中定义的 view() 方法</span><br><span class="line">    ball.view(); // 引用类型为基类引用，因此均是调用基类中定义的 view() 方法</span><br></pre></td></tr></table></figure></code></pre></li></ul></li><li><p>若<code>view()</code>被声明为虚方法,<code>view()</code>方法在基类与派生类中均有定义</p><ul><li><pre><code class="c">// ball 是 basketball 的基类basketball object(&quot;vs&quot;);ball opp(&quot;str&quot;);ball &amp;f_object1 = opp; // 基类引用指向基类对象ball &amp;f_object2 = object;  // 基类引用指向派生类对象ball.view(); // 引用类型为基类引用，引用指向的对象为基类对象，因此使用基类的方法ball.view(); // 引用类型为基类引用，引用指向的对象为派生类对象，因此使用派生类的方法<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.虚函数注意事项</span><br><span class="line"></span><br><span class="line">* 若定义的类被用作基类，则应该将那些**派生类中重新定义的方法声明为虚函数**</span><br><span class="line">* **构造函数**不能为虚函数</span><br><span class="line">* **析构函数应当是虚函数，除非类不做基类**，即使类不做基类，将其析构函数定义为虚函数也没有问题</span><br><span class="line">* **友元函数不能为虚函数，因为友元函数不是成员函数，为非成员函数**</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 十八、为何需要虚的析构函数</span><br><span class="line"></span><br><span class="line">**若析构函数不是虚函数**，通过一个基类指针/引用指向派生类对象时，当基类指针/引用被释放时，则将只调用对应于指针或者引用类型的析构函数。，这意味着只有基类的对象被调用，即使指向了一个派生类对象。</span><br><span class="line"></span><br><span class="line">**若析构函数为虚函数**，通过一个基类指针/引用指向派生类对象时，当基类指针/引用被释放时，则将先调用派生类即**指针/引用指向对象的析构函数**，然后**自动调用基类的析构函数**即指针/引用类型的析构函数。</span><br><span class="line"></span><br><span class="line">**总结：**使用虚的析构函数，可以确保正确的析构函数序列被调用。**好习惯：在基类中声明虚的析构函数**</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 十九、抽象基类（纯虚函数）</span><br><span class="line"></span><br><span class="line">抽象基类**（ABC）**包含其所有派生类的共有（共性）部分</span><br><span class="line"></span><br><span class="line">C++通过纯虚函数来提供为实现的函数，**纯虚函数在结尾处为=0**</span><br><span class="line"></span><br><span class="line">```c</span><br><span class="line">virtual double Area()=0; // 纯虚函数</span><br></pre></td></tr></table></figure></code></pre></li></ul></li></ul><p><strong>若一个类需要成为抽象基类（ABC）则该基类中必须包含一个纯虚函数，含有纯虚函数的类（ABC）只能做基类，无法创建该类对象。但是即使该基类为抽象的，我们仍可以在实现文件中提供方法的定义</strong></p><h2 id="二十、类模板"><a href="#二十、类模板" class="headerlink" title="二十、类模板"></a>二十、类模板</h2><h3 id="1-创建类模板与模板函数分为以下三步："><a href="#1-创建类模板与模板函数分为以下三步：" class="headerlink" title="1.创建类模板与模板函数分为以下三步："></a>1.创建类模板与模板函数分为以下三步：</h3><ul><li>模板类—如下代码打头：<code>template&lt;class Type&gt;</code></li><li>每个函数头都将以相同的模板声明打头：<code>template&lt;class Type&gt;</code></li><li>最后需要将类限定符 从 <strong><code>类名::</code></strong> 改为 <strong><code>类名&lt;Type&gt;::</code></strong></li></ul><p><strong>重点：</strong></p><p><strong>这些模板不是类和成员函数的定义，他们是c++编译器指令，</strong>说明了如何生成类和成员函数定义，模板的具体实现被称为实例化或者具体化。不能将模板成员函数放在独立的实现文件中，<strong>由于模板不是函数，他们不能单独编译. 模板必须与特定的模板实例一起使用</strong>，最简单的方法是：<strong>将所有模板信息放在一个头文件中</strong></p><p><strong>例：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">template &lt;<span class="class"><span class="keyword">class</span> <span class="title">Type</span>&gt;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    private:</span><br><span class="line">    Type item[<span class="number">5</span>];</span><br><span class="line">    public:</span><br><span class="line">    <span class="type">bool</span> <span class="title function_">pop</span><span class="params">(Type &amp; item)</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 函数实现</span></span><br><span class="line">template &lt;<span class="class"><span class="keyword">class</span> <span class="title">Type</span>&gt;</span></span><br><span class="line"><span class="type">bool</span> Stack&lt;Type&gt;::pop(Type &amp; item)</span><br><span class="line">&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-模板隐式实例化、显示实例化、具体化"><a href="#2-模板隐式实例化、显示实例化、具体化" class="headerlink" title="2.模板隐式实例化、显示实例化、具体化"></a>2.模板隐式实例化、显示实例化、具体化</h3><p>① <strong>隐式实例化</strong>：给模板传入特定的类型参数并创建类对象时，会同时生成类声明，<strong>该操作在程序运行阶段生成类定义</strong></p><p>② <strong>显示实例化</strong>：使用template打头，并给模板传入特定的类型参数（无需创建类对象）。<strong>该操作在程序编译阶段产生类声明</strong></p><p>③ <strong>具体化</strong>：给模板使用具体的类型参数生成类声明。显&#x2F;隐实例化均通过具体的类型生成类声明。实例化属于具体化</p><p>类模板与函数模板很相似，均可以有隐式实例化、显示实例化和显示具体化，都统称为具体化，<strong>模板以泛型的方式描述类，而具体化使用具体的类型生成类声明</strong></p><p><strong>模板仅仅描述类的样子并无类的定义与声明，而通过具体化可生成类声明</strong></p><h2 id="二十一、异常"><a href="#二十一、异常" class="headerlink" title="二十一、异常"></a>二十一、异常</h2><h3 id="1-调用abort-函数"><a href="#1-调用abort-函数" class="headerlink" title="1.调用abort()函数"></a>1.调用abort()函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> <span class="title function_">hmean</span><span class="params">(<span class="type">double</span> a,<span class="type">double</span> b)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">double</span> x,y,z;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Enter 2 number:&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;x&gt;&gt;y)</span><br><span class="line">    &#123;</span><br><span class="line">        z = hmean(x,y);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Harmonic mean of &quot;</span> &lt;&lt; x &lt;&lt; <span class="string">&quot; and &quot;</span> &lt;&lt; y &lt;&lt; <span class="string">&quot; is &quot;</span> &lt;&lt; z &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Enter next set of number &lt;q to quit&gt;: &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Bye!\n&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> <span class="title function_">hmean</span><span class="params">(<span class="type">double</span> a,<span class="type">double</span> b)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(a==-b) <span class="comment">// 若当前 a==-b则会出现除数为0的情况</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot; untenable arguments to hmean()&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="built_in">abort</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*a*b / (a+b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong><code>abort()函数会在出现a==-b</code>的情况时会直接中止程序，程序运行阶段错误</strong></p><p><strong>运行结果：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/home/zxz/c++/demo/<span class="number">9</span>/cmake-build-debug/<span class="number">9</span></span><br><span class="line">Enter <span class="number">2</span> number:</span><br><span class="line"><span class="number">10</span> <span class="number">-10</span></span><br><span class="line"> untenable arguments to <span class="title function_">hmean</span><span class="params">()</span></span><br><span class="line"></span><br><span class="line">进程已结束，退出代码为 134 <span class="params">(interrupted by signal <span class="number">6</span>: SIGABRT)</span></span><br></pre></td></tr></table></figure><h3 id="2-异常机制"><a href="#2-异常机制" class="headerlink" title="2.异常机制"></a>2.异常机制</h3>]]></content>
      
      
      <categories>
          
          <category> 嵌入式 </category>
          
          <category> C加加 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 嵌入式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow 入门教程（1）</title>
      <link href="/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Tensorflow/Tensorflow%20%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/"/>
      <url>/2023/05/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Tensorflow/Tensorflow%20%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><h2 id="Tensorflow-入门教程（1）"><a href="#Tensorflow-入门教程（1）" class="headerlink" title="Tensorflow 入门教程（1）"></a>Tensorflow 入门教程（1）</h2><p>视频教程链接：</p><p><a href="https://www.bilibili.com/video/BV1Cg4y1q7Xq?p=9">【国家精品课程】北京大学人工智能实践-TensorFlow2.0</a></p><h3 id="一、张量以及常用函数"><a href="#一、张量以及常用函数" class="headerlink" title="一、张量以及常用函数"></a>一、张量以及常用函数</h3><h4 id="1-创建张量"><a href="#1-创建张量" class="headerlink" title="1.创建张量"></a>1.创建张量</h4><p>由<code>tf.constant()</code>函数进行创建</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>]],dtype=tf.int32)</span><br><span class="line">print(a)</span><br><span class="line">print(a.dtype)</span><br><span class="line">print(a.shape)</span><br></pre></td></tr></table></figure><p><strong>运行结果：</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011456814.png" alt="image-20230501145049017"></p><p><strong>将<code>numpy</code>格式转换为tensor格式</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = numpy.range(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line">b = tf.convert_to_tensor(a,dtype=tf.int64)</span><br></pre></td></tr></table></figure><p><strong>创建特殊张量</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 创建全部为<span class="number">0</span>的张量</span><br><span class="line">tf.zeros(维度)</span><br><span class="line">    </span><br><span class="line"># 创建全部为<span class="number">1</span>的张量</span><br><span class="line">tf.ones(维度)</span><br><span class="line">    </span><br><span class="line"># 创建全为指定值的张量</span><br><span class="line">tf.fill(维度，指定值)   <span class="meta"># tf.fill([2,2],9)</span></span><br><span class="line">    </span><br><span class="line"># 生成正太分布的随机数 默认均值为 <span class="number">0</span>  标准差为<span class="number">1</span></span><br><span class="line">tf.random.normal(维度,mean=均值,stddev=标准差)</span><br><span class="line"></span><br><span class="line"># 生成截断式正太分布的随机数  生成的随机数据取值在(均值<span class="number">-2</span>*标准差，均值+<span class="number">2</span>*标准差)</span><br><span class="line">tf.random.truncated_normal(维度,mean=均值,stddev=标准差)</span><br><span class="line">    </span><br><span class="line"># 生成均匀分布随机数 在[minval,maxval]之间</span><br><span class="line">tf.random.uniform(维度,minval=最小值,maxval=最大值)</span><br></pre></td></tr></table></figure><h4 id="2-常用函数"><a href="#2-常用函数" class="headerlink" title="2.常用函数"></a>2.常用函数</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 强制tensor转换该数据</span><br><span class="line">tf.cast(张量名,dtype=数据类型)</span><br><span class="line"></span><br><span class="line"># 计算张量维度上元素的最大值</span><br><span class="line">tf.reduce_min(张量名)</span><br><span class="line"></span><br><span class="line"># 计算张量维度上元素的最大值</span><br><span class="line">tf.reduce_max(张量名)</span><br></pre></td></tr></table></figure><p><strong>指定<code>axis</code></strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011508366.png" alt="image-20230501150803267"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 计算张量沿着指定维度的平均值</span><br><span class="line">tf.reduce_mean(张量名,axis=操作轴)</span><br><span class="line"></span><br><span class="line"># 计算张量沿着指定维度的和</span><br><span class="line">tf.reduce_sum(张量名,axis=操作轴)</span><br></pre></td></tr></table></figure><p>示例code:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>]],dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(tf.reduce_mean(a))</span><br><span class="line"></span><br><span class="line">print(tf.reduce_sum(a,axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011515660.png" alt="image-20230501151534626"></p><h5 id="（1）tf-Varible"><a href="#（1）tf-Varible" class="headerlink" title="（1）tf.Varible()"></a>（1）tf.Varible()</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将变量标记为可训练，被标记的变量会在反向传播中记录梯度信息</span><br><span class="line">tf.Varible(初始值)</span><br></pre></td></tr></table></figure><h5 id="（2）Tensorflow中的数学运算"><a href="#（2）Tensorflow中的数学运算" class="headerlink" title="（2）Tensorflow中的数学运算"></a>（2）Tensorflow中的数学运算</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011520916.png" alt="image-20230501152015863"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011520117.png" alt="image-20230501152046019"> </p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011522716.png" alt="image-20230501152232661"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011522399.png" alt="image-20230501152255367"></p><h5 id="（3）输入特征与标签配对的函数"><a href="#（3）输入特征与标签配对的函数" class="headerlink" title="（3）输入特征与标签配对的函数"></a>（3）输入特征与标签配对的函数</h5><p><strong>numpy与tensor格式都可以用该语句输入数据</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 切分传入张量的第一维度，生成输入特征与标签对，构建数据集</span><br><span class="line">tf.data.Dataset.from_tensor_slices((输入特征,标签))</span><br></pre></td></tr></table></figure><p>示例code:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">feature = tf.constant([<span class="number">12</span>,<span class="number">23</span>,<span class="number">10</span>,<span class="number">17</span>])</span><br><span class="line">labels = tf.constant([<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((feature,labels))</span><br><span class="line">print(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> els in dataset:</span><br><span class="line">    print(els)</span><br></pre></td></tr></table></figure><p><strong>运行结果：</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011534097.png" alt="image-20230501153406054"></p><h5 id="（4）tf-GradientTape"><a href="#（4）tf-GradientTape" class="headerlink" title="（4）tf.GradientTape()"></a>（4）tf.GradientTape()</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 函数对参数的求导运算,with结构记录计算过程，gradient求出张量的梯度</span><br><span class="line">with tf.GradientTape() as tape:</span><br><span class="line"> 若干计算过程</span><br><span class="line">grad = tape.gradient(函数,对谁求导)</span><br></pre></td></tr></table></figure><p>示例：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011543126.png" alt="image-20230501154342092"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with tf.GradientTape() as tape:</span><br><span class="line">    w = tf.Variable(tf.constant(<span class="number">3.0</span>))</span><br><span class="line">    loss = tf.<span class="built_in">pow</span>(w, <span class="number">2</span>)</span><br><span class="line">grad = tape.gradient(loss, w)</span><br><span class="line">print(grad)</span><br></pre></td></tr></table></figure><p>运行:</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011543834.png" alt="image-20230501154356793"></p><h5 id="（5）tf-one-hot"><a href="#（5）tf-one-hot" class="headerlink" title="（5）tf.one_hot()"></a>（5）tf.one_hot()</h5><p>独热编码：在分类问题中，常用独热编码作为标签，标记类别：1表示是，0表示非</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将待转换的数据 转换为one-hot形式的数据输出</span><br><span class="line">tf.one_hot(待转换的数据,depth=几分类)</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#tf.one_hot</span></span><br><span class="line">classes=<span class="number">3</span></span><br><span class="line">labels=tf.constant([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]) # 输入的元素值最小为<span class="number">0</span>  最大为<span class="number">2</span></span><br><span class="line">output=tf.one_hot(labels,depth=classes)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011552258.png" alt="image-20230501155233232"></p><h5 id="（6）tf-nn-softmax"><a href="#（6）tf-nn-softmax" class="headerlink" title="（6）tf.nn.softmax()"></a>（6）tf.nn.softmax()</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011556222.png" alt="image-20230501155629072"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011556634.png" alt="image-20230501155656570"></p><h5 id="（7）-assign-sub"><a href="#（7）-assign-sub" class="headerlink" title="（7） assign_sub()"></a>（7） assign_sub()</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 赋值操作 更新参数的值并且返回</span><br><span class="line"># 调用assign_sub之前，先用tf.Varible定义w为可以训练（可自更新）</span><br><span class="line">w.assign(w要自减的内容)</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#assign_sub</span><br><span class="line">w=tf.Variable(<span class="number">4</span>)</span><br><span class="line">w.assign_sub(<span class="number">1</span>)</span><br><span class="line">print(w)</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011602129.png" alt="image-20230501160203099"></p><h5 id="（8）tf-argmax"><a href="#（8）tf-argmax" class="headerlink" title="（8）tf.argmax()"></a>（8）tf.argmax()</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 返回张量沿指定维度最大值的索引</span><br><span class="line">tf.argmax(张量名,axis=操作轴)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011603296.png" alt="image-20230501160348245"></p><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#tf.argmax</span></span><br><span class="line">import numpy as np</span><br><span class="line">test=np.<span class="built_in">array</span>([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">8</span>,<span class="number">7</span>,<span class="number">2</span>]])</span><br><span class="line">print(test)</span><br><span class="line">print(tf.argmax(test,axis=<span class="number">0</span>))#返回每一列最大值的索引</span><br><span class="line">print(tf.argmax(test,axis=<span class="number">1</span>))#返回每一行最大值的索引</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">4</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">7</span> <span class="number">2</span>]]</span><br><span class="line">tf.Tensor([<span class="number">3</span> <span class="number">3</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br></pre></td></tr></table></figure><h3 id="二、鸢尾花分类"><a href="#二、鸢尾花分类" class="headerlink" title="二、鸢尾花分类"></a>二、鸢尾花分类</h3><p>数据集介绍:</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011608240.png" alt="image-20230501160800120"></p><h4 id="1-准备数据"><a href="#1-准备数据" class="headerlink" title="1.准备数据"></a>1.准备数据</h4><h5 id="（1）数据集读入"><a href="#（1）数据集读入" class="headerlink" title="（1）数据集读入"></a>（1）数据集读入</h5><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn import datasets</span><br><span class="line">import pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data    <span class="meta"># 返回iris数据集所有数据集</span></span><br><span class="line">y_data = datasets.load_iris().target  <span class="meta"># 返回iris数据集所有标签</span></span><br><span class="line"><span class="meta"># print(&quot;x_data:\n&quot;, x_data)</span></span><br><span class="line"><span class="meta"># print(&quot;y_data:\n&quot;, y_data)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta"># 将数据变成表格的形式</span></span><br><span class="line">x_data = pd.DataFrame(x_data, columns=[<span class="string">&#x27;花萼长&#x27;</span>, <span class="string">&#x27;花萼宽&#x27;</span>, <span class="string">&#x27;花瓣长&#x27;</span>, <span class="string">&#x27;花瓣宽&#x27;</span>])</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.unicode.east_asian_width&#x27;</span>, True)  <span class="meta"># 设置列名对齐</span></span><br><span class="line">print(<span class="string">&#x27;x_data add index:\n&#x27;</span>, x_data)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 在表格中增加一列</span></span><br><span class="line">x_data[<span class="string">&#x27;类别&#x27;</span>] = <span class="function">y_data</span></span><br><span class="line"><span class="function"><span class="title">print</span>(<span class="params"><span class="string">&#x27;x_data add a column:\n&#x27;</span>, x_data</span>)</span></span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011617229.png" alt="image-20230501161709190"></p><h5 id="（2）数据集乱序"><a href="#（2）数据集乱序" class="headerlink" title="（2）数据集乱序"></a>（2）数据集乱序</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">from pandas import DataFrame</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line"># 读入数据</span><br><span class="line">x_data = datasets.load_iris().data    # 返回iris数据集所有数据集</span><br><span class="line">y_data = datasets.load_iris().target  # 返回iris数据集所有标签</span><br><span class="line"></span><br><span class="line"># 数据集乱序</span><br><span class="line">np.random.seed(<span class="number">116</span>)  # 使用相同的seed，使输入特征/标签一一对应</span><br><span class="line">np.random.shuffle(x_data)  # 打乱</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line"></span><br><span class="line"># 数据集分出永不相见的训练集和测试集</span><br><span class="line">x_train = x_data[:<span class="number">-30</span>]</span><br><span class="line">y_train = y_data[:<span class="number">-30</span>]</span><br><span class="line">x_test = x_data[<span class="number">-30</span>:]</span><br><span class="line">y_test = y_data[<span class="number">-30</span>:]</span><br><span class="line"></span><br><span class="line"># 转换数据类型</span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"># 配成输入特征，标签对，每次喂入一小撮（<span class="number">32</span>个样本为一个batch）</span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure><h4 id="2-训练"><a href="#2-训练" class="headerlink" title="2.训练"></a>2.训练</h4><p>两层网络，输入层以及输出层（4特征输入  3特征输出）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 定义神经网络中所有可训练参数(初始化的参数权值)</span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))</span><br></pre></td></tr></table></figure><p><strong>整体code:</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">from pandas import DataFrame</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data    # 返回iris数据集所有数据集</span><br><span class="line">y_data = datasets.load_iris().target  # 返回iris数据集所有标签</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">116</span>)  # 使用相同的seed，使输入特征/标签一一对应</span><br><span class="line">np.random.shuffle(x_data)  # 打乱</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line"></span><br><span class="line"># 数据集分出永不相见的训练集和测试集</span><br><span class="line">x_train = x_data[:<span class="number">-30</span>]</span><br><span class="line">y_train = y_data[:<span class="number">-30</span>]</span><br><span class="line">x_test = x_data[<span class="number">-30</span>:]</span><br><span class="line">y_test = y_data[<span class="number">-30</span>:]</span><br><span class="line"></span><br><span class="line"># 转换数据类型</span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"># 配成输入特征，标签对，每次喂入一小撮</span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"># 定义神经网络中所有可训练参数</span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">train_loss_result = []  # 将每轮loss记录在此列表中，为后续画loss曲线提供数据</span><br><span class="line">test_acc = []</span><br><span class="line">epoch = <span class="number">500</span></span><br><span class="line">loss_all = <span class="number">0</span>  # 每轮分<span class="number">4</span>个step，loss_all记录四个step生成<span class="number">4</span>个loss值</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;----------- 嵌套循环迭代，with结构更新参数，显示当前loss -----------&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">for</span> epoch in range(epoch):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;-------训练部分--------&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) in enumerate(train_db):</span><br><span class="line">        with tf.GradientTape() as tape:  # 记录梯度信息</span><br><span class="line">            # 前向传播过程记录y</span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  # 前向传播计算预测值</span><br><span class="line">            y = tf.nn.softmax(y)  # 使得预测y得分符合概率分布</span><br><span class="line">            # 对预测结果使用softmax()函数之后才可以与实际标签的one_hot进行比较</span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  # 将标签值独热编码</span><br><span class="line">            # 记录总loss</span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失值mse</span><br><span class="line">            loss_all += loss.numpy()</span><br><span class="line">        # 计算loss对各个参数的梯度</span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line">        # 实现梯度更新，w1=w1-lr*w1_grad b=b-lr*b_grad</span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">    # 每个epoch打印loss信息</span><br><span class="line">    print(<span class="string">&quot;epoch&#123;&#125;,loss&#123;&#125;&quot;</span>.format(epoch, loss_all / <span class="number">4</span>))  # (训练集有<span class="number">120</span>组数据，每个step只能喂入<span class="number">32</span>组数据，需要batch级别循环<span class="number">4</span>次，求每个step的平均loss)</span><br><span class="line">    train_loss_result.append(loss_all / <span class="number">4</span>)  # 将四个step的loss求平均值记录在此变量中</span><br><span class="line">    loss_all = <span class="number">0</span>  # loss_all值归<span class="number">0</span>，为下一个epoch的loss做准备</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;-------测试部分--------&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    # 计算当前参数前向传播后的准确率，显示当前acc</span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test in test_db:  # 遍历测试集中所有数据</span><br><span class="line">        y = tf.matmul(x_test, w1) + b1  # 前向传播计算预测值</span><br><span class="line">        y = tf.nn.softmax(y)  <span class="meta"># y符合概率分布</span></span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  # 返回y中最大值的索引，即预测的分类</span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)  # 调整数据类型与标签一致</span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test),</span><br><span class="line">                          dtype=tf.int32)  <span class="meta"># equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，而是逐个元素进行判断，如果相等就是True，不相等，就是False。由于是逐个元素判断，所以x，y 的维度要一致。</span></span><br><span class="line">        correct = tf.reduce_sum(correct)  # 将所有batch中的correct数加起来</span><br><span class="line">        total_correct += <span class="type">int</span>(correct)  # 将所有batch中的correct数加起来</span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]  # 在矩阵中，[<span class="number">0</span>]就表示行数（样本数），[<span class="number">1</span>]则表示列数，[<span class="number">2</span>]代表通道数</span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    print(<span class="string">&quot;test_acc&quot;</span>, acc)</span><br><span class="line">    print(<span class="string">&quot;------------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;---------------可视化------------&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="meta"># acc/loss可视化</span></span><br><span class="line"># 绘制loss曲线</span><br><span class="line">plt.title(<span class="string">&#x27;loss Curve&#x27;</span>)  # 图片隐私</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="meta"># x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.plot(train_loss_result, label=<span class="string">&quot;$loss$&quot;</span>)  # 逐点画出test_acc值并连线，？epoch?</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># 绘制accuracy曲线</span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)  # 图片隐私</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="meta"># x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)</span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy$&quot;</span>)  # 逐点画出test_acc值并连线，？epoch?</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="三、神经网络的优化过程"><a href="#三、神经网络的优化过程" class="headerlink" title="三、神经网络的优化过程"></a>三、神经网络的优化过程</h3><h4 id="1-预备知识"><a href="#1-预备知识" class="headerlink" title="1.预备知识"></a>1.预备知识</h4><h5 id="（1）tf-where"><a href="#（1）tf-where" class="headerlink" title="（1）tf.where()"></a>（1）tf.where()</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011705991.png" alt="image-20230501170511960"></p><p>示例code:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">b = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">c = tf.where(tf.greater(a, b), a, b)  # 若a&gt;b，返回a对应位置的元素，否则返回b对应位置的元素</span><br><span class="line">print(<span class="string">&quot;c：&quot;</span>, c)</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011706202.png" alt="image-20230501170624125"></p><h5 id="（2）np-random-RandomState-rand"><a href="#（2）np-random-RandomState-rand" class="headerlink" title="（2）np.random.RandomState.rand()"></a>（2）np.random.RandomState.rand()</h5><p>产生一个0-1之间的随机数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.RandomState.rand(维度)</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(seed=1)  # 随机数种子 seed为常数导致每一次生成的随机数相同</span><br><span class="line">a = rdm.rand()</span><br><span class="line">b = rdm.rand(2, 3)</span><br><span class="line">print(&quot;a:&quot;, a)</span><br><span class="line">print(&quot;b:&quot;, b)</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011711953.png" alt="image-20230501171105924"></p><h5 id="（3）np-vstack"><a href="#（3）np-vstack" class="headerlink" title="（3）np.vstack()"></a>（3）np.vstack()</h5><p>将两个数组按照垂直方向叠加</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.vstack(数组<span class="number">1</span>,数组<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.<span class="built_in">array</span>([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.<span class="built_in">array</span>([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">c = np.vstack((a, b))  </span><br><span class="line">print(<span class="string">&quot;c:\n&quot;</span>, c)</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011714125.png" alt="image-20230501171401103"></p><h5 id="（4）np-mgrid-ravel-np-c"><a href="#（4）np-mgrid-ravel-np-c" class="headerlink" title="（4）np.mgrid[]     .ravel()   np.c_[]"></a>（4）np.mgrid[]     .ravel()   np.c_[]</h5><p><strong>np.mgrid[]  返回若干组维度相同的等差数组</strong></p><p><strong>x.ravel()  将x变为一维数组  把<code>.</code>前的变量拉直</strong></p><p><strong>np.c_[]  使返回的间隔数组点配对</strong></p><p>&#96;np.c_[数组1，数组2，…]  </p><p>示例;</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 生成等间隔数值点</span><br><span class="line">x, y = np.mgrid[<span class="number">1</span>:<span class="number">3</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="number">4</span>:<span class="number">0.5</span>]</span><br><span class="line"># 将x, y拉直，并合并配对为二维张量，生成二维坐标点</span><br><span class="line">grid = np.c_[x.ravel(), y.ravel()]</span><br><span class="line">print(<span class="string">&quot;x:\n&quot;</span>, x)</span><br><span class="line">print(<span class="string">&quot;y:\n&quot;</span>, y)</span><br><span class="line">print(<span class="string">&quot;x.ravel():\n&quot;</span>, x.ravel())</span><br><span class="line">print(<span class="string">&quot;y.ravel():\n&quot;</span>, y.ravel())</span><br><span class="line">print(<span class="string">&#x27;grid:\n&#x27;</span>, grid)</span><br></pre></td></tr></table></figure><p>示例;</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305011727663.png" alt="image-20230501172726637"></p><h4 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2.损失函数"></a>2.损失函数</h4><h5 id="（1）均方损失函数"><a href="#（1）均方损失函数" class="headerlink" title="（1）均方损失函数"></a>（1）均方损失函数</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021027411.png" alt="image-20230502100229116"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOSS_MSE = tf.reduce_mean(tf.square(y_ - y))   <span class="meta"># y 模型前向传播值    y_ 样本标签</span></span><br></pre></td></tr></table></figure><h5 id="（2）交叉熵损失函数"><a href="#（2）交叉熵损失函数" class="headerlink" title="（2）交叉熵损失函数"></a>（2）交叉熵损失函数</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021027538.png" alt="image-20230502100543147"></p><p><strong>y_与y的距离越近(越接近)其交叉熵就越小</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOSS_fn = tf.loss.categorical_crossentropy(y_ - y)   <span class="meta"># y 模型前向传播值     </span></span><br></pre></td></tr></table></figure><h6 id="softmax-与交叉熵结合"><a href="#softmax-与交叉熵结合" class="headerlink" title="softmax 与交叉熵结合"></a>softmax 与交叉熵结合</h6><p>输出先过<code>softmax</code>函数，再计算<code>y</code>与<code>y_</code>的交叉熵损失函数</p><p>分步计算</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># y已经经过前向传播得到的预测值</span></span><br><span class="line">y_pro = tf.nn.softmax(y)     #  经过 softmax函数，得到概率分布</span><br><span class="line">loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)   # y_ 样本标签</span><br></pre></td></tr></table></figure><p>结合计算</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_ce2 = tf.softmax_cross_entropy_with_logits(y_,y)</span><br></pre></td></tr></table></figure><h4 id="3-优化器"><a href="#3-优化器" class="headerlink" title="3.优化器"></a>3.优化器</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021027502.png" alt="image-20230502102645614"></p><h5 id="（1）SGD"><a href="#（1）SGD" class="headerlink" title="（1）SGD"></a>（1）SGD</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021030914.png" alt="image-20230502103018837"></p><h5 id="（2）SGDM"><a href="#（2）SGDM" class="headerlink" title="（2）SGDM"></a>（2）SGDM</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021032429.png" alt="image-20230502103205345"></p><p>对于单层网络</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m_w,m_b = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span>   # 动量超参数</span><br><span class="line"></span><br><span class="line">m_w = beta * m_w + (<span class="number">1</span> - beta) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta * m_b + (<span class="number">1</span> - beta) * grads[<span class="number">1</span>]</span><br><span class="line">w1.assign_sub(lr * m_w)   # 参数自更新</span><br><span class="line">b1.assign_sub(lr * m_b)</span><br></pre></td></tr></table></figure><h5 id="（3）Adagrad"><a href="#（3）Adagrad" class="headerlink" title="（3）Adagrad"></a>（3）Adagrad</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021039514.png" alt="image-20230502103919433"></p><p>Adagrad的一阶动量mt就是当前时刻的梯度，二阶动量是梯度平方的累积和</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v_w,v_b = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line"></span><br><span class="line">v_w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub = (lr *grads[<span class="number">0</span>] / tf.<span class="built_in">sqrt</span>(v_w))</span><br><span class="line">b1.ssign_sub = (lr *grads[<span class="number">1</span>] /tf.<span class="built_in">sqrt</span>(v_b))</span><br></pre></td></tr></table></figure><h5 id="（4）RMSProp"><a href="#（4）RMSProp" class="headerlink" title="（4）RMSProp"></a>（4）RMSProp</h5><p>在SGD的基础上增加二阶动量，二阶动量vt使用指数滑动平均值计算，表征过去一段时间的平均值</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021050499.png" alt="image-20230502105046419"></p><p>一阶动量是当前时刻梯度</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">v_w,v_b = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line">    </span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span>-beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span>-beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub = (lr *grads[<span class="number">0</span>] / tf.<span class="built_in">sqrt</span>(v_w))</span><br><span class="line">b1.assign_sub = (lr *grads[<span class="number">1</span>]/  tf.<span class="built_in">sqrt</span>(v_b))</span><br></pre></td></tr></table></figure><h5 id="（5）Adam"><a href="#（5）Adam" class="headerlink" title="（5）Adam"></a>（5）Adam</h5><p>同时结合SGDM一阶动量和RMSProp二阶动量</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021058167.png" alt="image-20230502105816067"></p><h3 id="四、使用八股搭建神经网络"><a href="#四、使用八股搭建神经网络" class="headerlink" title="四、使用八股搭建神经网络"></a>四、使用八股搭建神经网络</h3><h4 id="1-使用Tensorflow-API-tf-keras搭建网络八股"><a href="#1-使用Tensorflow-API-tf-keras搭建网络八股" class="headerlink" title="1.使用Tensorflow API : tf.keras搭建网络八股"></a>1.使用Tensorflow API : tf.keras搭建网络八股</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021101357.png" alt="image-20230502110121291"></p><h5 id="（1）tf-keras-models-Sequential"><a href="#（1）tf-keras-models-Sequential" class="headerlink" title="（1）tf.keras.models.Sequential()"></a>（1）tf.keras.models.Sequential()</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.models.Sequential([网络结构])   # 描述各层网络</span><br></pre></td></tr></table></figure><h6 id="拉直层"><a href="#拉直层" class="headerlink" title="拉直层"></a>拉直层</h6><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021104691.png" alt="image-20230502110427660"></p><h6 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h6><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021104758.png" alt="image-20230502110453675"></p><p>卷积层</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021105726.png" alt="image-20230502110506668"></p><p>LSTM层</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021105144.png" alt="image-20230502110522107"></p><h5 id="（2）model-compile"><a href="#（2）model-compile" class="headerlink" title="（2）model.compile()"></a>（2）model.compile()</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer = 优化器,loss = 损失函数, metrics = [<span class="string">&quot;准确率&quot;</span>])</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021109398.png" alt="image-20230502110921320"></p><p><code>from_logits</code>询问是否是原始输出，若神经网络的预测结果经过了softmax概率分布则填写<code>False</code></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021109229.png" alt="image-20230502110938173"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021112574.png" alt="image-20230502111246499"></p><h5 id="（3）model-fit"><a href="#（3）model-fit" class="headerlink" title="（3）model.fit()"></a>（3）model.fit()</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.fit(训练集的属土特征,训练集标签,batch_size= , epochs = ,</span><br><span class="line">          validation_data = (测试集的输入特征,测试集的标签),</span><br><span class="line">          validation_split=从训练集划分多少比例给测试集,</span><br><span class="line">          validation_freq = 多少次epoch测试一次</span><br><span class="line">          )</span><br></pre></td></tr></table></figure><h5 id="（4）model-summary"><a href="#（4）model-summary" class="headerlink" title="（4）model.summary()"></a>（4）model.summary()</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary() # 打印出网络的结构</span><br></pre></td></tr></table></figure><h4 id="2-使用tf-keras复现鸢尾花分类"><a href="#2-使用tf-keras复现鸢尾花分类" class="headerlink" title="2.使用tf.keras复现鸢尾花分类"></a>2.使用tf.keras复现鸢尾花分类</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    使用tf.keras实现鸢尾花分类</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 读入鸢尾花数据</span><br><span class="line">x_train = datasets.load_iris().data</span><br><span class="line">y_train = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"># 数据集乱序</span><br><span class="line">np.random.seed(<span class="number">116</span>)   # 随机种子一致导致后文  标签与对应样本的特征乱序顺序一致</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"># 搭建模型(三分类--单层网络)</span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">3</span>,activation=<span class="string">&#x27;softmax&#x27;</span>,kernel_regularizer=tf.keras.regularizers.l2())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>),</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 训练拟合(validation_split=<span class="number">0.2</span> 从训练集划分<span class="number">20</span>%比例给测试集,validation_freq=<span class="number">20</span> <span class="number">20</span>次epoch测试一次)</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">32</span>,epochs=<span class="number">500</span>,validation_split=<span class="number">0.2</span>,validation_freq=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021138385.png" alt="image-20230502113808342"></p><h4 id="3-自定义class-类搭建网络结构"><a href="#3-自定义class-类搭建网络结构" class="headerlink" title="3.自定义class 类搭建网络结构"></a>3.自定义class 类搭建网络结构</h4><p>用<code>Sequential</code>搭建出上层输出就是下层输入的顺序网络结构</p><p>但是无法写出带有跳连的非顺序网络结构（<strong>此时类<code>class</code>可以</strong>）—类似于pytorch定义自己的网络结构</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021145518.png" alt="image-20230502114506383"></p><p>示例;</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    使用class实现鸢尾花分类</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 读入鸢尾花数据</span><br><span class="line">x_train = datasets.load_iris().data</span><br><span class="line">y_train = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"># 数据集乱序</span><br><span class="line">np.random.seed(<span class="number">116</span>)   # 随机种子一致导致后文  标签与对应样本的特征乱序顺序一致</span><br><span class="line">np.random.shuffle(x_train)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_train)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"># 自定义类class搭建模型(三分类--单层网络)</span><br><span class="line">class IrisModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(IrisModel,self).__init__()</span><br><span class="line">        self.d1 = tf.keras.layers.Dense(<span class="number">3</span>,activation=<span class="string">&#x27;softmax&#x27;</span>,kernel_regularizer=tf.keras.regularizers.l2())</span><br><span class="line"></span><br><span class="line">    def call(self,x):</span><br><span class="line">        y = self.d1(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = IrisModel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>),</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 训练拟合(validation_split=<span class="number">0.2</span> 从训练集划分<span class="number">20</span>%比例给测试集,validation_freq=<span class="number">20</span> <span class="number">20</span>次epoch测试一次)</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">32</span>,epochs=<span class="number">500</span>,validation_split=<span class="number">0.2</span>,validation_freq=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h4 id="4-MNIST数据集"><a href="#4-MNIST数据集" class="headerlink" title="4.MNIST数据集"></a>4.MNIST数据集</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021210767.png" alt="image-20230502121024663"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021210755.png" alt="image-20230502121037702"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021210898.png" alt="image-20230502121050845"></p><p><strong>可视化：</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021212655.png" alt="image-20230502121213615"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021212924.png" alt="image-20230502121233830"></p><h5 id="（1）手写数字识别（Sequential-）"><a href="#（1）手写数字识别（Sequential-）" class="headerlink" title="（1）手写数字识别（Sequential()）"></a>（1）手写数字识别（Sequential()）</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    手写识别MNIST</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 读入MNIST数据</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train,y_train),(x_test,y_test) = mnist.load_data()</span><br><span class="line"># 将特征皈依化到<span class="number">0</span><span class="number">-1</span> 之间 加快模型收敛</span><br><span class="line">x_train,x_test = x_train / <span class="number">255.0</span> , x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    # 将输入特征展平 <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>,activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    # 输出 <span class="number">10</span> 分类</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 训练拟合(validation_split=<span class="number">0.2</span> 从训练集划分<span class="number">20</span>%比例给测试集,validation_freq=<span class="number">20</span> <span class="number">20</span>次epoch测试一次)</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">32</span>,epochs=<span class="number">5</span>,validation_data=(x_test,y_test),validation_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021222818.png" alt="image-20230502122241785"></p><h5 id="（2）手写数字识别-自定义类class"><a href="#（2）手写数字识别-自定义类class" class="headerlink" title="（2）手写数字识别(自定义类class)"></a>（2）手写数字识别(自定义类class)</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    手写识别MNIST---自定义类class</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 读入MNIST数据</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train,y_train),(x_test,y_test) = mnist.load_data()</span><br><span class="line"># 将特征皈依化到<span class="number">0</span><span class="number">-1</span> 之间 加快模型收敛</span><br><span class="line">x_train,x_test = x_train / <span class="number">255.0</span> , x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"># 搭建模型</span><br><span class="line">class MnistModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MnistModel,self).__init__()</span><br><span class="line">        self.flatten = tf.keras.layers.Flatten(),</span><br><span class="line">        self.d1 = tf.keras.layers.Dense(<span class="number">128</span>,activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d2 = tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    def call(self,x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.d1(x)</span><br><span class="line">        y = self.d2(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = MnistModel()</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    # 将输入特征展平 <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>,activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    # 输出 <span class="number">10</span> 分类</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 训练拟合(validation_split=<span class="number">0.2</span> 从训练集划分<span class="number">20</span>%比例给测试集,validation_freq=<span class="number">20</span> <span class="number">20</span>次epoch测试一次)</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">32</span>,epochs=<span class="number">5</span>,validation_data=(x_test,y_test),validation_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h4 id="5-FASHION数据集"><a href="#5-FASHION数据集" class="headerlink" title="5.FASHION数据集"></a>5.FASHION数据集</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021229859.png" alt="image-20230502122907656"></p><h3 id="五、八股进阶"><a href="#五、八股进阶" class="headerlink" title="五、八股进阶"></a>五、八股进阶</h3><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305021231960.png" alt="image-20230502123140865"></p><h4 id="1-自制数据集"><a href="#1-自制数据集" class="headerlink" title="1.自制数据集"></a>1.自制数据集</h4><p>数据结构如下：（图片名 最后的数字即为其标签）</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022136757.png" alt="image-20230502204829177"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    自制数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from PIL import Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">train_path = <span class="string">&quot;./data/mnist_image_label/mnist_train_jpg_60000/&quot;</span></span><br><span class="line">train_txt = <span class="string">&quot;./data/mnist_image_label/mnist_train_jpg_60000.txt&quot;</span></span><br><span class="line">x_train_savepath = <span class="string">&quot;./data/mnist_image_label/mnist_x_train.npy&quot;</span></span><br><span class="line">y_train_savepath = <span class="string">&quot;./data/mnist_image_label/mnist_y_train.npy&quot;</span></span><br><span class="line"></span><br><span class="line">test_path = <span class="string">&quot;./data/mnist_image_label/mnist_test_jpg_10000/&quot;</span></span><br><span class="line">test_txt = <span class="string">&quot;./data/mnist_image_label/mnist_train_jpg_10000.txt&quot;</span></span><br><span class="line">x_test_savepath = <span class="string">&quot;./data/mnist_image_label/mnist_x_test.npy&quot;</span></span><br><span class="line">y_test_savepath = <span class="string">&quot;./data/mnist_image_label/mnist_y_test.npy&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate(path,txt):</span><br><span class="line">    f = open(txt,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    contents = f.readlines()   # 读取txt文件中所有行</span><br><span class="line">    f.close()</span><br><span class="line">    x,y_ = [],[]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> content in contents:</span><br><span class="line">        value = content.split()  # 以空格分开，图片路径为value[<span class="number">0</span>]  标签文件为value[<span class="number">1</span>]  存入列表</span><br><span class="line">        img_path = path + value[<span class="number">0</span>]  # 拼出图片路径以及文件名</span><br><span class="line">        img = Image.open(img_path)  # 读入图片</span><br><span class="line">        img = np.<span class="built_in">array</span>(img.convert(<span class="string">&#x27;L&#x27;</span>))  # 图片变为<span class="number">8</span>位宽灰度值的np.<span class="built_in">array</span>格式</span><br><span class="line">        img = img / <span class="number">255.</span>   # 皈依化</span><br><span class="line">        x.append(img)</span><br><span class="line">        y_.append(value[<span class="number">1</span>])</span><br><span class="line">        print(<span class="string">&#x27;loading : &#x27;</span> + content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = np.<span class="built_in">array</span>(x)  # 变为np.<span class="built_in">array</span>形式</span><br><span class="line">        y_ = np.<span class="built_in">array</span>(y_)</span><br><span class="line"></span><br><span class="line">        y_ = y_.astype(np.int64)   # 变为<span class="number">64</span>位整型</span><br><span class="line">        <span class="keyword">return</span> x,y_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;---数据集---&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath) and os.path.exists(x_test_savepath) and os.path.exists(y_test_savepath):</span><br><span class="line">    print(<span class="string">&quot;-----------Load Datasets----------&quot;</span>)</span><br><span class="line">    x_train_save = np.load(x_train_savepath)</span><br><span class="line">    y_train = np.load(y_train_savepath)</span><br><span class="line">    x_test_save = np.load(x_test_savepath)</span><br><span class="line">    y_test = np.load(y_test_savepath)</span><br><span class="line">    x_train = np.reshape(x_train_save,(len(x_train_save),<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">    x_test = np.reshape(x_test_save,(len(x_test_save,),<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&quot;----------Generate Datasets----------&quot;</span>)</span><br><span class="line">    x_train,y_train = generate(train_path,train_txt)</span><br><span class="line">    x_test,y_test = generate(test_path,test_txt)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;---------- Save Datasets----------&quot;</span>)</span><br><span class="line">    x_train_save = np.reshape(x_train,(len(x_train),<span class="number">-1</span>))</span><br><span class="line">    x_test_save = np.reshape(x_test,(len(x_test),<span class="number">-1</span>))</span><br><span class="line">    np.save(x_train_savepath,x_train_save)</span><br><span class="line">    np.save(y_train_savepath,y_train)</span><br><span class="line">    np.save(x_test_savepath, x_test_save)</span><br><span class="line">    np.save(y_test_savepath, y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    # 输出 <span class="number">10</span> 分类</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 训练拟合(validation_split=<span class="number">0.2</span> 从训练集划分<span class="number">20</span>%比例给测试集,validation_freq=<span class="number">20</span> <span class="number">20</span>次epoch测试一次)</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">32</span>,epochs=<span class="number">5</span>,validation_data=(x_test,y_test),validation_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h4 id="2-数据增强"><a href="#2-数据增强" class="headerlink" title="2.数据增强"></a>2.数据增强</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022136660.png" alt="image-20230502213605177"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022136351.png" alt="image-20230502213622414"></p><h5 id="（1）注意"><a href="#（1）注意" class="headerlink" title="（1）注意"></a>（1）注意</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_gen_teain.fit(x_train)</span><br></pre></td></tr></table></figure><p><code>fit()</code>函数需要参数为4D参数，因此需要对数据进行<code>reshape()</code></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022139857.png" alt="image-20230502213949787"></p><p>6000为样本数量，28*28为样本尺寸大小，1为通道数</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022141008.png" alt="image-20230502214156936"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><br><span class="line"></span><br><span class="line"># 加载mnist 数据</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line">x_train = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)  # 给数据增加一个维度,从(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>)reshape为(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"># 数据增强</span><br><span class="line">image_gen_train = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span> / <span class="number">1.</span>,  # 如为图像，分母为<span class="number">255</span>时，可归至<span class="number">0</span>～<span class="number">1</span></span><br><span class="line">    rotation_range=<span class="number">45</span>,  # 随机<span class="number">45</span>度旋转</span><br><span class="line">    width_shift_range=<span class="number">.15</span>,  # 宽度偏移</span><br><span class="line">    height_shift_range=<span class="number">.15</span>,  # 高度偏移</span><br><span class="line">    horizontal_flip=False,  # 水平翻转</span><br><span class="line">    zoom_range=<span class="number">0.5</span>  # 将图像随机缩放阈量<span class="number">50</span>％</span><br><span class="line">)</span><br><span class="line">image_gen_train.fit(x_train)</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(image_gen_train.flow(x_train, y_train, batch_size=<span class="number">32</span>), epochs=<span class="number">5</span>, validation_data=(x_test, y_test),</span><br><span class="line">          validation_freq=<span class="number">1</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h4 id="3-读取保存模型"><a href="#3-读取保存模型" class="headerlink" title="3.读取保存模型"></a>3.读取保存模型</h4><h5 id="（1）读取模型"><a href="#（1）读取模型" class="headerlink" title="（1）读取模型"></a>（1）读取模型</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load_weights(路径文件名)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkpoint_save_path = <span class="string">&quot;./mnist.ckpt&quot;</span></span><br><span class="line"># 生成ckpt文件时，会同步生成索引表<span class="string">&#x27;.index&#x27;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;-------load the model--------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br></pre></td></tr></table></figure><h5 id="（2）保存模型"><a href="#（2）保存模型" class="headerlink" title="（2）保存模型"></a>（2）保存模型</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022154400.png" alt="image-20230502215429327"></p><p><code>save_weights_only</code>为是否只保留模型参数、<code>save_best_only</code>为是否只保留最优结果</p><p>执行训练时加入cp_callback选项记录至history中</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022158052.png" alt="image-20230502215806987"></p><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 读取模型</span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/mnist.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line"># 创建callback选项</span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=True,</span><br><span class="line">                                                 save_best_only=True)</span><br><span class="line"></span><br><span class="line"># 执行训练时加入cp_callback选项记录至history中</span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h4 id="4-参数提取读入文本"><a href="#4-参数提取读入文本" class="headerlink" title="4.参数提取读入文本"></a>4.参数提取读入文本</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.trainable_varibles   #  返回模型中可训练的参数</span><br></pre></td></tr></table></figure><p>设置<code>print</code>输出格式</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.set_printoptions(threshold = 超过多少省略显示)</span><br><span class="line"></span><br><span class="line"># 示例  （这样可以将参数全部显示打印出来，而不会出现省略号）</span><br><span class="line">    np.set_printoptions(threshold = np.inf)   <span class="meta"># np.inf 表示无限大</span></span><br></pre></td></tr></table></figure><p><strong>将参数写文本</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">prinr(model.trainable_varibles)</span><br><span class="line">file = open(<span class="string">&#x27;./weights.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> v in model.trainable_varibles:</span><br><span class="line">file.write(str(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    参数提取读入文本</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 设置参数打印方式 （threshold=np.inf  不出现省略全部打印出来）</span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line"># 读取数据集</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"># 网络模型</span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查是否存在模型训练参数保存的ckpt文件</span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/mnist.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    # 若存在则直接加载权重文件</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line"># 若不存在模型训练参数的权重文件---创建回调</span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=True,</span><br><span class="line">                                                 save_best_only=True)</span><br><span class="line"></span><br><span class="line"># 训练并且将模型训练参数权重文件进行保存</span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"># 打印训练权重文件的参数</span><br><span class="line">print(model.trainable_variables)</span><br><span class="line"></span><br><span class="line"># 将参数提取写入txt文件中</span><br><span class="line">file = open(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><h4 id="5-acc-以及-loss-的可视化"><a href="#5-acc-以及-loss-的可视化" class="headerlink" title="5.acc 以及 loss 的可视化"></a>5.acc 以及 loss 的可视化</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022249112.png" alt="image-20230502224907025"></p><p>在训练<code>model.fit()</code>的过程中同步记录了以下信息：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022249158.png" alt="image-20230502224952084"></p><p>可用以下代码进行提取：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022250511.png" alt="image-20230502225038439"></p><p>可视化代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">##############################################<span class="meta">#    show   ###############################################</span></span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022252978.png" alt="image-20230502225230915"></p><h4 id="6-使用训练后的模型进行预测"><a href="#6-使用训练后的模型进行预测" class="headerlink" title="6.使用训练后的模型进行预测"></a>6.使用训练后的模型进行预测</h4><p>输入一张手写数字图片  —-&gt;  输出识别结果</p><p><strong>前向传播执行应用</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(输入特征,batch_size = 整数)   # 返回前向传播计算结果</span><br></pre></td></tr></table></figure><p>下面是预测过程：（复现模型—&gt; 加载参数—&gt; 预测结果）</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305022256218.png" alt="image-20230502225644100"></p><p>但是输入的数据需要满足训练的神经网络对于输入数据的要求</p><h3 id="六、CNN"><a href="#六、CNN" class="headerlink" title="六、CNN"></a>六、CNN</h3><p>卷积神经网络的主要模块</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041045891.png" alt="image-20230504104500795"></p><p>卷积就是特征提取器：CBAPD</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041045822.png" alt="image-20230504104556710"></p><h4 id="1-感受野"><a href="#1-感受野" class="headerlink" title="1.感受野"></a>1.感受野</h4><p><strong>卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305030938126.png" alt="image-20230503093858057"></p><p>上图中<code>1</code>是原始输入数据，为<code>5x5</code>，<code>1</code>通过一个<code>3x3</code>的卷积核变为<code>2</code>，则2的感受野是<code>3</code>.</p><p>在对<code>2</code>经过一个<code>3x3</code>的卷积核变为<code>3</code>，则<code>3</code>的感受野是<code>5</code>（原始输入数据为<code>1</code>）</p><p><code>4</code>的感受野也是<code>5</code></p><h4 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2.卷积层"></a>2.卷积层</h4><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041018339.png" alt="image-20230504100730776"></p><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">6</span>,<span class="number">5</span>,padding=<span class="string">&#x27;valid&#x27;</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">6</span>,(<span class="number">5</span>,<span class="number">5</span>),padding=<span class="string">&#x27;valid&#x27;</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(<span class="number">2</span>,(<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">6</span>,kernel_size=(<span class="number">5</span>,<span class="number">5</span>),padding=<span class="string">&#x27;valid&#x27;</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>),strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])                     </span><br></pre></td></tr></table></figure><h4 id="3-批量标准化（BN）"><a href="#3-批量标准化（BN）" class="headerlink" title="3.批量标准化（BN）"></a>3.批量标准化（BN）</h4><p>神经网络对于0附近的数据更加敏感，但是随着网络层数的增加，特征数据会出现偏离0均值的情况</p><p>标准化：使得数据符合0均值,1为标准差的分布（将偏移的数据重新拉回到0附近）</p><p>批标准化：对一个batch数据，做标准化处理</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041017685.png" alt="image-20230504101708450"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041018839.png" alt="image-20230504101739435"></p><p><strong>批标准化操作会让每个像素点进行减均值除以标准差的自更新计算</strong></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041018596.png" alt="image-20230504101846539"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041020725.png" alt="image-20230504102055604"></p><p><strong>在反向传播时,缩放因子，与偏移因子会与其他待训练的参数一同被训练优化，使得标准正太分布后的特征数据，通过缩放因子与偏移因子优化了投入特征数据的宽窄与偏移量，保证了网络的非线性表达力</strong></p><p><code>BN</code>层位于卷积层之后，激活层之前</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041025242.png" alt="image-20230504102525166"></p><h5 id="（1）BN操作函数"><a href="#（1）BN操作函数" class="headerlink" title="（1）BN操作函数"></a>（1）BN操作函数</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.BatchNormalization(),   # BN层操作</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">6</span>,<span class="number">5</span>,padding=<span class="string">&#x27;valid&#x27;</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    tf.keras.layers.BatchNormalization(),   # BN层操作</span><br><span class="line">    tf.keras.layers.Activation(<span class="string">&#x27;relu&#x27;</span>),     # 激活层</span><br><span class="line">    tf.keras.layers.MaxPool2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.2</span>),           # Dropout()层</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h4 id="4-池化操作"><a href="#4-池化操作" class="headerlink" title="4.池化操作"></a>4.池化操作</h4><p>池化用于减少特征的数据量</p><p><strong>最大池化：</strong>可以提取图片纹理</p><p><strong>均值池化</strong>：可以保留背景特征</p><h5 id="（1）池化函数"><a href="#（1）池化函数" class="headerlink" title="（1）池化函数"></a>（1）池化函数</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041038633.png" alt="image-20230504103811461"></p><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">6</span>,<span class="number">5</span>,padding=<span class="string">&#x27;valid&#x27;</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    tf.keras.layers.BatchNormalization(),   # BN层操作</span><br><span class="line">    tf.keras.layers.Activation(<span class="string">&#x27;relu&#x27;</span>),     # 激活层</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>),strides=<span class="number">2</span>,padding=<span class="string">&#x27;same&#x27;</span>),  # 池化</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.2</span>),           # Dropout()层</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h4 id="5-舍弃"><a href="#5-舍弃" class="headerlink" title="5.舍弃"></a>5.舍弃</h4><p>舍弃（<code>Dropout()</code>）是一种正则化操作，也是为了防止神经网络过拟合</p><p><strong>在神经网络训练时，将一部分神经网络按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元恢复连接</strong></p><h5 id="（1）Dropout函数"><a href="#（1）Dropout函数" class="headerlink" title="（1）Dropout函数"></a>（1）Dropout函数</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Dropout(舍弃的概率)         # Dropout()层</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">6</span>,<span class="number">5</span>,padding=<span class="string">&#x27;valid&#x27;</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    tf.keras.layers.BatchNormalization(),   # BN层操作</span><br><span class="line">    tf.keras.layers.Activation(<span class="string">&#x27;relu&#x27;</span>),     # 激活层</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>),strides=<span class="number">2</span>,padding=<span class="string">&#x27;same&#x27;</span>),  # 池化</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.2</span>),           # Dropout()层</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h3 id="七、CIFAR10数据集"><a href="#七、CIFAR10数据集" class="headerlink" title="七、CIFAR10数据集"></a>七、CIFAR10数据集</h3><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041048799.png" alt="image-20230504104838723"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041049014.png" alt="image-20230504104901853"></p><h4 id="1-导入数据集"><a href="#1-导入数据集" class="headerlink" title="1.导入数据集"></a>1.导入数据集</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">    </span><br><span class="line">cifar10 = tf.keras.datasets.cifar10</span><br><span class="line">(x_train,y_train),(x_test,y_test) = cifar10.load_data()</span><br></pre></td></tr></table></figure><h5 id="（1）可视化数据集"><a href="#（1）可视化数据集" class="headerlink" title="（1）可视化数据集"></a>（1）可视化数据集</h5><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041051641.png" alt="image-20230504105141527"></p><p>下载的数据集到下面目录（ubuntu下<code>/home/用户名/.kera</code>）   <code>.kera</code>是隐藏用户名</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041116255.png" alt="image-20230504111637223"></p><h4 id="2-使用卷积神经网络训练CIFAR10数据集"><a href="#2-使用卷积神经网络训练CIFAR10数据集" class="headerlink" title="2.使用卷积神经网络训练CIFAR10数据集"></a>2.使用卷积神经网络训练CIFAR10数据集</h4><p>卷积模型如下：（卷积就是CBAPD—卷积–批量归一化—激活—池化—舍弃）</p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041055341.png" alt="image-20230504105534244"></p><p><img src="https://gitee.com/zhou-xuezhi/mypic2/raw/master/img/202305041057960.png" alt="image-20230504105722853"></p><p>示例:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    搭建卷积神经网络训练 CIFAR10</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line">cifar10 = tf.keras.datasets.cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"># 搭建网络模型</span><br><span class="line">class Baseline(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Baseline, self).__init__()</span><br><span class="line">        self.c1 = tf.keras.layers.Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&#x27;same&#x27;</span>)  # C卷积层</span><br><span class="line">        self.b1 = tf.keras.layers.BatchNormalization()  # B BN层</span><br><span class="line">        self.a1 = tf.keras.layers.Activation(<span class="string">&#x27;relu&#x27;</span>)  # A 激活层</span><br><span class="line">        self.p1 = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)  # P 池化层</span><br><span class="line">        self.d1 = tf.keras.layers.Dropout(<span class="number">0.2</span>)  # D dropout层</span><br><span class="line"></span><br><span class="line">        self.flatten = tf.keras.layers.Flatten()  # 展平层</span><br><span class="line">        self.f1 = tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)  # 线性层</span><br><span class="line">        self.d2 = tf.keras.layers.Dropout(<span class="number">0.2</span>)</span><br><span class="line">        self.f2 = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)  # 最后<span class="number">10</span>分类，经过softmax变为概率分布</span><br><span class="line"></span><br><span class="line">    def call(self, x):</span><br><span class="line">        x = self.c1(x)</span><br><span class="line">        x = self.b1(x)</span><br><span class="line">        x = self.a1(x)</span><br><span class="line">        x = self.p1(x)</span><br><span class="line">        x = self.d1(x)</span><br><span class="line"></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = self.d2(x)</span><br><span class="line">        y = self.f2(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"># 创建模型示例</span><br><span class="line">model = Baseline()</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              # 损失函数 传入的非原始数据 from_logits=False  (经过了softmax)</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),</span><br><span class="line">              metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"># 加载模型权重参数</span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/Baseline.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line"># 创建回调（用于训练时，保存模型权重参数）</span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=True,</span><br><span class="line">                                                 save_best_only=True)</span><br><span class="line"></span><br><span class="line"># 训练</span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"># 读取模型训练的权重参数，并进行保存至文本</span><br><span class="line"><span class="meta"># print(model.trainable_variables)</span></span><br><span class="line">file = open(<span class="string">&#x27;./weights.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line">##############################################<span class="meta">#    show   ###############################################</span></span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
